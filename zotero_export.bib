
@book{astrom_introduction_1971,
  title = {Introduction to {{Stochastic Control Theory}}},
  author = {Åström, Karl J.},
  date = {1971-02-27},
  publisher = {{Elsevier}},
  abstract = {In this book, we study theoretical and practical aspects of computing methods for mathematical modelling of nonlinear systems. A number of computing techniques are considered, such as methods of operator approximation with any given accuracy; operator interpolation techniques including a non-Lagrange interpolation; methods of system representation subject to constraints associated with concepts of causality, memory and stationarity; methods of system representation with an accuracy that is the best within a given class of models; methods of covariance matrix estimation;methods for low-rank matrix approximations; hybrid methods based on a combination of iterative procedures and best operator approximation; andmethods for information compression and filtering under condition that a filter model should satisfy restrictions associated with causality and different types of memory.As a result, the book represents a blend of new methods in general computational analysis,and specific, but also generic, techniques for study of systems theory ant its particularbranches, such as optimal filtering and information compression.- Best operator approximation,- Non-Lagrange interpolation,- Generic Karhunen-Loeve transform- Generalised low-rank matrix approximation- Optimal data compression- Optimal nonlinear filtering},
  isbn = {978-0-08-095579-7},
  keywords = {Mathematics / Algebra / Linear,Mathematics / Calculus,Mathematics / Mathematical Analysis,Mathematics / Numerical Analysis,Technology & Engineering / Mechanical},
  langid = {english},
  pagetotal = {318}
}

@book{bertsekas_stochastic_1978,
  title = {Stochastic {{Optimal Control}}: {{The Discrete}}-{{Time Case}}},
  author = {Bertsekas, Dimitir P. and Shreve, Steven},
  date = {1978},
  publisher = {{Academic press}},
  abstract = {The book is a comprehensive and theoretically sound treatment of the mathematical foundations of stochastic optimal control of discrete-time systems, including the treatment of the intricate measure-theoretic issues.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\5JHEMU42\\book.pdf},
  isbn = {1-886529-03-5}
}

@article{briol_probabilistic_2017,
  title = {Probabilistic {{Integration}}: {{A Role}} in {{Statistical Computation}}?},
  shorttitle = {Probabilistic {{Integration}}},
  author = {Briol, François-Xavier and Oates, Chris J. and Girolami, Mark and Osborne, Michael A. and Sejdinovic, Dino},
  date = {2017-10-18},
  url = {http://arxiv.org/abs/1512.00933},
  urldate = {2020-02-20},
  abstract = {A research frontier has emerged in scientific computation, wherein numerical error is regarded as a source of epistemic uncertainty that can be modelled. This raises several statistical challenges, including the design of statistical methods that enable the coherent propagation of probabilities through a (possibly deterministic) computational work-flow. This paper examines the case for probabilistic numerical methods in routine statistical computation. Our focus is on numerical integration, where a probabilistic integrator is equipped with a full distribution over its output that reflects the presence of an unknown numerical error. Our main technical contribution is to establish, for the first time, rates of posterior contraction for these methods. These show that probabilistic integrators can in principle enjoy the "best of both worlds", leveraging the sampling efficiency of Monte Carlo methods whilst providing a principled route to assess the impact of numerical error on scientific conclusions. Several substantial applications are provided for illustration and critical evaluation, including examples from statistical modelling, computer graphics and a computer model for an oil reservoir.},
  archivePrefix = {arXiv},
  eprint = {1512.00933},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\439DVI6X\\Briol et al. - 2017 - Probabilistic Integration A Role in Statistical C.pdf;C\:\\Users\\pub\\Zotero\\storage\\PIGM8TG3\\1512.html},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{cockayne_bayesian_2019,
  title = {Bayesian {{Probabilistic Numerical Methods}}},
  author = {Cockayne, Jon and Oates, Chris and Sullivan, Tim and Girolami, Mark},
  date = {2019-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {61},
  pages = {756--789},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/17M1139357},
  url = {http://arxiv.org/abs/1702.03673},
  urldate = {2020-02-14},
  abstract = {The emergent field of probabilistic numerics has thus far lacked clear statistical principals. This paper establishes Bayesian probabilistic numerical methods as those which can be cast as solutions to certain inverse problems within the Bayesian framework. This allows us to establish general conditions under which Bayesian probabilistic numerical methods are well-defined, encompassing both non-linear and non-Gaussian models. For general computation, a numerical approximation scheme is proposed and its asymptotic convergence established. The theoretical development is then extended to pipelines of computation, wherein probabilistic numerical methods are composed to solve more challenging numerical tasks. The contribution highlights an important research frontier at the interface of numerical analysis and uncertainty quantification, with a challenging industrial application presented.},
  archivePrefix = {arXiv},
  eprint = {1702.03673},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\IYE9Y4P8\\Cockayne et al. - 2019 - Bayesian Probabilistic Numerical Methods.pdf;C\:\\Users\\pub\\Zotero\\storage\\X8T74HX4\\1702.html},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Methodology},
  number = {3}
}

@article{cockayne_probabilistic_2017,
  title = {Probabilistic {{Numerical Methods}} for {{Partial Differential Equations}} and {{Bayesian Inverse Problems}}},
  author = {Cockayne, Jon and Oates, Chris and Sullivan, Tim and Girolami, Mark},
  date = {2017-07-11},
  url = {http://arxiv.org/abs/1605.07811},
  urldate = {2020-02-18},
  abstract = {This paper develops a probabilistic numerical method for solution of partial differential equations (PDEs) and studies application of that method to PDE-constrained inverse problems. This approach enables the solution of challenging inverse problems whilst accounting, in a statistically principled way, for the impact of discretisation error due to numerical solution of the PDE. In particular, the approach confers robustness to failure of the numerical PDE solver, with statistical inferences driven to be more conservative in the presence of substantial discretisation error. Going further, the problem of choosing a PDE solver is cast as a problem in the Bayesian design of experiments, where the aim is to minimise the impact of solver error on statistical inferences; here the challenge of non-linear PDEs is also considered. The method is applied to parameter inference problems in which discretisation error in non-negligible and must be accounted for in order to reach conclusions that are statistically valid.},
  archivePrefix = {arXiv},
  eprint = {1605.07811},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\PDK3PR2J\\Cockayne et al. - 2017 - Probabilistic Numerical Methods for Partial Differ.pdf;C\:\\Users\\pub\\Zotero\\storage\\MIWRP8CG\\1605.html},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Methodology},
  primaryClass = {cs, math, stat},
  version = {3}
}

@thesis{damianou_deep_2015,
  title = {Deep {{Gaussian}} Processes and Variational Propagation of Uncertainty},
  author = {Damianou, Andreas},
  date = {2015},
  institution = {{University of Sheffield}},
  url = {http://etheses.whiterose.ac.uk/id/eprint/9968},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\JEWANA87\\Damianou - 2015 - Deep Gaussian processes and variational propagatio.pdf;C\:\\Users\\pub\\Zotero\\storage\\DKUPAECG\\9968.html}
}

@thesis{deisenroth_efficient_2010,
  title = {Efficient {{Reinforcement Learning}} Using {{Gaussian Processes}}},
  author = {Deisenroth, Marc Peter},
  date = {2010},
  institution = {{KIT Scientific Publishing}},
  url = {http://www.cs.washington.edu/research/projects/aiweb/media/papers/tmppqidj5},
  urldate = {2016-04-19},
  file = {C\:\\Users\\pub\\Zotero\\storage\\K5TE3E38\\deisenroth.pdf;C\:\\Users\\pub\\Zotero\\storage\\63UCNMSK\\books.html}
}

@inproceedings{deisenroth_pilco_2011,
  title = {{{PILCO}}: {{A}} Model-Based and Data-Efficient Approach to Policy Search},
  shorttitle = {{{PILCO}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on Machine Learning ({{ICML}}-11)},
  author = {Deisenroth, Marc and Rasmussen, Carl E.},
  date = {2011},
  pages = {465--472},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Deisenroth_323.pdf},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\YEDBHXGB\\Deisenroth and Rasmussen - 2011 - PILCO A model-based and data-efficient approach t.pdf}
}

@book{gauss_theoria_1809,
  title = {Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientium},
  author = {Gauss, Carl Friedrich},
  date = {1809},
  publisher = {{sumtibus Frid. Perthes et IH Besser}},
  url = {https://books.google.de/books?id=VKhu8yPcat8C},
  urldate = {2016-06-06},
  file = {C\:\\Users\\pub\\Zotero\\storage\\NEPIE6G8\\books.html}
}

@article{levine_reinforcement_2018,
  title = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}: {{Tutorial}} and {{Review}}},
  shorttitle = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}},
  author = {Levine, Sergey},
  date = {2018-05-20},
  url = {http://arxiv.org/abs/1805.00909},
  urldate = {2020-02-18},
  abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
  archivePrefix = {arXiv},
  eprint = {1805.00909},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\N8D2BMI9\\Levine - 2018 - Reinforcement Learning and Control as Probabilisti.pdf;C\:\\Users\\pub\\Zotero\\storage\\FN6PF6J5\\1805.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{murphy_machine_2012,
  title = {Machine {{Learning}}: {{A Probabilistic Perspective}}},
  shorttitle = {Machine {{Learning}}},
  author = {Murphy, Kevin P.},
  date = {2012-08-24},
  publisher = {{MIT Press}},
  abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package -- PMTK (probabilistic modeling toolkit) -- that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\FERQXL3E\\17.pdf},
  isbn = {978-0-262-01802-9},
  keywords = {Computers / Machine Theory,Textbook},
  langid = {english},
  pagetotal = {1098}
}

@article{oates_modern_2019,
  title = {A {{Modern Retrospective}} on {{Probabilistic Numerics}}},
  author = {Oates, C. J. and Sullivan, T. J.},
  date = {2019-01-14},
  url = {http://arxiv.org/abs/1901.04457},
  urldate = {2019-10-02},
  abstract = {This article attempts to place the emergence of probabilistic numerics as a mathematical-statistical research field within its historical context and to explore how its gradual development can be related both to applications and to a modern formal treatment. We highlight in particular the parallel contributions of Sul'din and Larkin in the 1960s and how their pioneering early ideas have reached a degree of maturity in the intervening period, mediated by paradigms such as average-case analysis and information-based complexity. We provide a subjective assessment of the state of research in probabilistic numerics and highlight some difficulties to be addressed by future works.},
  archivePrefix = {arXiv},
  eprint = {1901.04457},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\D67KFS6E\\Oates and Sullivan - 2019 - A Modern Retrospective on Probabilistic Numerics.pdf;C\:\\Users\\pub\\Zotero\\storage\\HUTII4YQ\\1901.html},
  keywords = {62-03; 65-03; 01A60; 01A65; 01A67,Mathematics - History and Overview,Mathematics - Numerical Analysis,Mathematics - Probability,Statistics - Machine Learning},
  primaryClass = {math, stat}
}

@article{petersen_matrix_2008,
  title = {The Matrix Cookbook},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind and others},
  date = {2008},
  journaltitle = {Technical University of Denmark},
  volume = {7},
  pages = {15},
  url = {http://www.cim.mcgill.ca/~dudek/417/Papers/matrixOperations.pdf},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\N4W86FFD\\imm3274.pdf}
}

@book{rasmussen_gaussian_2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward},
  date = {2006},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.86.3414},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\TFXHMXDE\\summary.html}
}

@book{scholkopf_learning_2002,
  title = {Learning with {{Kernels}}: {{Support Vector Machines}}, {{Regularization}}, {{Optimization}}, and {{Beyond}}},
  shorttitle = {Learning with {{Kernels}}},
  author = {Schölkopf, Bernhard and Smola, Alexander J.},
  date = {2002-01},
  publisher = {{MIT Press}},
  abstract = {In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -- -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.},
  isbn = {978-0-262-19475-4},
  keywords = {Computers / Intelligence (AI) & Semantics,Computers / Programming / General},
  langid = {english},
  pagetotal = {658}
}

@article{silver_mastering_2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  date = {2017-10},
  journaltitle = {Nature},
  volume = {550},
  pages = {354--359},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  url = {https://www.nature.com/articles/nature24270},
  urldate = {2019-04-04},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\9VNUJ3GJ\\Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf;C\:\\Users\\pub\\Zotero\\storage\\DLDP8I3H\\nature24270.html;C\:\\Users\\pub\\Zotero\\storage\\RE8QUYV6\\nature24270.html;C\:\\Users\\pub\\Zotero\\storage\\SBYNLYP2\\display.html;C\:\\Users\\pub\\Zotero\\storage\\ZW2KG8K4\\nature24270.html},
  langid = {english},
  number = {7676},
  options = {useprefix=true}
}

@thesis{snelson_flexible_2007,
  title = {Flexible and Efficient {{Gaussian}} Process Models for Machine Learning},
  author = {Snelson, Edward Lloyd},
  date = {2007},
  institution = {{Citeseer}},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.4041},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\T34X6F3F\\Snelson - 2007 - Flexible and efficient Gaussian process models for.pdf}
}

@inproceedings{snelson_sparse_2005,
  title = {Sparse {{Gaussian}} Processes Using Pseudo-Inputs},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Snelson, Edward and Ghahramani, Zoubin},
  date = {2005},
  pages = {1257--1264},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2005_543.pdf},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\6U2PZBFH\\Snelson and Ghahramani - 2005 - Sparse Gaussian processes using pseudo-inputs.pdf}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018},
  edition = {2},
  publisher = {{MIT Press}},
  location = {{Cambridge, Mass}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\6EQF4BV3\\Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf},
  isbn = {978-0-262-19398-6},
  keywords = {Reinforcement learning},
  langid = {english},
  pagetotal = {322},
  series = {Adaptive Computation and Machine Learning}
}

@inproceedings{titsias_variational_2009,
  title = {Variational {{Learning}} of {{Inducing Variables}} in {{Sparse Gaussian Processes}}.},
  booktitle = {{{AISTATS}}},
  author = {Titsias, Michalis K.},
  date = {2009},
  volume = {5},
  pages = {567--574},
  url = {http://www.jmlr.org/proceedings/papers/v5/titsias09a/titsias09a.pdf},
  urldate = {2017-04-06},
  file = {C\:\\Users\\pub\\Zotero\\storage\\UTMCPPXS\\Titsias - 2009 - Variational Learning of Inducing Variables in Spar.pdf}
}

@article{ziebart_modeling_2010,
  title = {Modeling {{Interaction}} via the {{Principle}} of {{Maximum Causal Entropy}}},
  author = {Ziebart, Brian D and Bagnell, J Andrew and Dey, Anind K},
  date = {2010},
  pages = {8},
  abstract = {The principle of maximum entropy provides a powerful framework for statistical models of joint, conditional, and marginal distributions. However, there are many important distributions with elements of interaction and feedback where its applicability has not been established. This work presents the principle of maximum causal entropy—an approach based on causally conditioned probabilities that can appropriately model the availability and influence of sequentially revealed side information. Using this principle, we derive models for sequential data with revealed information, interaction, and feedback, and demonstrate their applicability for statistically framing inverse optimal control and decision prediction tasks.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\KJJBGJPC\\Ziebart et al. - Modeling Interaction via the Principle of Maximum .pdf},
  langid = {english}
}


