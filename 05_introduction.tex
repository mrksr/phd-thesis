\chapter{Introduction}
\label{toc:introduction}
In many machine learning problems, the task is to derive a model for observational data that has been generated by some process in nature that is not fully understood.
Starting with an input-vector~$\mat{x}$ describing its initial state, nature produces an output-vector~$\mat{y}$.
Given a set of observational pairs $(\mat{x}, \mat{y})$, we want to learn about the underlying, possibly stochastic, functional dependency
\begin{align}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, 1);
        \node[random variable, observed] (x) at (0, 2) {$\mat{x}$};
        \node[random variable, observed] (y) at (0, 0) {$\mat{y}$};
        % ---
        \node[
            white box component,
            minimum height=42pt,
            text width=100pt,
            align=center,
            shape=rectangle,
        ] (nature) at (0, 1) {Nature};
        % ---
        \draw[edge, directed] (x) -- (nature);
        \draw[edge, directed] (nature) -- (y);
    \end{tikzpicture}
     &   &
    \mat{y}
     & =
    f(\mat{x}).
\end{align}
The goal of learning is often to either gain new knowledge about nature by describing the data or to be able to predict outputs $\mat{y}_\ast$ for previously unseen inputs $\mat{x}_\ast$.

\begin{itemize}
    \item There are two approaches to formalizing such a functional dependency: Data-driven and model-driven.
    \item They are associated with machine learning for data and statistics for models.
    \item Both have pros and cons.
\end{itemize}

In many machine learning problems, the task is to derive a model for observations of some process in nature that is not fully understood.
While traditional statistics favor the white-box modeling culture that focuses on explaining observational data using causal theoretic models, machine learning is more closely associated with the black-box culture~\parencite{breiman_statistical_2001,shmueli_explain_2010}.
When formulating a black-box model, we accept that we cannot model nature directly and instead focus on the functional dependency between inputs and outputs.
Using general function approximators like neural networks or Gaussian processes, any smooth dependency can be modeled given enough observations.
With growing computational power and the availability of data sets, black-box approaches have led to impressive advancements in domains that are hard to formalize.
Successes have been seen in a wide range of digital tasks such as speech recognition~\parencite{hochreiter_long_1997,chorowski_attention-based_2015,bahdanau_neural_2014,chorowski_attention-based_2015}, computer vision~\parencite{russakovsky_imagenet_2015,lecun_backpropagation_1989}, non-cooperative games~\parencite{berner_dota_2019,silver_mastering_2016}, or machine translation~\parencite{johnson_googles_2017}.
The abundance of data in these domains shifts the focus away from traditional statistical models to approaches that are highly adaptabe and scalable.


\section{Black-box models in deep learning}
\begin{itemize}
    \item Highly scalable models can fit to large amounts of data
    \item Neural networks are at the core of deep learning and inherently non-interpretable.
    \item You can get more principled with BNNs, but you can never make them white-box.
\end{itemize}

The highly adaptable and scalable models that lead to success in digital domains are not well-suited to be deployed in safety-critical environments.
Large neural networks generally suffer from low interpretability~\parencite{rudin_stop_2019}, the need for large amounts of data and their inability to quantify uncertainties~\parencite{goodfellow_deep_2016}.
In this work, we explore how to formulate machine learning models that can satisfy the requirements of physical environments.

The black-box culture is more closely associated with machine learning~\parencite{mitchell_machine_1997,goodfellow_deep_2016}.
When formulating a black-box model, we accept that we cannot model nature directly and instead focus on the functional dependency $f$ between inputs and outputs.
\begin{align}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, 1);
        \node[random variable, observed] (x) at (0, 2) {$\mat{x}$};
        \node[random variable, observed] (y) at (0, 0) {$\mat{y}$};
        % ---
        \node[
            black box component,
            minimum height=42pt,
            text width=100pt,
            align=center,
            shape=rectangle,
        ] (nature) at (0, 1) {Neural network};
        % ---
        \draw[edge, directed] (x) -- (nature);
        \draw[edge, directed] (nature) -- (y);
    \end{tikzpicture}
     &   &
    \mat{y}
     & =
    f_{\mat{w}}(\mat{x}).
\end{align}
Since one can show that many algorithms including neural networks are general approximators for smooth functions~\parencite{rasmussen_gaussian_2006}, given enough observations, they are expected to be able to approximate $f$ as well.
Since a sufficiently powerful approximator can explain any observation set, a black-box model can no longer be evaluated by how well it explains the training data.
Many functional dependencies can be used to generate a finite set of observations and since a black-box model does not have a scientific basis, it is hard to judge if the true process has been uncovered.
Instead, the performance of a black-box model is typically measured in terms of its ability to predict a test set of previously unseen data.
With growing computational power and the availability of large data sets, black-box approaches have led to impressive advancements in domains that are hard to formalize such as image recognition tasks~\parencite{lecun_backpropagation_1989} or playing the game of Go~\parencite{silver_mastering_2016}.
Since black-box models do not claim to uncover or represent knowledge about the true process in nature, their internal structure is generally not human-interpretable and their behavior is not well-understood.

Many successful approaches in machine learning can be interpreted as structured models.
A well-known example is the convolutional neural network (CNN) used in computer vision tasks~\parencite{lecun_backpropagation_1989}.
Instead of, for example, using a standard multi-layer perceptron with sigmoid activation functions, the structure of a CNN is inspired by high-level knowledge about vision tasks.
Recognizing the importance of edge-detection and scale-invariance, a CNN consists of a stack of layers that implement a potentially large set of convolution kernels.
In contrast to earlier (more white-box) approaches to vision such as SIFT-features~\parencite{lowe_object_1999}, the parameters of the convolutions are learned from the data, thereby introducing a black-box component.
Another application of structured models can be found in natural language processing.
Formulating a white-box model for human language is prohibitively difficult.
However, insights from formal language theory~\parencite{chomsky_aspects_2014} can be used to formulate abstract expectations about the interaction of words in any language.
These insights lead to concepts such as the long short-term memory (LSTM)~\parencite{hochreiter_long_1997} or the attention mechanism~\parencite{bahdanau_neural_2014,chorowski_attention-based_2015}.

\begin{Pros}[Deep learning]
    \begin{compactdesc}
        \item[Universal approximation] asd
        \item[Strong scalability] asd
        \item[Potential for automation] asd
    \end{compactdesc}
\end{Pros}

\begin{Cons}[Deep learning]
    \begin{compactdesc}
        \item[Strong data bias] asd
        \item[Weak interpretability] asd
        \item[Complex parametrization] asd
        \item[High data requirements] asd
        \item[Unsafe generalization] asd
    \end{compactdesc}
\end{Cons}


\section{White-box models in Bayesian statistics}
\begin{itemize}
    \item White-box models are founded on a theoretical understanding of nature
    \item Inference comes down to a small number of parameters
    \item Why not other approaches like algorithms or rule-based systems?
    \item You can get more general with Gaussian processes, but at some point they are just black-box.
\end{itemize}

\begin{align}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, 1);
        \node[random variable, observed] (x) at (0, 2) {$\mat{x}$};
        \node[random variable, observed] (y) at (0, 0) {$\mat{y}$};
        % ---
        \node[white box component, latent] (a) at (-0.75, 1.25) {};
        \node[white box component, hyperparameter] (b) at (0.75, 1.25) {};
        \node[white box component, variational] (c) at (0, 0.75) {};
        \node[white box component, ] (d) at (-0.75, 0.75) {};
        % ---
        \draw[edge, directed] (x) -- (a);
        \draw[edge, directed] (x) -- (b);
        \draw[edge, directed] (a) -- (c);
        \draw[edge, directed] (b) -- (c);
        \draw[edge, directed] (d) -- (c);
        \draw[edge, directed] (c) -- (y);
        \begin{scope}[on background layer]
            \node[
                white box,
                fit=(a)(b)(c)(d),
            ] {};
        \end{scope}
    \end{tikzpicture}
     &   &
     \Prob*{\mat{y} \given \mat{x}}
     & =
    \begin{aligned}
        & \int
        \Prob*{
            \mat{y},
            \whiteboxblob{latent},
            \whiteboxblob{hyperparameter},
            \whiteboxblob{variational},
            \whiteboxblob{}
            \given \mat{x}
        }
        \diff\whiteboxblob{latent}
        \diff\whiteboxblob{hyperparameter}
        \diff\whiteboxblob{variational}
        \diff\whiteboxblob{}
    \end{aligned}
\end{align}

Traditional statistics favor white-box models which place a focus on describing and understanding observational data~\parencite{andrew_gelman_bayesian_2013,casella_statistical_2002,cox_planning_1958}.
In this worldview, data analysis is based on a stochastic model of nature that can explain how the data has been generated in great detail.
Such a model typically has a small number of unknown parameters whose values are estimated from the data.
Based on the scientific understanding of nature, the process of interest is factorized into smaller components that are modeled separately and that interact to jointly explain the data.
Crucially, all components of a white-box model are designed by humans and are white-box as well to ensure an interpretable result.
Since white-box models represent a scientific understanding of nature, a model can be evaluated by how well it can describe the available observations:
A simple model with few parameters which is able to explain observations is expected to approximate the true process in nature reasonably well, thereby implementing Occam's razor~\parencite{thorburn_occams_1915}.
White-box models have been the driving force behind many advances in economics, medicine, and the natural sciences and allowed researchers to find and understand the structure in complex and heterogenous data~\parencite{efron_modern_2005,giulio_d._agostini_bayesian_2003}.
However, as processes get more complex and the amount of available data grows, formulating white-box models becomes a very difficult task~\parencite{sutton_bitter_2019}.

A key technique that allows us to cope with the requirements of critical systems in the physical world is probabilistic modeling.
Bayesian inference offers a framework to formalize expert knowledge in prior assumptions and combine them with potentially uncertain data to represent knowledge about a system.
Using Bayes' rule, a distribution of plausible models is reweighed by their ability to explain observed data to yield a posterior distribution over plausible explanations.
To make predictions, all plausible models are taken into account, allowing us to both quantify confidence in predictions and take more unlikely but relevant scenarios into consideration.

Bayesian reasoning was pioneered by Pierre-Simon Laplace in the eighteenth century.
Laplace extended the prevalent mechanistic world-view held by scholars like Newton with the idea that knowledge can be imperfect or incomplete.
Laplace used Bayesian inference to formalize scientific reasoning under uncertainty and to enable the integration of multiple sources of data of varying quality.
Based on white-box models grounded in scientific understanding, this reasoning, for example, led to improved estimates of the masses of Jupiter and Saturn and the discovery of the uneven proportion of male and female human births~\parencite{pulskamp_laplace_2020,laplace_theorie_1820}.
Modern science has moved beyond phenomena that can be explained by the deterministic world-view held by Newton and Laplace towards subjects where randomness plays a major role.
Large-scale experiments in fields such as particle physics~\parencite{khachatryan_observation_2015} or astronomical observations~\parencite{collaboration_first_2019,the_ligo_scientific_collaboration_observation_2016} uncover scientific knowledge hidden in a large number of random events.
However, These analyses are still based on white-box modelling approaches to ensure that results conform to the requirements of scientific rigor.


\begin{Pros}[Bayesian statistics]
    \begin{compactdesc}
        \item[Trustworthy predictions] asd
        \item[Strong interpretability] asd
        \item[Safe generalization] asd
    \end{compactdesc}
\end{Pros}
\begin{Cons}[Bayesian statistics]
    \begin{compactdesc}
        \item[Strong model bias] asd
        \item[Subjective model-selection] asd
        \item[Weak scalability] asd
    \end{compactdesc}
\end{Cons}


\section{Structured models with Gaussian processes}

\begin{table}[t]
    \caption[Research questions]{
        \label{tab:introduction:questions}
        Research questions
    }
    \centering
    \newcolumntype{Y}{>{\centering\arraybackslash}X}
    \newcommand{\yes}{✔}
    \newcommand{\no}{\textcolor{sRedLight}{✘}}
    \newcommand\tabnode[2]{%
        \tikz[remember picture, baseline] \node[inner sep=0pt, anchor=base] (#1) {#2};
    }
    \begin{tabularx}{.9\textwidth}{lYY}
        \toprule
                                 & Deep learning    & Bayesian statistics \\
        \midrule
        Data-driven insights     & \yes             & \tabnode{a}{\no}    \\
        Strong scalability       & \yes             & \tabnode{b}{\no}    \\
        \addlinespace
        Interpretable results    & \tabnode{c}{\no} & \yes                \\
        Trustworthy predictions  & \tabnode{d}{\no} & \yes                \\
        \addlinespace
        Semantic model-selection & \tabnode{f}{\no} & \tabnode{e}{\no}    \\
        \bottomrule
    \end{tabularx}
    \begin{tikzpicture}[remember picture, overlay]
        \tikzstyle{question} = [
        draw=sGreenDark, fill=sGreenDark,
        fill opacity=.2, very thick,
        inner xsep=6pt, inner ysep=3pt,
        ]
        \node[
            question,
            fit=(a)(b),
            label={[sGreenDark, anchor=west]east:RQ1}
        ] {};
        \node[
            question,
            fit=(c)(d),
            label={[sGreenDark, anchor=east]west:RQ2}
        ] {};
        \node[
            question,
            fit=(e)(f),
            label={[sGreenDark, anchor=west]east:RQ3}
        ] {};
    \end{tikzpicture}
\end{table}
However, bridging the gap to applications in critical systems in the physical world has proved challenging.
Problem domains like robotics or industrial control introduce a new set of requirements.
In industrial control, machine learning is used to find more efficient or safer control strategies not obvious to the engineers designing a machine~\parencite{hein_benchmark_2017}.
In such scenarios, relevant data are often scarce.
The most valuable data, such as when a machine will fail or how it will behave in new situations, is never produced.
As applications become more safety-relevant, gathering data through exploration can be problematic due to the adverse consequences of failure.
If they are to operate in safety-critical areas, interact with people, or carry responsibility, machine learning systems must be robust, trustworthy, and assessable.
Besides optimizing for good average-case performance, a responsible system needs to reason about plausible worst-cases to be able to avoid them.
To ensure trust, ML-systems often need to be verified by domain experts before deployment, emphasizing a need for interpretability.
Models are required to incorporate and reproduce expert knowledge and make consistent predictions, while at the same time yielding new insights from data.

In this work, we will study what we call structured models.
Structured models exist between these extremes as they are designed based on our scientific understanding of the underlying problem.
At the same time, we accept that there are problem-components we do not or cannot understand and use general function approximators to model these components.
The goal of the structured modeling approach is to be more data-efficient compared to black-box models because there is less knowledge that needs to be inferred from data.
At the same time, imposing structure allows us to characterize desired solutions and formulate what we want to learn from data.
As a result, structured models can be formulated to gain insights about poorly understood system components from data.

This dissertation explores how advances in black-box machine learning can be integrated in Bayesian reasoning to futher relax the mechanistic world-view and allow us to tackle problems where white-box models are not available.
For applications of data-centric approaches in the physical world, expert knowledge is often based on an intuitive understanding of the underlying physics leading to coarse expectations about system-behavior on different layers of abstraction.
However, fully white-box models are often too complex to formulate or computationally intractable.
We will discuss how abstract knowledge can be exploited to formulate structured Bayesian models that reproduce expectations but are still able to learn from data and explore two approaches to adding such structure.
First, white-box mathematical structure can be relaxed to a structured hierarchical system to learn new insights from data.
And second, structure motivated by expert-understanding of an underlying physical process can be used to formulate semantic constraints to a data-centric black-box approach.

\begin{RQs}
    \begin{compactdesc}
        \item[RQ1:] How can we relax the model-bias of white-box models and efficiently learn from data?
        \item[RQ2:] How can we reduce the data-bias of black-box models and reliably reproduce expert expectations?
        \item[RQ3:] How can we improve model selection in situations where internal model structure and generalization behavior come into focus?
    \end{compactdesc}
\end{RQs}

\paragraph{RQ1: How can we relax the model-bias of white-box models and efficiently learn from data?}
We explore how to embed general function approximators in Bayesian probabilistic models to enforce structure and learn from data.
We discuss how to formulate inference schemes based on composite and hierarchical Gaussian process models that yield informative uncertainties.

\paragraph{RQ2: How can we reduce the data-bias of black-box models and reliably reproduce expert expectations?}
We study how to use structured models to reproduce expert knowledge and factorize uncertainties to achieve interpretability.
We explore how rich internal structure helps models to generalize to unseen situations.

\paragraph{RQ3: How can we improve model selection in situations where internal model structure and generalization behavior come into focus?}
The black-box and white-box modelling cultures follow different approaches to evaluating models.
While a white-box model is evaluated through a combination of goodness-of-fit measures on observations used during inference and subjective measures based on interpretability, black-box approaches typically focus on performance measures on test-sets.
As evaluation data in critical parts of the system is often scarce in physical environments, performance measures such as the generalization error are not enough to identify a desirable structured model.
To formalize the subjectiveness in the description of desirable models, we will explore how downstream tasks can be taken into account for model selection.


\section{Contributions}
This work presents novel Bayesian structured models inspired by industrial applications.
Our main contributions are as follows:
\begin{compactitem}
    \item We formulate fully Bayesian interpretations of the data association problem and nonlinear timeseries alignment.
    \item We formulate efficient and scalable inference schemes for these models by extending the variational inference schemes for deep Gaussian processes to handle multi-output GPs and discrete structure.
    \item We present a Bayesian view on model-based reinforcement learning and formulate an efficient probabilistic policy search algorithm for structured dynamics models.
    \item We discuss the limitations of current variational inference schemes for hierarchical Gaussian process models.
    \item We argue why surrogate models with suboptimal marginal likelihoods can perform well in Bayesian optimization problems.
\end{compactitem}

The main contributions presented in this thesis appeared in a number of peer-reviewed publications and patents we outline below.
In \cref{contrib:data_association,contrib:patent_depeweg,contrib:patent_geipel}, we formulate a Bayesian interpretation of the data association problem and discuss an application of this model to the prediction of the combustion dynamics of a gas-turbine, where the structured model allows us to distinguish between different dynamical regimes.
In~\cref{contrib:windpower,contrib:patent_egedal,contrib:patent_kaiser}, we formulate a hierarchical Bayesian model for a nonlinear time-series alignment problem, show that this model is capable of representing the complex interactions between turbines in a wind farm and discuss how this model can be used to derive more efficient controllers for wind-turbines in.
In \cref{contrib:interpretable_rl,contrib:multi_modal_rl}, we apply our data association model to a reinforcement learning task and show how a semantic decomposition of the dynamics reduces the data requirements and produces interpretable solutions.
In \cref{contrib:surrogates}, we formulate surrogate models for Bayesian optimization problems that focus on the informative structure of the objective functions by sacrificing local accuracy and in \cref{contrib:composition}, we present an argument for why inference schemes based on factorizations between layers cannot represent heterogeneous posteriors.


\subsubsection{Own publications}
\begin{enumerate}
    \item \label[contribution]{contrib:windpower}\fullfullcite{kaiser_bayesian_2018}
    \item \label[contribution]{contrib:data_association}\fullfullcite{kaiser_data_2019}
    \item \label[contribution]{contrib:interpretable_rl}\fullfullcite{kaiser_interpretable_2019}
    \item \label[contribution]{contrib:multi_modal_rl}\fullfullcite{kaiser_bayesian_2020}
    \item \label[contribution]{contrib:surrogates}\fullfullcite{bodin_modulating_2020}
    \item \label[contribution]{contrib:composition}\fullfullcite{ustyuzhaninov_compositional_2020}
\end{enumerate}

\subsubsection{Patents}
\begin{enumerate}
    \setcounter{enumi}{6}
    \item \label[contribution]{contrib:patent_egedal}\fullfullcite{egedal_verfahren_2019}
    \item \label[contribution]{contrib:patent_kaiser}\fullfullcite{kaiser_verfahren_2019}
    \item \label[contribution]{contrib:patent_geipel}\fullfullcite{geipel_transferlernen_2020}
    \item \label[contribution]{contrib:patent_depeweg}\fullfullcite{depeweg_computer_2020}
\end{enumerate}


\section{Thesis outline}
The content of this thesis is structured as follows.
In \cref{toc:gp}, we provide a short introduction to Bayesian machine learning and Bayesian nonparametrics with a focus on Gaussian processes (GPs).
We discuss the connection between statistical learning theory and Bayesian inference and their application to hierarchical models.
After introducing GPs, we discuss variational approaches to sparse GPs, their extension to hierarchical GP models, and efficient inference schemes.

In \cref{toc:data_association}, we explore how to formulate structured hierarchical models using the data association problem as an example.
We separate the data association problem into hierarchical components and derive an efficient joint inference scheme, resulting in a fully Bayesian model.
We show that this model is able to factorize multi-modal data into independent processes, providing explicit models for both the processes and assignment probabilities.
Comparisons with previous models show the additional qualitative model capabilities of the structured approach and competitive black-box performance.
However, experiments also show that standard measures are not enough to identify desirable models.

In \cref{toc:alignment}, we formulate a structured Bayesian model that reproduces expert knowledge about wind propagation in a wind-farm.
We interpret the problem of modeling the power production of multiple wind-turbines as a nonlinear alignment problem and derive an efficient inference scheme.
Based on expert knowledge about the underlying latent and turbulent wind field, we can derive a structured model that enforces a physically plausible dependency structure between multiple time-series.
We discuss how these constraints help to solve an otherwise highly ambiguous learning problem and show that the imposed structure leads to a rich internal model-structure that can be interpreted by experts.

In \cref{toc:interpretable_rl}, we consider how to formalize the subjectiveness introduced via expert knowledge and model ambiguities to evalute structured models.
We include the task a model will be used to solve into model selection, which allows us to distinguish between qualitatively different models showing similar performance metrics.
We revisit the data association problem and embed it into a reinforcement learning problem, where identifying the correct underlying dynamics, and therefore a desirable model, is critical to finding a successful policy.
We show that semantic hierarchical structure increases data efficiency and allows domain experts to influence agent behavior through detailed insights into the dynamics of a system.

\Cref{toc:discussion} concludes the work presented in this thesis and interprets the results in a broader context by further exploring the properties and evaluation of structured hierarchical models.
In \cref{toc:discussion:composition}, we present an intuitive argument for why inference schemes based on factorizations between layers cannot represent heterogeneous posteriors.
In \cref{toc:discussion:bo}, we argue why models with suboptimal marginal likelihoods can perform well in hierarchical systems.
In \cref{toc:discussion:mountaincar}, we explore this idea further and consider how tasks can be included in the inference problem directly.
Finally, we discuss possible further directions of research.
