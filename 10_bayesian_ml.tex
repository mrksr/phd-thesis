\chapter{Bayesian Machine Learning}
\label{toc:bayesian_ml}

\begin{itemize}
    \item Machine Learning is an extension of algorithmics (and numerics) with different goals
    \item In algorithmics the goal is usually clear and well-defined
    \item Use polynomial interpolation as an example.
          We can use that for curve-fitting, but we generally do not associate it with machine learning because the goal is not to minimize some loss function but rather to find a uniquely defined solution.
          The "ML-variant" of polynomial interpolation would be generalized linear models with polynomial basis functions and there, the correct answer is much less clear. (PCA, "least squares" quadratic fit, etc)
    \item In ML, an essential element is a lack of information and our freedom to handle that lack of information. A subjective \emph{goal} is integral to machine learning. ML is about making the best of the situation and is inherently subjective. And inherently uncertain.
    \item The lack of information is generally introduced via finite information for an infinite object
    \item We can formulate this situation nicely in terms of probability distributions
          \begin{itemize}
              \item A ML problem is an inference problem
              \item We always care about some kind of joint distribution
              \item Depending on which structure we assume, different branches of ML arise
              \item The two main strategies to formulate finiteness (parameters and data) give rise to the (non-)parametric split
          \end{itemize}
    \item Things like learning theory try for objectivity but that does not scale
    \item In practice, ML is about the step from risk minimization to empirical risk minimization and ways to handle the annoying results
    \item We get back to global arguments via encoding knowledge in
          \begin{itemize}
              \item information operator
              \item hypothesis space
              \item loss function
              \item optimization algorithm
          \end{itemize}
    \item Turns out the problem statement and hypothesis space (same thing?) are generally preferred
    \item Bayesian ML then offers a nice language to formulate such hypotheses
    \item Maybe insert the core probabilistic numerics argument about algorithms here?
    \item Most importantly, Bayes' rule offers a consistent and simple way to combine local and global assumptions that does not break probability theory.
          And has a nice semantic interpretation.
          It is not a unique solution though.
    \item With Bayes' rule, a structurally simple \emph{learning algorithm} can be formulated where the complexity is in the hypothesis space (prior) and loss function (likelihood). No optimization needed at first glance.
    \item Turns out this is a nice fit for problems in the physical word which has a few notable properties:
          \begin{itemize}
              \item Inevitability of uncertainty
              \item Availability of knowledge
              \item Need for interpretability
          \end{itemize}
    \item Discuss the differences between Web ML and Industrial ML
    \item Industrial ML motivates our thesis, where we want to formulate models that are inherently interpretable and trustworthy
    \item It will turn out that in this space, judging whether a model is good is actually very tricky.
\end{itemize}
One of the roots of machine learning (ML) lies in the study of algorithms in theoretical computer science.
An algorithm is a well-defined sequence of computational steps transforming a set of inputs to a set of outputs.
It is a tool for solving a computational problem, which is defined by an abstract definition of admissable inputs and expected outputs.
An algorithm solves such a problem if for every possible input, the algorithm provably yields the correct output.

Consider the computational problem of sorting a list of numbers in ascending order.
One possible definition~\parencite{cormen_introduction_2009} of the sorting problem is:
\begin{labeling}{Output:}
    \item[Input:] A sequence of $n$ integers $\mat{I} = (i_1, \dots, i_n)$
    \item[Output:] A reordering of $\mat{I}$ called $\mat{O} = (o_1, \dots, o_n)$ such that $o_1 \leq \dots \leq o_n$.
\end{labeling}
For example, for the input $\mat{\hat{I}}=(12, 8, 23, 4)$ the correct output is $\mat{\hat{O}}=(4, 8, 12, 23)$.
A concrete input $\mat{\hat{I}}$ is called an instance of a problem.
Importantly, such an instance contains all the required information to compute the unique output $\mat{\hat{O}}$.
The correctness of the output can be checked via the formal problem definition.
The various available (correct) sorting algorithms only differ in which and how many computational steps they take to arrive at the output, not in the output itself.

Machine learning can be seen as an extension of algorithmics towards problems where a formal description of a uniquely defined solution does not exist.
Instead, problems in ML are characterized by the observation of finitely many examples or data.
The objective is to derive knowledge about how these examples were generated.
This knowledge can then be used to describe common patterns in the data or generate new examples that conform to previous observations.

\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_data}
        \caption{
            Data
            \label{fig:bayesian_ml:polynoms:data}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_lagrange}
        \caption{
            \enquote{Algorithms}
            \label{fig:bayesian_ml:polynoms:lagrange}
        }
    \end{subfigure}
    \caption[Algorithmic Interpolation]{
        ???
    }
\end{figure}

As an example, consider the data shown in \cref{fig:bayesian_ml:polynoms:data}.
The figure depicts a dataset $\Dc = \Set*{\left(x_n, y_n\right)}_{n=1}^N \subseteq \Rb^2$ of $N$ pairs of real numbers.

\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_linear}
        \caption{
            Linear ML
            \label{fig:bayesian_ml:polynoms:linear}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_polynomial}
        \caption{
            Polynomial ML
            \label{fig:bayesian_ml:polynoms:polynomial}
        }
    \end{subfigure}
    \caption[ML Interpolation]{
        ???
    }
\end{figure}
