\chapter{Structure from Composite Problems}
\label{toc:structure_from_composite_problems}


\section{Semantic Hierarchies}
\label{toc:semantic_hierarchies}
?


\section{Data Association}
\label{toc:data_association:data_association}
The data association problem is concerned with separating data coming from different generating processes, for example when data comes from different data sources, contain significant noise, or exhibit multimodality.
We present a fully Bayesian approach to this problem.
Our model is capable of simultaneously solving the data association problem and the induced supervised learning problem.
Underpinning our approach is the use of Gaussian process priors to encode the structure of both the data and the data associations.
We present an efficient learning scheme based on doubly stochastic variational inference and discuss how it can be applied to deep Gaussian process priors.


\subsection{Introduction}
\label{toc:data_association:introduction}
Real-world data often include multiple operational regimes of the considered system, for example a wind turbine or gas turbine~\parencite{hein_benchmark_2017}.
As an example, consider a model describing the lift resulting from airflow around the wing profile of an airplane as a function of the attack angle.
At low values the lift increases linearly with the attack angle until the wing stalls and the characteristic of the airflow changes fundamentally.
Building a truthful model of such data requires learning two separate models and correctly associating the observed data to each of the dynamical regimes.
A similar example would be if our sensors that measure the lift are faulty in a manner such that we either get an accurate reading or a noisy one.
Estimating a model in this scenario is often referred to as a \emph{data association problem}~\parencite{barshalom_tracking_1990,cox_review_1993}, where we consider the data to have been generated by a mixture of processes and we are interested in factorising the data into these components.

\cref{fig:data_association:choicenet_data} shows an example of faulty sensor data, where sensor readings are disturbed by uncorrelated and asymmetric noise.
Applying standard machine learning approaches to such data can lead to model pollution, where the expressive power of the model is used to explain noise instead of the underlying signal.
Solving the data association problem by factorizing the data into signal and noise gives rise to a principled approach to avoid this behavior.

\begin{figure}[t]
    \centering
    \includestandalone{figures/choicenet_data_intro}
    \caption{
        \label{fig:data_association:choicenet_data}
        A data association problem consisting of two generating processes, one of which is a signal we wish to recover and one is an uncorrelated noise process.
    }
\end{figure}
Early approaches to explaining data using multiple generative processes are based on separating the input space and training local expert models explaining easier subtasks~\parencite{jacobs_adaptive_1991,tresp_mixtures_2001, rasmussen_infinite_2002}.
The assignment of data points to local experts is handled by a gating network, which learns a function from the inputs to assignment probabilities.
However, it is still a central assumption of these models that at every position in the input space exactly one expert should explain the data.
Another approach is presented in~\parencite{bishop_mixture_1994}, where the multimodal regression tasks are interpreted as a density estimation problem.
A high number of candidate distributions is reweighed to match the observed data without modeling the underlying generative process.

In contrast, we are interested in a generative process, where data at the same location in the input space could have been generated by a number of global independent processes.
Inherently, the data association problem is ill-posed and requires assumptions on both the underlying functions and the association of the observations.
In~\parencite{lazaro-gredilla_overlapping_2012} the authors place Gaussian process (GP) priors on the different generative processes which are assumed to be relevant globally.
The associations are modelled via a latent association matrix and inference is carried out using an expectation maximization algorithm.
This approach takes both the inputs and the outputs of the training data into account to solve the association problem.
A drawback is that the model cannot give a posterior estimate about the relevance of the different generating processes at different locations in the input space.
This means that the model can be used for data exploration but additional information is needed in order to perform predictive tasks.
Another approach in~\parencite{bodin_latent_2017} expands this model by allowing interdependencies between the different generative processes and formulating the association problem as an inference problem on a latent space and a corresponding covariance function.
However, in this approach the number of components is a free parameter and is prone to overfitting, as the model has no means of turning off components.

In this paper, we formulate a Bayesian model for the data association problem.
Underpinning our approach is the use of GP priors which encode structure both on the functions and the associations themselves, allowing us to incorporate the available prior knowledge about the proper factorization into the learning problem.
The use of GP priors allows us to achieve principled regularization without reducing the solution space leading to a well-regularized learning problem.
Importantly, we simultaneously solve the association problem for the training data taking both inputs and outputs into account while also obtaining posterior belief about the relevance of the different generating processes in the input space.
Our model can describe non-stationary processes in the sense that a different number of processes can be activated in different locations in the input space.
We describe this non-stationary structure using additional GP priors which allows us to make full use of problem specific knowledge.
This leads to a flexible yet interpretable model with a principled treatment of uncertainty.

The paper has the following contributions:
In \cref{toc:data_association:model}, we propose the data association with Gaussian processes model (DAGP).
In \cref{toc:data_association:variational_approximation}, we present an efficient learning scheme via a variational approximation which allows us to simultaneously train all parts of our model via stochastic optimization and show how the same learning scheme can be applied to deep GP priors.
We demonstrate our model on a noise separation problem, an artificial multimodal data set, and a multi-regime regression problem based on the cart-pole benchmark in \cref{toc:data_association:experiments}.


\subsection{Data Association with Gaussian Processes}
\label{toc:data_association:model}
\begin{figure}[t]
    \centering
    \includestandalone{figures/data_association_graphical_model}
    \caption{
    \label{fig:data_association:data_association_graphical_model}
    The graphical model of DAGP.
    The violet observations $(\mat{x_n}, \mat{y_n})$ are generated by the latent process (green).
    Exactly one of the $K$ latent functions $f^{\pix{k}}$ and likelihood $\mat{y_n^{\pix{k}}}$ are evaluated to generate $\mat{y_n}$.
    We can place shallow or deep GP priors on these latent function values $\mat{f_n^{\pix{k}}}$.
    The assignment $\mat{a_n}$ to a latent function is driven by input-dependent weights $\mat{\alpha_n^{\pix{k}}}$ which encode the relevance of the different functions at $\mat{x_n}$.
    The different parts of the model are determined by the hyperparameters $\mat{\theta}, \mat{\sigma}$ (yellow) and variational parameters $\mat{u}$ (blue).
    }
\end{figure}
The data association with Gaussian processes (DAGP) model assumes that there exist $K$ independent functions $\Set*{f^{\pix{k}}}_{k=1}^K$, which generate pairs of observations $\Dc = \Set*{(\mat{x_n}, \mat{y_n})}_{n=1}^N$.
Each data point is generated by evaluating one of the $K$ latent functions and adding Gaussian noise from a corresponding likelihood.
The assignment of the $\nth{n}$ data point to one of the functions is specified by the indicator vector $\mat{a_n} \in \Set*{0, 1}^K$, which has exactly one non-zero entry.
Our goal is to formulate simultaneous Bayesian inference on the functions $f^{\pix{k}}$ and the assignments $\mat{a_n}$.

For notational conciseness, we follow the GP related notation in \parencite{hensman_scalable_2015} and collect all $N$ inputs as $\mat{X} = \left(\mat{x_1}, \ldots, \mat{x_N}\right)$ and all outputs as $\mat{Y} = \left(\mat{y_1}, \ldots, \mat{y_N}\right)$.
We further denote the $\nth{k}$ latent function value associated with the $\nth{n}$ data point as $\rv{f_n^{\pix{k}}} = \Fun{f^{\pix{k}}}{\mat{x_n}}$ and collect them as $\mat{F^{\pix{k}}} = \left( \rv{f_1^{\pix{k}}}, \ldots, \rv{f_N^{\pix{k}}} \right)$ and $\mat{F} = \left( \mat{F^{\pix{1}}}, \ldots, \mat{F^{\pix{K}}} \right)$.
We refer to the $\nth{k}$ entry in $\mat{a_n}$ as $a_n^{\pix{k}}$ and denote $\mat{A} = \left(\mat{a_1}, \ldots, \mat{a_N}\right)$.

Given this notation, the marginal likelihood of DAGP can be separated into the likelihood, the latent function processes, and the assignment process and is given by,
\begin{align}
    \begin{split}
        \label{eq:data_association:true_marginal_likelihood}
        \Prob*{\mat{Y} \given \mat{X}} &=
        \int
        \Prob*{\mat{Y} \given \mat{F}, \mat{A}}
        \Prob*{\mat{F} \given \mat{X}}
        \Prob*{\mat{A} \given \mat{X}}
        \diff \mat{A} \diff \mat{F} \\
        \Prob*{\mat{Y} \given \mat{F}, \mat{A}} &=
        \prod_{n=1}^N\prod_{k=1}^K
        \Gaussian*{\mat{y_n} \given \mat{f_n^{\pix{k}}}, \left(\sigma^{\pix{k}}\right)^2}_{^{\displaystyle,}}^{\Fun{\Ind}{a_n^{\pix{k}} = 1}}
    \end{split}
\end{align}
where $\sigma^{\pix{k}}$ is the noise of the $\nth{k}$ Gaussian likelihood and $\Ind$ is the indicator function.

Since we assume the $K$ processes to be independent given the data and assignments, we place independent GP priors on the latent functions
$\Prob*{\mat{F} \given \mat{X}} = \prod_{k=1}^K \Gaussian*{\mat{F^{\pix{k}}} \given \Fun*{\mu^{\pix{k}}}{\mat{X}}, \Fun*{\K^{\pix{k}}}{\mat{X}, \mat{X}}}$ with mean function $\mu^{\pix{k}}$ and kernel $\K^{\pix{k}}$.
Our prior on the assignment process is composite.
First, we assume that the $\mat{a_n}$ are drawn independently from multinomial distributions with logit parameters $\mat{\alpha_n} = \left( \alpha_n^{\pix{1}}, \ldots, \alpha_n^{\pix{K}} \right)$.
One approach to specify $\mat{\alpha_n}$ is to assume them to be known a priori and to be equal for all data points~\parencite{lazaro-gredilla_overlapping_2012}.
Instead, we want to infer them from the data.
Specifically, we assume that there is a relationship between the location in the input space $\mathbf{x}$ and the associations.
By placing independent GP priors on $\mat{\alpha^{\pix{k}}}$, we can encode our prior knowledge of the associations by the choice of covariance function
$\Prob*{\mat{\alpha} \given \mat{X}} = \prod_{k=1}^K \Gaussian*{\rv{\alpha^{\pix{k}}} \given \mat{0}, \Fun{\K_\alpha^{\pix{k}}}{\mat{X}, \mat{X}}}$.
The prior on the assignments $\mat{A}$ is given by marginalizing the $\mat{\alpha^{\pix{k}}}$, which, when normalized, parametrize a batch of multinomial distributions,
\begin{align}
    \begin{split}
        \label{eq:data_association:multinomial_likelihood}
        \Prob*{\mat{A} \given \mat{X}} &=
        \int
        \Multinomial*{\mat{A} \given \Fun{\softmax}{\mat{\alpha}}} \Prob*{\mat{\alpha} \given \mat{X}}
        \diff \rv{\alpha}.
    \end{split}
\end{align}
Modelling the relationship between the input and the associations allows us to efficiently model data, which, for example, is unimodal in some parts of the input space and bimodal in others.
A simple smoothness prior will encode a belief for how quickly the components switch across the input domain.

Since the GPs of the $\mat{\alpha^{\pix{k}}}$ use a zero mean function, our prior assumption is a uniform distribution of the different generative processes everywhere in the input space.
If inference on the $\mat{a_n}$ reveals that, say, all data points at similar positions in the input space can be explained by the same $\nth{k}$ process, the belief about $\mat{\alpha}$ can be adjusted to make a non-uniform distribution favorable at this position, thereby increasing the likelihood via $\Prob*{\mat{A} \given \mat{X}}$.
This mechanism introduces an incentive for the model to use as few functions as possible to explain the data and importantly allows us to predict a relative importance of these functions when calculating the posterior of the new observations $\mat{x_\ast}$.

\cref{fig:data_association:data_association_graphical_model} shows the resulting graphical model, which divides the generative process for every data point in the application of the latent functions on the left side and the assignment process on the right side.
The interdependencies between the data points are introduced through the GP priors on $\rv{f_n^{\pix{k}}}$ and $\rv{\alpha_n^{\pix{k}}}$ and depend on the hyperparameters $\mat{\theta} = \Set*{\mat{\theta^{\pix{k}}}, \mat{\theta_\alpha^{\pix{k}}}, \sigma^{\pix{k}}}_{k=1}^K$.

The priors for the $f^{\pix{k}}$ can be chosen independently to encode different prior assumptions about the underlying processes.
In \cref{toc:data_association:choicenet}, we use different kernels to separate a non-linear signal from a noise process.
Going further, we can also use deep GP as priors for the $f^{\pix{k}}$~\parencite{damianou_deep_2013, salimbeni_doubly_2017}.
Since many real word systems are inherently hierarchical, prior knowledge can often be formulated more easily using composite functions~\parencite{kaiser_bayesian_2018}.


\subsection{Optimization of the Lower Bound}
\label{toc:data_association:computation}
An important property of the variational bound for DSVI~\parencite{salimbeni_doubly_2017} is that taking samples for single data points is straightforward and can be implemented efficiently.
Specifically, for some $k$ and $n$, samples $\mat{\hat{f}_n^{\pix{k}}}$ from $\Variat*{\mat{f_n^{\pix{k}}}}$ are independent of all other parts of the model and can be drawn using samples from univariate unit Gaussians using reparametrizations~\parencite{kingma_variational_2015,rezende_stochastic_2014}.

Note that it would not be necessary to sample from the different processes, since $\Variat*{\mat{F^{\pix{k}}}}$ can be computed analytically~\parencite{hensman_gaussian_2013}.
However, we apply the sampling scheme to the optimization of both the assignment processes $\mat{\alpha}$ and the assignments $\mat{A}$ as for $\mat{\alpha}$, the analytical propagation of uncertainties through the $\softmax$ renormalization and multinomial likelihoods is intractable but can easily be evaluated using sampling.

We optimize $\Lc_{\text{DAGP}}$ to simultaneously recover maximum likelihood estimates of the hyperparameters $\mat{\theta}$, the variational parameters $\Set*{\mat{Z}, \mat{m}, \mat{S}}$, and assignments $\mat{A}$.
For every $n$, we represent the belief about $\mat{a_n}$ as a $K$-dimensional discrete distribution $\Variat*{\mat{a_n}}$.
This distribution models the result of drawing a sample from $\Multinomial*{\mat{a_n} \given \Fun{\softmax}{\mat{\alpha_n}}}$ during the generation of the data point $(\mat{x_n}, \mat{y_n})$.

Since we want to optimize $\Lc_{\text{DAGP}}$ using (stochastic) gradient descent, we need to employ a continuous relaxation to gain informative gradients of the bound with respect to the binary (and discrete) vectors $\mat{a_n}$.
One straightforward way to relax the problem is to use the current belief about $\Variat*{\mat{a_n}}$ as parameters for a convex combination of the $\mat{f_n^{\pix{k}}}$, that is, to approximate $\mat{f_n} \approx \sum_{k=1}^K \Variat*{\mat{a_n^{\pix{k}}}}\mat{\hat{f}_n^{\pix{k}}}$.
Using this relaxation is problematic in practice.
Explaining data points as mixtures of the different generating processes violates the modelling assumption that every data point was generated using exactly one function but can substantially simplify the learning problem.
Because of this, special care must be taken during optimization to enforce the sparsity of $\Variat*{\mat{a_n}}$.

To avoid this problem, we propose using a different relaxation based on additional stochasticity.
Instead of directly using $\Variat*{\mat{a_n}}$ to combine the $\mat{f_n^{\pix{k}}}$, we first draw a sample $\mat{\hat{a}_n}$ from a concrete random variable as suggested by~\textcite{maddison_concrete_2016}, parameterized by $\Variat*{\mat{a_n}}$.
Based on a temperature parameter $\lambda$, a concrete random variable enforces sparsity but is also continuous and yields informative gradients using automatic differentiation.
Samples from a concrete random variable are unit vectors and for $\lambda \to 0$ their distribution approaches a discrete distribution.

Our approximate evaluation of the bound in \cref{eq:data_association:variational_bound} during optimization has multiple sources of stochasticity, all of which are unbiased.
First, we approximate the expectations using Monte Carlo samples $\mat{\hat{f}_n^{\pix{k}}}$, $\mat{\hat{\alpha}_n^{\pix{k}}}$, and $\mat{\hat{a}_n}$.
And second, the factorization of the bound along the data allows us to use mini-batches for optimization~\parencite{salimbeni_doubly_2017, hensman_gaussian_2013}.


\subsection{Approximate Predictions}
\label{toc:data_association:predictions}
Predictions for a test location $\mat{x_\ast}$ are mixtures of $K$ independent Gaussians, given by,
\begin{align}
    \begin{split}
        \label{eq:data_association:predictive_posterior}
        \Variat*{\mat{f_\ast} \given \mat{x_\ast}}
        &= \int \sum_{k=1}^K \Variat*{a_\ast^{\pix{k}} \given \mat{x_\ast}} \Variat*{\mat{f_\ast^{\pix{k}}} \given \mat{x_\ast}} \diff \mat{a_\ast^{\pix{k}}}
        \approx \sum_{k=1}^K \hat{a}_\ast^{\pix{k}} \mat{\hat{f}_\ast^{\pix{k}}}.
    \end{split}
\end{align}
The predictive posteriors of the $K$ functions $\Variat*{\mat{f_\ast^{\pix{k}}} \given \mat{x_\ast}}$ are given by $K$ independent shallow GPs and can be calculated analytically~\parencite{hensman_gaussian_2013}.
Samples from the predictive density over $\Variat*{\mat{a_\ast} \given \mat{x_\ast}}$ can be obtained by sampling from the GP posteriors $\Variat*{\mat{\alpha_\ast^{\pix{k}}} \given \mat{x_\ast}}$ and renormalizing the resulting vector $\mat{\alpha_\ast}$ using the $\softmax$-function.
The distribution $\Variat*{\mat{a_\ast} \given \mat{x_\ast}}$ reflects the model's belief about how many and which of the $K$ generative processes are relevant at the test location $\mat{x_\ast}$ and their relative probability.


\subsection{Deep Gaussian Processes}
\label{toc:data_association:deep_gp}
For clarity, we have described the variational bound in terms of a shallow GP.
However, as long as their variational bound can be efficiently sampled, any model can be used in place of shallow GPs for the $f^{\pix{k}}$.
Since our approximation is based on DSVI, an extension to deep GPs is straightforward.
Analogously to~\parencite{salimbeni_doubly_2017}, our new prior assumption about the $\nth{k}$ latent function values $\Prob*{\mat{F^{\prime\pix{k}}} \given \mat{X}}$ is given by,
\begin{align}
    \begin{split}
        \Prob*{\mat{F^{\prime\pix{k}}} \given \mat{X}} = \prod_{l=1}^L \Prob*{\mat{F_l^{\prime\pix{k}}} \given \mat{u_l^{\prime\pix{k}}} \mat{F_{l-1}^{\prime\pix{k}}}, \mat{Z_l^{\prime\pix{k}}}},
    \end{split}
\end{align}
for an $L$-layer deep GP and with $\mat{F_0^{\prime\pix{k}}} \coloneqq \mat{X}$.
Similar to the single-layer case, we introduce sets of inducing points $\mat{Z_l^{\prime\pix{k}}}$ and a variational distribution over their corresponding function values $\Variat*{\mat{u_l^{\prime\pix{k}}}} = \Gaussian*{\mat{u_l^{\prime\pix{k}}} \given \mat{m_l^{\prime\pix{k}}}, \mat{S_l^{\prime\pix{k}}}}$.
We collect the latent multi-layer function values as $\mat{F^\prime} = \Set{\mat{F_l^{\prime\pix{k}}}}_{k=1,l=1}^{K,L}$ and corresponding $\mat{U^\prime}$ and assume an extended variational distribution,
\begin{align}
    \begin{split}
        \label{eq:data_association:deep_variational_distribution}
        \Variat*{\mat{F^\prime}, \mat{\alpha}, \mat{U^\prime}}
        &= \Variat*{\mat{\alpha}, \Set*{\mat{u_\alpha^{\pix{k}}}}_{k=1}^K, \Set*{\mat{F_l^{\prime\pix{k}}}, \mat{u_l^{\prime\pix{k}}}}_{k=1,l=1}^{K,L}} \\
        \MoveEqLeft[4] = \prod_{k=1}^K\prod_{n=1}^N \Prob*{\mat{\alpha_n^{\pix{k}}} \given \mat{u_\alpha^{\pix{k}}}, \mat{x_n}}\Variat*{\mat{u_\alpha^{\pix{k}}}}
        \prod_{k=1}^K \prod_{l=1}^L \prod_{n=1}^N \Prob*{\mat{f_{n,l}^{\prime\pix{k}}} \given \mat{u_l^{\prime\pix{k}}}, \mat{x_n}}\Variat*{\mat{u_l^{\prime\pix{k}}}},
    \end{split}
\end{align}
where we identify $\mat{f_n^{\prime\pix{k}}} = \mat{f_{n,L}^{\prime\pix{k}}}$.
As the $\nth{n}$ marginal of the $\nth{L}$ layer depends only on the $\nth{n}$ marginal of all layers above sampling from them remains straightforward~\parencite{salimbeni_doubly_2017}.
The marginal is given by,
\begin{align}
    \begin{split}
        \Variat{\mat{f_{n,L}^{\prime\pix{k}}}} =
        \int
        \Variat{\mat{f_{n,L}^{\prime\pix{k}}} \given \mat{f_{n,L-1}^{\prime\pix{k}}}}
        \prod_{l=1}^{L-1} \Variat{\mat{f_{n,l}^{\prime\pix{k}}} \given \mat{f_{n,l-1}^{\prime\pix{k}}}}
        \diff \mat{f_{n,l}^{\prime\pix{k}}}.
    \end{split}
\end{align}

The complete bound is structurally similar to \cref{eq:data_association:variational_bound} and given by,
\begin{align}
    \begin{split}
        \label{eq:data_association:deep_variational_bound}
        \Lc^\prime_{\text{DAGP}}
        &= \sum_{n=1}^N \Moment*{\E_{\Variat*{\mat{f^\prime_n}}}}{\log \Prob*{\mat{y_n} \given \mat{f^\prime_n}, \mat{a_n}}}
        + \sum_{n=1}^N \Moment*{\E_{\Variat*{\mat{\alpha_n}}}}{\log \Prob*{\mat{a_n} \given \mat{\alpha_n}}} \\
        \MoveEqLeft - \sum_{k=1}^K \sum_{l=1}^L \KL{\Variat{\mat{u_l^{\pix{k}}}}}{\Prob{\mat{u_l^{\pix{k}}} \given \mat{Z_l^{\pix{k}}}}}
        - \sum_{k=1}^K \KL{\Variat*{\mat{u_\alpha^{\pix{k}}}}}{\Prob*{\mat{u_\alpha^{\pix{k}}} \given \mat{Z_\alpha^{\pix{k}}}}}.
    \end{split}
\end{align}
To calculate the first term, samples have to be propagated through the deep GP structures.
This extended bound thus has complexity $\Fun*{\Oh}{NM^2LK}$ to evaluate in the general case and complexity $\Fun*{\Oh}{NM^2\cdot\Fun{\max}{L, K}}$ if the assignments $\mat{a_n}$ take binary values.


\subsection{Experiments}
\label{toc:data_association:experiments}
\begin{table}[t]
    \centering
    \caption{
        \label{tab:data_association:model_capabilities}
        Comparison of qualitative model capabilities.
        A model has a capability if it contains components which enable it to solve the respective task in principle.
    }
    \scriptsize
    \newcolumntype{Y}{>{\centering\arraybackslash}X}%
    \newcommand{\yes}{✔}
    \newcommand{\no}{--}
    \newcommand{\resultrow}[9]{#1 & #4 & #7 & #3 & #9 & #5 & #6 & #8 \\}
    % \setlength{\tabcolsep}{1pt}
    \begin{tabularx}{\linewidth}{lYYYYYYYY}
        \toprule
        \resultrow{}{Bayesian}{Scalable Inference}{Predictive Posterior}{Data Association}{Predictive Assoc.}{Multimodal Data}{Separate Models}{Interpretable Priors}
        \midrule
        Experiment &  &  &  &  & \cref{tab:data_association:choicenet} & \cref{tab:data_association:cartpole} & \cref{fig:data_association:semi_bimodal} \\
        \midrule
        \resultrow{DAGP}{\yes}{\yes}{\yes}{\yes}{\yes}{\yes}{\yes}{\yes}
        \addlinespace
        \resultrow{OMGP \parencite{lazaro-gredilla_overlapping_2012}}{\yes}{\no}{\yes}{\yes}{\no}{\yes}{\yes}{\yes}
        \resultrow{RGPR \parencite{rasmussen_infinite_2002}}{\yes}{\no}{\yes}{\no}{\no}{\yes}{\no}{\yes}
        \resultrow{MLE \parencite{tresp_mixtures_2001}}{\yes}{\no}{\yes}{\no}{\no}{\no}{\no}{\yes}
        \resultrow{LatentGP \parencite{bodin_latent_2017}}{\yes}{\no}{\yes}{\no}{\no}{\yes}{\no}{\yes}
        \resultrow{GPR \parencite{rasmussen_gaussian_2006}}{\yes}{\yes}{\yes}{\no}{\no}{\no}{\no}{\yes}
        \addlinespace
        \resultrow{BNN+LV \parencite{depeweg_learning_2016}}{\yes}{\yes}{\yes}{\no}{\no}{\yes}{\no}{\no}
        \resultrow{MDN \parencite{bishop_mixture_1994}}{\no}{\yes}{\yes}{\no}{\no}{\yes}{\no}{\no}
        \resultrow{MLP}{\no}{\yes}{\yes}{\no}{\no}{\no}{\no}{\no}
        \bottomrule
    \end{tabularx}
\end{table}
In this section, we investigate the behavior of the DAGP model.
We use an implementation of DAGP in TensorFlow~\parencite{abadi_tensorflow_2015} based on GPflow~\parencite{matthews_gpflow_2017} and the implementation of DSVI~\parencite{salimbeni_doubly_2017}.
\cref{tab:data_association:model_capabilities} compares qualitative properties of DAGP and related work.
All models can solve standard regression problems and yield unimodal predictive distributions or, in case of multi-layer perceptrons (MLP), a single point estimate.
Both standard Gaussian process regression (GPR) and MLP do not impose structure which enables the models to handle multi-modal data.
Mixture density networks (MDN)~\parencite{bishop_mixture_1994} and the infinite mixtures of Gaussian processes (RGPR)~\parencite{rasmussen_infinite_2002} model yield multi-modal posteriors through mixtures with many components but do not solve an association problem.
Similarly, Bayesian neural networks with added latent variables (BNN+LV)~\parencite{depeweg_learning_2016} represent such a mixture through a continuous latent variable.
Both the overlapping mixtures of Gaussian processes (OMGP)~\parencite{lazaro-gredilla_overlapping_2012} model and DAGP explicitly model the data association problem and yield independent models for the different generating processes.
However, OMGP assumes global relevance of the different modes.
In contrast, DAGP infers a spacial posterior of this relevance.
We evaluate our model on three problems to highlight the following advantages of the explicit structure of DAGP:

\emph{Interpretable priors give structure to ill-posed data association problems.}
In \cref{toc:data_association:choicenet}, we consider a noise separation problem, where a signal of interest is disturbed with uniform noise.
To solve this problem, assumptions about what constitutes a signal are needed.
The hierarchical structure of DAGP allows us to formulate independent and interpretable priors on the noise and signal processes.

\emph{Predictive associations represent knowledge about the relevance of generative processes.}
In \cref{toc:data_association:semi_bimodal}, we investigate the implicit incentive of DAGP to explain data using as few processes as possible.
Additional to a joint posterior explaining the data, DAGP also gives insight into the relative importance of the different processes in different parts of the input space.
DAGP is able to explicitly recover the changing number of modes in a data set.

\emph{Separate models for independent generating processes avoid model pollution.}
In \cref{toc:data_association:cartpole}, we simulate a system with multiple operational regimes via mixed observations of two different cart-pole systems.
DAGP successfully learns an informative joint posterior by solving the underlying association problem.
We show that the DAGP posterior contains two separate models for the two original operational regimes.


\subsection{Noise Separation}
\label{toc:data_association:choicenet}
%
\begin{table}[t]
    \centering
    \caption{
        \label{tab:data_association:choicenet}
        Results on the ChoiceNet data set.
        The gray part of the table shows RMSE results for baseline models from~\parencite{choi_choicenet_2018}.
        For our experiments using the same setup, we report RMSE comparable to the previous results together with MLL.
        Both are calculated based on a test set of 1000 equally spaced samples of the noiseless underlying function.
    }%
    \newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}
    \newcolumntype{Y}{>{\centering\arraybackslash}X}%
    \newcolumntype{Z}{>{\columncolor{sStone!33}\centering\arraybackslash}X}%
    \setlength{\tabcolsep}{4pt}
    \begin{tabularx}{\linewidth}{rYYYY|ZZZZHZ}
        \toprule
        Outliers & DAGP            & OMGP            & DAGP             & OMGP             & CN               & MDN              & MLP              & GPR              & LGPR             & RGPR             \\
                 & \scriptsize MLL & \scriptsize MLL & \scriptsize RMSE & \scriptsize RMSE & \scriptsize RMSE & \scriptsize RMSE & \scriptsize RMSE & \scriptsize RMSE & \scriptsize RMSE & \scriptsize RMSE \\
        \midrule
        0\,\%    & \textbf{2.86}   & 2.09            & 0.008            & \textbf{0.005}   & 0.034            & 0.028            & 0.039            & 0.008            & 0.022            & 0.017            \\
        20\,\%   & \textbf{2.71}   & 1.83            & 0.008            & \textbf{0.005}   & 0.022            & 0.087            & 0.413            & 0.280            & 0.206            & 0.013            \\
        40\,\%   & \textbf{2.12}   & 1.60            & \textbf{0.005}   & 0.007            & 0.018            & 0.565            & 0.452            & 0.447            & 0.439            & 1.322            \\
        60\,\%   & 0.874           & \textbf{1.23}   & 0.031            & \textbf{0.006}   & 0.023            & 0.645            & 0.636            & 0.602            & 0.579            & 0.738            \\
        80\,\%   & \textbf{0.126}  & -1.35           & 0.128            & 0.896            & \textbf{0.084}   & 0.778            & 0.829            & 0.779            & 0.777            & 1.523            \\
        \bottomrule
    \end{tabularx}
\end{table}
\begin{figure}[t]
    \centering
    \includestandalone{figures/choicenet_data_40}\hspace{-7pt}%
    \includestandalone{figures/choicenet_joint_40}\hspace{-7pt}%
    \includestandalone{figures/choicenet_attrib_40}\\%
    \vspace{\figureskip}%
    \includestandalone{figures/choicenet_data}\hspace{-7pt}%
    \includestandalone{figures/choicenet_joint}\hspace{-7pt}%
    \includestandalone{figures/choicenet_attrib}%
    \caption{
        \label{fig:data_association:choicenet}
        DAGP on the ChoiceNet data set with 40\,\% outliers (upper row) and 60\,\% outliers (lower row).
        We show the raw data (left), joint posterior (center) and assignments (right).
        The bimodal DAGP identifies the signal perfectly up to 40\,\% outliers.
        For 60\,\% outliers, some of the noise is interpreted as signal, but the latent function is still recovered.
    }
\end{figure}
%
We consider an experiment based on a noise separation problem.
We apply DAGP to a one-dimensional regression problem with uniformly distributed asymmetric outliers in the training data.
We use a task proposed by~\textcite{choi_choicenet_2018} where we sample $x \in [-3, 3]$ uniformly and apply the function $\Fun{f}{x} = (1 - \delta)(\Fun{\cos}{\sfrac{\pi}{2} \cdot x}\Fun{\exp}{-(\sfrac{x}{2})^2} + \gamma) + \delta \cdot \epsilon$, where $\delta \sim \Fun{\Ber}{\lambda}$, $\epsilon \sim \Fun{\Uni}{-1, 3}$ and $\gamma \sim \Gaussian{0, 0.15^2}$.
That is, a fraction $\lambda$ of the training data, the outliers, are replaced by asymmetric uniform noise.
We sample a total of 1000 data points and use $25$ inducing points for every GP in our model.

Every generating process in our model can use a different kernel and therefore encode different prior assumptions.
For this setting, we use two processes, one with a squared exponential kernel and one with a white noise kernel.
This encodes the problem statement that every data point is either part of the signal we wish to recover or uncorrelated noise.
To avoid pathological solutions for high outlier ratios, we add a prior to the likelihood variance of the first process, which encodes our assumption that there actually is a signal in the training data.

The model proposed in~\parencite{choi_choicenet_2018}, called ChoiceNet (CN), is a specific neural network structure and inference algorithm to deal with corrupted data.
In their work, they compare their approach to the MLP, MDN, GPR, and RGPR models.
We add experiments for both DAGP and OMGP.
\cref{tab:data_association:choicenet} shows results for outlier rates varied from 0\,\% to 80\,\%.
Besides the root mean squared error (RMSE) reported in~\parencite{choi_choicenet_2018}, we also report the mean test log likelihood (MLL).

Since we can encode the same prior knowledge about the signal and noise processes in both OMGP and DAGP, the results of the two models are comparable:
For low outlier rates, they correctly identify the outliers and ignore them, resulting in a predictive posterior of the signal equivalent to standard GP regression without outliers.
In the special case of 0\,\% outliers, the models correctly identify that the process modelling the noise is not necessary, thereby simplifying to standard GP regression.
For high outlier rates, stronger prior knowledge about the signal is required to still identify it perfectly.
\cref{fig:data_association:choicenet} shows the DAGP posterior for an outlier rate of 60\,\%.
While the function has still been identified well, some of the noise is also explained using this process, thereby introducing slight errors in the predictions.


\subsection{Multimodal Data}
\label{toc:data_association:semi_bimodal}
%
\begin{figure}[tp]
    \centering
    \includestandalone{figures/semi_bimodal_joint}\\%
    \vspace{\figureskip}%
    \includestandalone{figures/semi_bimodal_attrib}\\%
    \vspace{\figureskip}%
    \includestandalone{figures/semi_bimodal_attrib_process}%
    \caption{
        \label{fig:data_association:semi_bimodal}
        The DAGP posterior on an artificial data set with bimodal and trimodal parts.
        The joint predictions (top) are mixtures of four Gaussians weighed by the assignment probabilities $\mat{\alpha}$ (bottom).
        The weights are represented via the opacity of the modes.
        The model has learned that the mode $k = 2$ is irrelevant, that the mode $k = 1$ is only relevant around the interval $[0, 5]$.
        Outside this interval, the mode $k = 3$ is twice as likely as the mode $k = 4$.
        The concrete assignments $\mat{a}$ (middle) of the training data show that the mode $k = 1$ is only used to explain observations where the training data is trimodal.
        The mode $k = 2$ is never used.
    }
\end{figure}
%
Our second experiment applies DAGP to a multimodal data set.
The data, together with recovered posterior attributions, can be seen in \cref{fig:data_association:semi_bimodal}.
We uniformly sample 350 data points in the interval $x \in [-2\pi, 2\pi]$ and obtain $y_1 = \Fun{\sin}{x} + \epsilon$, $y_2 = \Fun{\sin}{x} - 2 \Fun{\exp}{-\sfrac{1}{2} \cdot (x-2)^2} + \epsilon$ and $y_3 = -1 - \sfrac{3}{8\pi} \cdot x + \sfrac{3}{10} \cdot \Fun*{\sin}{2x} + \epsilon$ with additive independent noise $\epsilon \sim \Gaussian*{0, 0.005^2}$.
The resulting data set $\Dc = \Set{\left( x, y_1 \right), \left( x, y_2 \right), \left( x, y_3 \right)}$ is trimodal in the interval $[0, 5]$ and is otherwise bimodal with one mode containing double the amount of data than the other.

We use squared exponential kernels as priors for both the $f^{\pix{k}}$ and $\alpha^{\pix{k}}$ and $25$ inducing points in every GP.
\cref{fig:data_association:semi_bimodal} shows the posterior of a DAGP with $K = 4$ modes applied to the data, which correctly identified the underlying functions.
The figure shows the posterior belief about the assignments $\mat{A}$ and illustrates that DAGP recovered that it needs only three of the four available modes to explain the data.
One of the modes is only assigned points in the interval $[0, 5]$ where the data is actually trimodal.

This separation is explicitly represented in the model via the assignment processes $\mat{\alpha}$ (bottom panel in \cref{fig:data_association:semi_bimodal}).
Importantly, DAGP does not only cluster the data with respect to the generating processes but also infers a factorization of the input space with respect to the relative importance of the different processes.
The model has disabled the mode $k = 2$ in the complete input space and has learned that the mode $k = 1$ is only relevant in the interval $[0, 5]$ where the three enabled modes each explain about a third of the data.
Outside this interval, the model has learned that one of the modes has about twice the assignment probability than the other one, thus correctly reconstructing the true generative process.
The DAGP is implicitly incentivized to explain the data using as few modes as possible through the likelihood term of the inferred $\mat{a_n}$ in \cref{eq:data_association:variational_bound}.
At $x = -10$ the inferred modes and assignment processes start reverting to their respective priors away from the data.


\subsection{Mixed Cart-pole Systems}
\label{toc:data_association:cartpole}
\begin{table}[t]
    \centering
    \caption{
        \label{tab:data_association:cartpole}
        Results on the cart-pole data set.
        We report mean log likelihoods with their standard error for ten runs.
        The upper results are obtained by training the model on the mixed data set and evaluating it jointly (left) on multi-modal predictions.
        We evaluate the two inferred sub-models for the default system (center) and short-pole system (right).
        We provide gray baseline comparisons with BNN+LV and GPR models which cannot solve the data assignment problem.
        BNN+LV yields joint predictions which cannot be separated into sub-models.
        Specialized GPR models trained the individual training sets give a measure of the possible performance if the data assignment problem would be solved perfectly.
    }%
    \newcolumntype{H}{>{\setbox0=\hbox\bgroup\begin{math}}c<{\end{math}\egroup}@{}}
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{HlSSHSHS}
        \toprule
             &             & \multicolumn{2}{c}{Mixed} & \multicolumn{2}{c}{Default only} & \multicolumn{2}{c}{Short-pole only}                                                                           \\
        \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
        Runs &             & {Train}                   & {Test}                           & {Train}                             & {Test}                    & {Train}         & {Test}                    \\
        \midrule
        10   & DAGP        & \bfseries 0.575 \pm 0.013 & \bfseries 0.521 \pm 0.009        & 0.855 \pm 0.002                     & 0.844 \pm 0.002           & 0.686 \pm 0.009 & \bfseries 0.602 \pm 0.005 \\
        % 10 & U-DAGP & 0.472 \pm 0.002 & 0.425 \pm 0.002 & {---} & {---} & {---} & {---} \\
        10   & DAGP 2      & 0.548 \pm 0.012           & \bfseries 0.519 \pm 0.008        & 0.851 \pm 0.003                     & \bfseries 0.859 \pm 0.001 & 0.673 \pm 0.013 & 0.599 \pm 0.011           \\
        10   & DAGP 3      & 0.527 \pm 0.004           & 0.491 \pm 0.003                  & 0.858 \pm 0.002                     & 0.852 \pm 0.002           & 0.624 \pm 0.011 & 0.545 \pm 0.012           \\
        % 10 & DAGP 4 & 0.517 \pm 0.006 & 0.485 \pm 0.003 & 0.858 \pm 0.001 & 0.852 \pm 0.002 & 0.602 \pm 0.011 & 0.546 \pm 0.010 \\
        % 10 & DAGP 5 & 0.535 \pm 0.004 & 0.506 \pm 0.005 & 0.851 \pm 0.003 & 0.851 \pm 0.003 & 0.662 \pm 0.009 & 0.581 \pm 0.012 \\
        \addlinespace
        10   & OMGP        & -1.04 \pm 0.02            & -1.11 \pm 0.03                   & 0.64 \pm 0.02                       & 0.66 \pm 0.02             & -0.9 \pm 0.2    & -0.81 \pm 0.12            \\
        \midrule
        \rowcolor{sStone!33}
        10   & BNN+LV      & 0.519 \pm 0.005           & 0.524 \pm 0.005                  & {---}                               & {---}                     & {---}           & {---}                     \\
        \rowcolor{sStone!33}
        10   & GPR Mixed   & 0.452 \pm 0.003           & 0.421 \pm 0.003                  & {---}                               & {---}                     & {---}           & {---}                     \\
        \rowcolor{sStone!33}
        10   & GPR Default & {---}                     & {---}                            & 0.873 \pm 0.001                     & 0.867 \pm 0.001           & -7.01 \pm 0.11  & -7.54 \pm 0.14            \\
        \rowcolor{sStone!33}
        10   & GPR Short   & {---}                     & {---}                            & -5.24 \pm 0.04                      & -5.14 \pm 0.04            & 0.903 \pm 0.003 & 0.792 \pm 0.003           \\
        \bottomrule
    \end{tabular}
\end{table}
Our third experiment is based on the cart-pole benchmark for reinforcement learning as described by~\textcite{barto_neuronlike_1983} and implemented in OpenAI Gym~\parencite{brockman_openai_2016}.
In this benchmark, the objective is to apply forces to a cart moving on a frictionless track to keep a pole, which is attached to the cart via a joint, in an upright position.
We consider the regression problem of predicting the change of the pole's angle given the current state of the cart and the action applied.
The current state of the cart consists of the cart's position and velocity and the pole's angular position and velocity.
To simulate a dynamical system with changing system characteristics our experimental setup is to sample trajectories from two different cart-pole systems and merging the resulting data into one training set.
The task is not only to learn a model which explains this data well, but to solve the association problem introduced by the different system configurations.
This task is important in reinforcement learning settings where we study systems with multiple operational regimes.

We sample trajectories from the system by initializing the pole in an almost upright position and then applying 10 uniform random actions.
We add Gaussian noise $\epsilon \sim \Gaussian*{0, 0.01^2}$ to the observed angle changes.
To increase the non-linearity of the dynamics, we apply the action for five consecutive time steps and allow the pole to swing freely instead of ending the trajectory after reaching a specific angle.
The data set consists of 500 points sampled from the \emph{default} cart-pole system and another 500 points sampled from a \emph{short-pole} cart-pole system in which we halve the mass of the pole to 0.05 and shorten the pole to 0.1, a tenth of its default length.
This short-pole system is more unstable and the pole reaches higher speeds.
Predictions in this system therefore have to take the multimodality into account, as mean predictions between the more stable and the more unstable system can never be observed.
We consider three test sets, one sampled from the default system, one sampled from the short-pole system, and a mixture of the two.
They are generated by sampling trajectories with an aggregated size of 5000 points from each system for the first two sets and their concatenation for the mixed set.

For this data set, we use squared exponential kernels for both the $f^{\pix{k}}$ and $\alpha^{\pix{k}}$ and 100 inducing points in every GP.
We evaluate the performance of deep GPs with up to three layers and squared exponential kernels as models for the different functions.
As described in~\parencite{salimbeni_doubly_2017,kaiser_bayesian_2018}, we use identity mean functions for all but the last layers and initialize the variational distributions with low covariances.
We compare our models with OMGP and three-layer relu-activated Bayesian neural networks with added latent variables (BNN+LV).
The latent variables can be used to effectively model multimodalities and stochasticity in dynamical systems for model-based reinforcement learning~\parencite{depeweg_decomposition_2018}.
We also compare DAGP to three kinds of sparse GPs (GPR)~\parencite{hensman_scalable_2015}.
They are trained on the mixed data set, the default system and the short-pole system respectively and serve as a baseline comparison as these models cannot handle multi-modal data.

\cref{tab:data_association:cartpole} shows results for ten runs of these models.
The GPR model predicts a unimodal posterior for the mixed data set which covers both systems.
Its mean prediction is approximately the mean of the two regimes and is physically implausible.
The DAGP and BNN+LV models yield informative multi-modal predictions with comparable performance.
In our setup, OMGP could not successfully solve the data association problem and thus does not produce a useful joint posterior.
The OMGP's inference scheme is tailored to ordered one-dimensional problems.
It does not trivially translate to the 4D cart-pole problem.

As BNN+LV does not explicitly solve the data association problem, the model does not yield sub-models for the two different systems.
Similar results would be obtained with the MDN and RGPR models, which also cannot be separated into sub-models.
OMGP and DAGP yield such sub-models which can independently be used for predictions in the default or short-pole systems.
Samples drawn from these models can be used to generate physically plausible trajectories in the respective system.
OMGP fails to model the short-pole system but does yield a viable model for the default system which evolves more slowly due to higher torque and is therefore easier to learn.
In contrast, the two sub-models inferred by DAGP perform well on their respective systems, showing that DAGP reliably solves the data association problem and successfully avoids model pollution by separating the two systems well.
Given this separation, shallow and deep models for the two modes show comparable performance.
The more expressive deep GPs model the default system slightly better while sacrificing performance on the more difficult short-pole system.


\subsection{Conclusion}
\label{toc:data_association:conclusion}
We have presented a fully Bayesian model for the data association problem.
Our model factorises the observed data into a set of independent processes and provides a model over both the processes and their association to the observed data.
The data association problem is inherently ill-constrained and requires significant assumptions to recover a solution.
In this paper, we make use of interpretable GP priors allowing global a priori information to be included into the model.
Importantly, our model is able to exploit information both about the underlying functions and the association structure.
We have derived a principled approximation to the marginal likelihood which allows us to perform inference for flexible hierarchical processes.
In future work, we would like to incorporate the proposed model in a reinforcement learning scenario where we study a dynamical system with different operational regimes.


\section{Non-Linear Timeseries Alignment}
\label{toc:timeseries_alignment}
We propose a novel Bayesian approach to modelling nonlinear alignments of time series based on latent shared information.
We apply the method to the real-world problem of finding common structure in the sensor data of wind turbines introduced by the underlying latent and turbulent wind field.
The proposed model allows for both arbitrary alignments of the inputs and non-parametric output warpings to transform the observations.
This gives rise to multiple deep Gaussian process models connected via latent generating processes.
We present an efficient variational approximation based on nested variational compression and show how the model can be used to extract shared information between dependent time series, recovering an interpretable functional decomposition of the learning problem.
We show results for an artificial data set and real-world data of two wind turbines.


\subsection{Introduction}
Many real-world systems are inherently hierarchical and connected.
Ideally, a machine learning method should model and recognize such dependencies.
Take wind power production, which is one of the major providers for renewable energy today, as an example:
To optimize the efficiency of a wind turbine the speed and pitch have to be controlled according to the local wind conditions (speed and direction).
In a wind farm turbines are typically equipped with sensors for wind speed and direction.
The goal is to use these sensor data to produce accurate estimates and forecasts of the wind conditions at every turbine in the farm.
For the ideal case of a homogeneous and very slowly changing wind field, the wind conditions at each geometrical position in a wind farm can be estimated using the propagation times (time warps) computed from geometry, wind speed, and direction \parencite{soleimanzadeh_controller_2011,bitar_coordinated_2013,schepers_improved_2007}.
In the real world, however, wind fields are not homogeneous, exhibit global and local turbulences, and interfere with the turbines and the terrain inside and outside the farm and further, sensor faults may lead to data loss.
This makes it extremely difficult to construct accurate analytical models of wind propagation in a farm.
Also, standard approaches for extracting such information from data, e.g.\ generalized time warping  \parencite{zhou_generalized_2012}, fail at this task because they rely on a high signal to noise ratio.
Instead, we want to construct Bayesian nonlinear dynamic data based models for wind conditions and warpings which handle the stochastic nature of the system in a principled manner.

In this paper, we look at a generalization of this type of problem and propose a novel Bayesian approach to finding nonlinear alignments of time series based on latent shared information.
We view the power production of different wind turbines as the outputs of a multi-output Gaussian process (MO-GP) \parencite{alvarez_kernels_2011} which models the latent wind fronts.
We embed this model in a hierarchy, adding a layer of non-linear alignments on top and a layer of non-linear warpings \parencites{snelson_warped_2004,lazaro-gredilla_bayesian_2012} below which increases flexibility and encodes the original generative process.
We show how the resulting model can be interpreted as a group of deep Gaussian processes with the added benefit of covariances between different outputs.
The imposed structure is used to formulate prior knowledge in a principled manner, restrict the representational power to physically plausible models and  recover the desired latent wind fronts and relative alignments.
The presented model can be interpreted as a group of $D$ deep GPs all of which share one layer which is a MO-GP.
This MO-GP acts as an interface to share information between the different GPs which are otherwise conditionally independent.

The paper has the following contributions:
In \cref{toc:timeseries_alignment:model}, we propose a hierarchical, warped and aligned  multi-output Gaussian process (AMO-GP).
In \cref{toc:timeseries_alignment:variational_approximation}, we present an efficient learning scheme via an approximation to the marginal likelihood which allows us to fully exploit the regularization provided by our structure, yielding highly interpretable results.
We show these properties for an artificial data set and for real-world data of two wind turbines in \cref{toc:timeseries_alignment:experiments}.

\subsection{Model Definition}
\label{toc:timeseries_alignment:model}
\begin{figure}[t]
    \centering
    \includestandalone{figures/wind_graphical_model}
    \caption{
        \label{fig:timeseries_alignment:graphical_model_supervised}
        The graphical model of AMO-GP with variational parameters (blue).
        A CP, informed by $R$ latent processes, models shared information between multiple data sets with nonlinear alignments and warpings.
        This CP connects multiple deep GPs through a shared layer.
    }
\end{figure}
\begin{figure}[t]
    \centering
    \includestandalonewithpath{figures/dampened_sine_decomposition_true}
    \caption{
        \label{fig:timeseries_alignment:dampened_sine_decomposition}
        An artificial example of hierarchical composite data with multiple observations of shared latent information.
        This hierarchy generates two data sets using a dampened sine function which is never observed directly.
    }
\end{figure}
We are interested in formulating shared priors over a set of functions $\Set{f_d}_{d=1}^D$ using GPs, thereby directly parameterizing their interdependencies.
In a traditional GP setting, multiple outputs are considered conditionally independent given the inputs, which significantly reduces the computational cost but also prevents the utilization of shared information.
Such interdependencies can be formulated via \emph{convolution processes (CPs)} as proposed by \textcite{boyle_dependent_2004}, a generalization of the \emph{linear model of coregionalization (LMC)} \parencite{journel_mining_1978,coburn_geostatistics_2000}.
In the CP framework, the output functions are the result of a convolution of the latent processes $w_r$ with smoothing kernel functions $T_{d,r}$ for each output $f_d$, defined as
\begin{align}
    f_d(\mat{x}) = \sum_{r=1}^R \int T_{d,r}(\mat{x} - \mat{z}) \cdot w_r(\mat{z}) \diff \mat{z}.
\end{align}
In this model, the convolutions of the latent processes generating the different outputs are all performed around the same point $\mat{x}$.
We generalize this by allowing different \emph{alignments} of the observations which depend on the position in the input space.
This allows us to model the changing relative interaction times for the different latent wind fronts as described in the introduction.
We also assume that the dependent functions $f_d$ are latent themselves and the data we observe is generated via independent noisy nonlinear transformations of their values.
Every function $f_d$ is augmented with an alignment function $a_d$ and a warping $g_d$ on which we place independent GP priors.

For simplicity, we assume that the outputs are evaluated all at the same positions $\mat{X} = \Set{\mat{x_n}}_{n=1}^N$.
This can easily be generalized to different input sets for every output.
In our application, the $\mat{x_n}$ are one-dimensional time indices.
However, since the model can be generalized to multi-dimensional inputs, we do not restrict ourselves to the one-dimensional case.
We note that in the multi-dimensional case, reasoning about priors on alignments can be challenging.
We call the observations associated with the $d$-th function $\mat{y_d}$ and use the stacked vector $\mat{y} = \left( \mat{y_1}, \dots, \mat{y_D} \right)$ to collect the data of all outputs.
The final model is then given by
\begin{align}
    \mat{y_d} & = g_d(f_d(a_d(\mat{X}))) + \mat{\epsilon_d},
\end{align}
where $\mat{\epsilon_d} \sim \Gaussian{0, \sigma_{y, d}^2\Eye}$ is a noise term.
The functions are applied element-wise.
This encodes the generative process described above:
For every turbine $\rv{y_d}$, observations at positions $\rv{X}$ are generated by first aligning to the latent wind fronts using $a_d$, applying the front in $f_d$, imposing turbine-specific components $g_d$ and adding noise in $\rv{\epsilon_d}$.

We assume independence between $a_d$ and $g_d$ across outputs and apply GP priors of the form $a_d \sim \GP(\id, k_{a, d})$ and $g_d \sim \GP(\id, k_{g, d})$.
By setting the prior mean to the identity function $\id(x) = x$, the standard CP model is our default assumption.
During learning, the model can choose the different $a_d$ and $g_d$ in a way to reveal the independent shared latent processes $\Set{w_r}_{r=1}^R$ on which we also place GP priors $w_r \sim \GP(0, k_{u, r})$.
Similar to \textcite{boyle_dependent_2004}, we assume the latent processes to be independent white noise processes by setting $\Moment{\cov}{w_r(\mat{z}), w_{r^\prime}(\mat{z^\prime})} = \delta_{rr^\prime}\delta_{\mat{z}\mat{z^\prime}}$.
Under this prior, the $f_d$ are also GPs with zero mean and $\Moment{\cov}{f_d(\mat{x}), f_{d^\prime}(\mat{x^\prime})} = \sum_{r=1}^R \int T_{d,r}(\mat{x} - \mat{z}) T_{d^\prime,r}(\mat{x^\prime} - \mat{z}) \diff \mat{z}$.

Using the squared exponential kernel for all $T_{d, r}$, the integral can be shown to have a closed form solution.
With $\Set{\sigma_{d,r}, \mat{\ell_{d, r}}}$ denoting the kernel hyper parameters associated with $T_{d,r}$, it is given by
\begin{align}
    \label{eq:timeseries_alignment:dependent_kernel}
    \begin{split}
        \MoveEqLeft[1] \Moment{\cov}{f_d(\mat{x}), f_{d^\prime}(\mat{x^\prime})} = \sum_{r=1}^R \frac{(2\pi)^{\frac{K}{2}}\sigma_{d, r}\sigma_{d^\prime, r}}{\prod_{k=1}^K \hat{\ell}_{d, d^\prime, r, k}\inv} \Fun*{\exp}{-\frac{1}{2} \sum_{k=1}^K \frac{(x_k - x^\prime_k)^2}{\hat{\ell}_{d, d^\prime, r, k}^2}},
    \end{split}
\end{align}
where $\mat{x}$ is $K$-dimensional and $\hat{\ell}_{d, d^\prime, r, k} = \sqrt{\ell_{d, r, k}^2 + \ell_{d^\prime, r, k}^2}$.

\subsection{Convolution Kernel Expectations}
\label{toc:timeseries_alignment:kernel_expectations}
In \cref{toc:timeseries_alignment:model} we assumed the latent processes $w_r$ to be white noise processes and the smoothing kernel functions $T_{d, r}$ to be squared exponential kernels, leading to an explicit closed form formulation for the covariance between outputs shown in \cref{eq:timeseries_alignment:dependent_kernel}.
In this section, we derive the $\Psi$-statistics for this generalized squared exponential kernel needed to evaluate \cref{eq:timeseries_alignment:full_bound}.

The uncertainty about the first layer is captured by the variational distribution of the latent alignments $\rv{a}$ given by $\Variat{\rv{a}} \sim \Gaussian{\mat{\mu_a}, \mat{\Sigma_a}}\text{, with } \rv{a} = \left( \rv{a_1}, \dots, \rv{a_d} \right)$.
Every aligned point in $\rv{a}$ corresponds to one output of $\rv{f}$ and ultimately to one of the $\rv{y_d}$.
Since the closed form of the multi output kernel depends on the choice of outputs, we will use the notation $\Fun{\hat{f}}{\rv{a_n}}$ to denote $\Fun{f_d}{\rv{a_n}}$ such that $\rv{a_n}$ is associated with output $d$.

For notational simplicity, we only consider the case of one single latent process $w_r$.
Since the latent processes are independent, the results can easily be generalized to multiple processes.
Then, $\psi_f$ is given by
\begin{align}
    \begin{split}
        \psi_f &= \Moment*{\E_{\Variat{\rv{a}}}}{\Fun*{\tr}{\mat{K_{ff}}}} \\
        &= \sum_{n=1}^N \Moment*{\E_{\Variat{\rv{a_n}}}}{\Moment*{\cov}{\Fun{\hat{f}}{\mat{a_n}}, \Fun{\hat{f}}{\mat{a_n}}}} \\
        &= \sum_{n=1}^N \int \Moment*{\cov}{\Fun{\hat{f}}{\mat{a_n}}, \Fun{\hat{f}}{\mat{a_n}}} \Variat{\rv{a_n}} \diff \rv{a_n} \\
        &= \sum_{n=1}^N \hat{\sigma}_{nn}^2.
    \end{split}
\end{align}
Similar to the notation $\Fun{\hat{f}}{\cdot}$, we use the notation $\hat{\sigma}_{nn^\prime}$ to mean the variance term associated with the covariance function $\Moment{\cov}{\Fun{\hat{f}}{\mat{a_n}}, \Fun{\hat{f}}{\mat{a_{n^\prime}}}}$.
The expectation $\mat{\Psi_f} = \Moment*{\E_{\Variat{\rv{a}}}}{\mat{K_{fu}}}$ connecting the alignments and the pseudo inputs is given by
\begin{align}
    \begin{split}
        \mat{\Psi_f} &= \Moment*{\E_{\Variat{\rv{a}}}}{\mat{K_{fu}}}\text{, with} \\
        \left( \mat{\Psi_f} \right)_{ni}
        &= \int \Moment*{\cov}{\Fun{\hat{f}}{\mat{a_n}}, \Fun{\hat{f}}{\mat{Z_i}}} \Variat{\rv{a_n}} \diff \rv{a_n} \\
        &= \hat{\sigma}_{ni}^2 \sqrt{\frac{(\mat{\Sigma_a})_{nn}\inv}{\hat{\ell}_{ni} + (\mat{\Sigma_a})_{nn}\inv}}
        \cdot \exp\left(-\frac{1}{2} \frac{(\mat{\Sigma_a})_{nn}\inv\hat{\ell}_{ni}}{(\mat{\Sigma_a})_{nn}\inv + \hat{\ell}_{ni}} \left((\mat{\mu_a})_n - \mat{Z_i}\right)^2\right)
    \end{split}
\end{align}
where $\hat{\ell}_{ni}$ is the combined length scale corresponding to the same kernel as $\hat{\sigma}_{ni}$.
Lastly, $\mat{\Phi_f} = \Moment*{\E_{\Variat{\rv{a}}}}{\mat{K_{uf}}\mat{K_{fu}}}$ connects alignments and pairs of pseudo inputs with the closed form
\begin{align}
    \begin{split}
        \mat{\Phi_f} &= \Moment*{\E_{\Variat{\rv{a}}}}{\mat{K_{uf}}\mat{K_{fu}}}\text{, with} \\
        \left(\mat{\Phi_f} \right)_{ij} &= \sum_{n=1}^N \int \Moment*{\cov}{\Fun{\hat{f}}{\mat{a_n}}, \Fun{\hat{f}}{\mat{Z_i}}}
        \cdot \Moment*{\cov}{\Fun{\hat{f}}{\mat{a_n}}, \Fun{\hat{f}}{\mat{Z_j}}} \Variat{\rv{a_n}} \diff \rv{a_n} \\
        &= \sum_{n=1}^N \hat{\sigma}_{ni}^2 \hat{\sigma}_{nj}^2 \sqrt{\frac{(\mat{\Sigma_a})_{nn}\inv}{\hat{\ell}_{ni} + \hat{\ell}_{nj} + (\mat{\Sigma_a})_{nn}\inv}}
        \cdot \exp\left( -\frac{1}{2} \frac{\hat{\ell}_{ni}\hat{\ell}_{nj}}{\hat{\ell}_{ni} + \hat{\ell}_{nj}} (\mat{Z_i} - \mat{Z_j})^2 \right. \\
        &\quad {} - \frac{1}{2} \frac{(\mat{\Sigma_a})_{nn}\inv(\hat{\ell}_{ni} + \hat{\ell}_{nj})}{(\mat{\Sigma_a})_{nn}\inv + \hat{\ell}_{ni} + \hat{\ell}_{nj}}
        \cdot \left.\left( (\mat{\mu_a})_n - \frac{\hat{\ell}_{ni} \mat{Z_i} + \hat{\ell}_{nj} \mat{Z_j}}{\hat{\ell}_{ni} + \hat{\ell}_{nj}} \right)^2 \right).
    \end{split}
\end{align}

Note that the $\Psi$-statistics factorize along the data and we only need to consider the diagonal entries of $\mat{\Sigma_a}$.
If all the data belong to the same output, the $\Psi$-statistics of the standard squared exponential kernel can be recovered as a special case.
It is used to propagate the uncertainties through the output-specific warpings $\rv{g}$.


\subsection{Approximative Predictions}
\label{toc:timeseries_alignment:predictions}
Using the variational lower bound in \cref{eq:timeseries_alignment:full_bound}, our model can be fitted to data, resulting in appropriate choices of the kernel hyper parameters and variational parameters.
Now assume we want to predict approximate function values $\mat{g_{d, \star}}$ for previously unseen points $\mat{X_{d, \star}}$ associated with output $d$, which are given by $ \mat{g_{d, \star}} = g_d(f_d(a_d(\mat{X_{d, \star}})))$.

Because of the conditional independence assumptions in the model, other outputs $d^\prime \neq d$ only have to be considered in the shared layer $\rv{f}$.
In this shared layer, the belief about the different outputs and the shared information and is captured by the variational distribution $\Variat{\rv{u_f}}$.
Given $\Variat{\rv{u_f}}$, the different outputs are conditionally independent of one another and thus, predictions for a single dimension in our model are equivalent to predictions in a single deep GP with nested variational compression as presented by \textcite{hensman_nested_2014}.

\todo{Below, I write what I currently know about how to calculate this. Should we just stop here?}Given inputs $\mat{X_{d, \star}}$, we propagate Gaussian messages through the different layers. The Gaussians are the variational approximations of $\rv{a_{d, \star}}$, $\rv{f_{d, \star}}$ and $\rv{g_{d, \star}}$.
Using $\rv{f_{d, \star}}$ as an example, in \cref{eq:timeseries_alignment:variational_assumption} we assumed that they are given by a standard integral
\begin{align}
    \begin{split}
        \MoveEqLeft\Variat{\rv{f_{d, \star}} \given \rv{a_{d, \star}}} = \\
        &= \int \aProb{\rv{f_{d, \star}} \given \rv{u_f}, \rv{a_{d, \star}}}\Variat{\rv{u_f}} \diff \rv{u_f} \\
        &= \Gaussian*{\rv{f_{d, \star}} \given \mat{\mu_{\star}}, \mat{\Sigma_{\star}} + \sigma_f^2 \Eye}
    \end{split}
\end{align}
with
\begin{align*}
    \mat{\mu_{\star}}    & = \mat{K_{\star m}}\mat{K_{mm}}\inv \mat{m}                                                                                         \\
    \mat{\Sigma_{\star}} & = \mat{K_{\star\star}} - \mat{K_{\star m}}\mat{K_{mm}}\inv \left(  \mat{K_{mm}} - \mat{S} \right) \mat{K_{mm}}\inv\mat{K_{m\star}}.
\end{align*}
Here, $\mat{K_{\star m}}$ denotes the kernel matrix of the previous layer's output $\rv{a_{d, \star}}$ and the current layer's inducing inputs $\mat{Z_f}$.
Since every message besides the initial $\rv{X_{d, \star}}$ is itself a Gaussian, it needs to be marginalized:
\begin{align}
    \begin{split}
        \MoveEqLeft\Variat{\rv{f_{d, \star}} \given \rv{X_{d, \star}}} = \\
        &= \int \Variat{\rv{f_{d, \star}}, \rv{a_{d, \star}} \given \rv{X_{d, \star}}} \diff \rv{a_{d, \star}} \\
        &= \int \Variat{\rv{f_{d, \star}} \given \rv{a_{d, \star}}} \Variat{\rv{a_{d, \star}} \given \rv{X_{d, \star}}} \diff \rv{a_{d, \star}} \\
        &= \int \Variat{\rv{f_{d, \star}} \given \rv{a_{d, \star}}} \Variat{\rv{a_{d, \star}} \given \rv{X_{d, \star}}} \diff \rv{a_{d, \star}} \\
        &= \Moment*{\E_{\Variat{\rv{a_{d, \star}} \given \rv{X_{d, \star}}}}}{\Variat{\rv{f_{d, \star}} \given \rv{a_{d, \star}}}} \\
        &= \Gaussian*{\rv{f_{d, \star}} \given \mat{\bar{\mu}_{\star}}, \mat{\bar{\Sigma}_{\star}}}
    \end{split}
\end{align}
with \todo{I am super uncertain about $\mat{\bar{\Sigma}}$.}
\begin{align*}
    \mat{\bar{\mu}_{\star}}    & = \mat{\Psi_{f\star}} \mat{K_{mm}}\inv \mat{m}                                        \\
    \mat{\bar{\Sigma}_{\star}} & = \mat{\Psi_{f\star}}\mat{K_{mm}}\inv\mat{S}\mat{K_{mm}}\inv\mat{\Psi_{f\star}}\tran.
\end{align*}
For the first layer, the expectation collapses to usual SVGP predictions as derived in \cite{hensman_scalable_2015}.
Given the approximation of $\rv{f_{d, \star}}$, the approximation of $\rv{g_{d, \star}}$ can be derived via the same procedure for the next layer.
This yields an approximate prediction for the final function values.


\subsection{Model Interpretation}
\label{toc:timeseries_alignment:interpretation}
The graphical model shown in \cref{fig:timeseries_alignment:graphical_model_supervised} illustrates that the presented model can be interpreted as a group of $D$ deep GPs all of which share one layer which is a CP.
This CP acts as an interface to share information between the different GPs which are otherwise conditionally independent.
This modelling-choice introduces a new quality to the model when compared to standard deep GPs with multiple output dimensions, since the latter are not able in principle to learn dependencies between the different outputs.
Compared to standard multi-output GPs, the AMO-GP introduces more flexibility with respect to the shared information.
CPs make strong assumptions about the relative alignments of the different outputs, that is, they assume constant time-offsets.
The AMO-GP extends this by introducing a principled Bayesian treatment of general nonlinear alignments $a_d$ on which we can place informative priors derived from the problem at hand.
Together with the warping layers $g_d$, our model can learn to share knowledge in an informative latent space learnt from the data.

Alternatively, this model can be interpreted as a shared and warped latent variable model with a very specific prior:
The indices $\mat{X}$ are part of the prior for the latent space $a_d(\mat{X})$ and specify a sense of order for the different data points $\mat{y}$ which is augmented with uncertainty by the alignment functions.
Using this order, the convolution processes enforce the covariance structure for the different datapoints specified by the smoothing kernels.

In order to derive an inference scheme, we need the ability to propagate uncertainties about the correct alignments and latent shared information through subsequent layers.
We adapted the approach of nested variational compression by \textcite{hensman_nested_2014}, which is originally concerned with a single deep GP.
The approximation is expanded to handle multiple GPs at once, yielding the bound in \cref{eq:timeseries_alignment:full_bound}.
The bound reflects the dependencies of the different outputs as the sharing of information between the different deep GPs is approximated through the shared inducing variables $\rv{u_{f,d}}$.
Our main contribution for the inference scheme is the derivation of a closed-form solution for the $\Psi$-statistics of the convolution kernel in \cref{eq:timeseries_alignment:psi_statistics}.


\subsection{Experiments}
\label{toc:timeseries_alignment:experiments}
\begin{figure}[tp]
    \centering
    \begin{subfigure}{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/dampened_sine_decomposition_shallow_gp}
        \caption{
            Shallow GP with RBF kernel.
            \label{fig:timeseries_alignment:dampened_sine_model_decomposition:a}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/dampened_sine_decomposition_mo_gp}
        \caption{
            Multi-Output GP with dependent RBF kernel.
            \label{fig:timeseries_alignment:dampened_sine_model_decomposition:b}
        }
    \end{subfigure}
    \\[\baselineskip]
    \begin{subfigure}{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/dampened_sine_decomposition_dgp}
        \caption{
            Deep GP with RBF kernels.
            \label{fig:timeseries_alignment:dampened_sine_model_decomposition:c}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/dampened_sine_decomposition_ours}
        \caption{
            AMO-GP with (dependent) RBF kernels.
            \label{fig:timeseries_alignment:dampened_sine_model_decomposition:d}
        }
    \end{subfigure}
    \caption{
        \label{fig:timeseries_alignment:dampened_sine_model_decomposition}
        A comparison of the AMO-GP with other GP models.
        The plots show mean predictions and a shaded area of two standard deviations.
        If available, the ground truth is displayed as a dashed line.
        Additional lines are noiseless samples drawn from the model.
        The shallow and deep GPs in \cref{fig:timeseries_alignment:dampened_sine_model_decomposition:a,fig:timeseries_alignment:dampened_sine_model_decomposition:c} model the data independently and revert back to the prior in $\rv{y_2}$.
        Because of the nonlinear alignment, a multi-output GP cannot model the data in \cref{fig:timeseries_alignment:dampened_sine_model_decomposition:b}.
        The AMO-GP in \cref{fig:timeseries_alignment:dampened_sine_model_decomposition:d} recovers the alignment and warping and shares information between the two outputs.
    }
\end{figure}
In this section we show how to apply the AMO-GP to the task of finding common structure in time series observations.
In this setting, we observe multiple time series $\Tc_d = (\mat{X_d}, \mat{y_d})$ and assume that there exist latent time series which determine the observations.

We will first apply the AMO-GP to an artificial data set in which we define a decomposed system of dependent time series by specifying a shared latent function generating the observations together with relative alignments and warpings for the different time series.
We will show that our model is able to recover this decomposition from the training data and compare the results to other approaches of modeling the data.
Then we focus on a real world data set of a neighbouring pair of wind turbines in a wind farm, where the model is able to recover a representation of the latent prevailing wind condition and the relative timings of wind fronts at the two turbines.

\subsection{Artificial data set}
\label{toc:timeseries_alignment:artificial_example}
Our data set consists of two time series $\Tc_1$ and $\Tc_2$ generated by a dampened sine function.
\begin{align}
    f : \left\{ \begin{aligned}
        [0, 1] & \to \Rb                                                                                                          \\
        x      & \mapsto \left( 1 - \frac{3}{4} \tanh \left( \frac{10\pi}{15} \cdot x \right) \right) \cdot \sin (10\pi \cdot x).
    \end{aligned}\right.
\end{align}
We choose the alignment of $\Tc_1$ and the warping of $\Tc_2$ to be the identity in order to prevent us from directly observing the latent function and apply a sigmoid warping to $\Tc_1$.
The alignment of $\Tc_2$ is selected to be a quadratic function.
\Cref{fig:timeseries_alignment:dampened_sine_decomposition} shows a visualization of this decomposed system of dependent time series.
To obtain training data we uniformly sampled 500 points from the two time series and added Gaussian noise.
We subsequently removed parts of the training sets to explore the generalization behaviour of our model, resulting in $\abs{\Tc_1}= 450$ and $\abs{\Tc_2} = 350$.

We use this setup to train our model using squared exponential kernels both in the conditionally independent GPs $\rv{a_d}$ and $\rv{g_d}$ and as smoothing kernels in $\rv{f}$.
We can always choose one alignment and one warping to be the identity function in order to constrain the shared latent spaces $\rv{a}$ and $\rv{f}$ and provide a reference the other alignments and warpings will be relative to.
Since we assume our artificial data simulates a physical system, we apply the prior knowledge that the alignment and warping processes have slower dynamics compared to the shared latent function which should capture most of the observed dynamics.
To this end we applied priors to the $\rv{a_d}$ and $\rv{g_d}$ which prefer longer length scales and smaller variances compared to $\rv{f}$.
Otherwise, the model could easily get stuck in local minima like choosing the upper two layers to be identity functions and model the time series independently in the $\rv{g_d}$.
Additionally, our assumption of identity mean functions prevents pathological cases in which the complete model collapses to a constant function.

\Cref{fig:timeseries_alignment:dampened_sine_model_decomposition:d} shows the AMO-GP's recovered function decomposition and joint predictions.
The model successfully recovered a shared latent dampened sine function, a sigmoid warping for the first time series and an approximate quadratic alignment function for the second time series.
In \cref{fig:timeseries_alignment:dampened_sine_model_decomposition:a,fig:timeseries_alignment:dampened_sine_model_decomposition:b,fig:timeseries_alignment:dampened_sine_model_decomposition:c}, we show the training results of a standard GP, a multi-output GP and a three-layer deep GP on the same data.
For all of these models, we used RBF kernels and, in the case of the deep GP, applied priors similar to our model in order to avoid pathological cases.
In \cref{tab:timeseries_alignment:dampened_sine_model_log_likelihoods} we report test log-likelihoods for the presented models, which illustrate the qualitative differences between the models.
Because all models are non-parametric and converge well, repeating the experiments with different initializations leads to very similar likelihoods.

Both the standard GP and deep GP cannot learn dependencies between time series and revert back to the prior where no data is available.
The deep GP has learned that two layers are enough to model the data and the resulting model is essentially a Bayesian warped GP which has identified the sigmoid warping for $\Tc_1$.
Uncertainties in the deep GP are placed in the middle layer areas where no data are available for the respective time series, as sharing information between the two outputs is impossible.
In contrast to the other two models, the multi-output GP can and must share information between the two time series.
As discussed in \cref{toc:timeseries_alignment:model} however, it is constrained to constant time-offsets and cannot model the nonlinear alignment in the data.
Because of this, the model cannot recover the latent sine function and can only model one of the two outputs.


\subsection{Pairs of wind turbines}
\label{toc:timeseries_alignment:wind_example}
\begin{figure}[tp]
    \centering
    \includestandalonewithpath{figures/wind_joint_model}
    \caption{
        \label{fig:timeseries_alignment:wind_joint_model}
        The joint posterior for two time series $\rv{y_1}$ and $\rv{y_2}$ of power production for a pair of wind turbines.
        The top and bottom plots show the two observed time series with training data and dashed missing data.
        The AMO-GP recovers an uncertain relative alignment of the two time series shown in the middle plot.
        High uncertainty about the alignment is placed in areas where multiple explanations are plausible due to the high noise or missing data.
    }
\end{figure}
\begin{figure}[tp]
    \centering
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/wind_shallow_gp_samples_left}
        \caption{
            \label{fig:timeseries_alignment:wind_samples:a}
            Samples from a GP.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/wind_mo_gp_samples_left}
        \caption{
            \label{fig:timeseries_alignment:wind_samples:b}
            Samples from a MO-GP.
        }
    \end{subfigure}\\[\figureskip]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/wind_dgp_samples_left}
        \caption{
            \label{fig:timeseries_alignment:wind_samples:c}
            Samples from a DGP.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/wind_alignment_samples_left}
        \caption{
            \label{fig:timeseries_alignment:wind_samples:d}
            Samples from the AMO-GP.
        }
    \end{subfigure}
    \caption{
        \label{fig:timeseries_alignment:wind_samples}
        A comparison of noiseless samples drawn from a GP, a MO-GP, a DGP and the AMO-GP.
        The separation of uncertainties implied by the model structure of AMP-GP gives rise to an informative model.
        Since the uncertainty in the generative process is mainly placed in the relative alignment shown in \cref{fig:timeseries_alignment:wind_joint_model}, all samples in \cref{fig:timeseries_alignment:wind_samples:d} resemble the underlying data in structure.
    }
\end{figure}
\begin{table}[tp]
    \centering
    \caption{
        \label{tab:timeseries_alignment:dampened_sine_model_log_likelihoods}
        Test-log-likelihoods for the models presented in \cref{toc:timeseries_alignment:experiments}.
    }
    \newcolumntype{Y}{>{\centering\arraybackslash}X}
    \begin{tabularx}{\linewidth}{rrYYYY}
        \toprule
        Experiment & Test set                       & GP    & MO-GP  & DGP   & AMO-GP (Ours)  \\
        \midrule
        Artificial & $[0.7, 0.8] \subseteq \Tc_1$   & -0.12 & -0.053 & 0.025 & \textbf{1.54}  \\
                   & $[0.35, 0.65] \subseteq \Tc_2$ & -0.19 & -5.66  & -0.30 & \textbf{0.72}  \\
        \midrule
        Wind       & $[40, 45] \subseteq \Tc_2 $    & -4.42 & -2.31  & -1.80 & \textbf{-1.43} \\
                   & $[65, 75] \subseteq \Tc_2 $    & -7.26 & -0.73  & -1.93 & \textbf{-0.69} \\
        \bottomrule
    \end{tabularx}
\end{table}
This experiment is based on real data recorded from a pair of neighbouring wind turbines in a wind farm.
The two time series $\Tc_1$ and $\Tc_2$ shown in gray in \cref{fig:timeseries_alignment:wind_joint_model} record the respective power generation of the two turbines over the course of one and a half hours, which was smoothed slightly using a rolling average over 60 seconds.
There are 5400 data points for the first turbine (blue) and 4622 data points for the second turbine (green).
We removed two intervals (drawn as dashed lines) from the second turbine's data set to inspect the behaviour of the model with missing data.
This allows us to evaluate and compare the generative properties of our model in \cref{fig:timeseries_alignment:wind_samples}.

The power generated by a wind turbine is mainly dependent on the speed of the wind fronts interacting with the turbine.
For system identification tasks concerned with the behaviour of multiple wind turbines, associating the observations on different turbines due to the same wind fronts is an important task.
However it is usually not possible to directly measure these correspondences or wind propagation speeds between turbines, which means that there is no ground truth available.
An additional problem is that the shared latent wind conditions are superimposed by turbine-specific local turbulences.
Since these local effects are of comparable amplitude to short-term changes of wind speed, it is challenging to decide which parts of the signal to explain away as noise and which part to identify as the underlying shared process.

Our goal is the simultaneous learning of the uncertain alignment in time $\rv{a}$ and of the shared latent wind condition $\rv{f}$.
Modelling the turbine-specific parts of the signals is not the objective, so they need to be explained by the Gaussian noise term.
We use a squared exponential kernel as a prior for the alignment functions $\rv{a_d}$ and as smoothening kernels in $\rv{f}$.
For the given data set we can assume the output warpings $\rv{g_d}$ to be linear functions because there is only one dimension, the power generation, which in this data set is of similar shape for both turbines.
Again we encode a preference for alignments with slow dynamics with a prior on the length scales of $\rv{a_d}$.
As the signal has turbine-specific autoregressive components, plausible alignments are not unique.
To constrain the AMO-GP, we want it to prefer alignments close to the identity function which we chose as a prior mean function.

In non-hierarchical models, this can be achieved by placing a prior on the kernel's variance preferring smaller $\sigma_a^2$.
In our case, the posterior space of the alignment process $\rv{a}$ is a latent space we have not placed any prior on.
The model can therefore choose the posterior distribution of $\rv{u_a}$ in a way to counteract the constrained scale of $\mat{K_{au}}$ and $\mat{K_{uu}}\inv$ in \cref{eq:timeseries_alignment:augmented_joint}\todo{This equation does not exist anymore} and thereby circumvent the prior.
To prevent this, we also place a prior on the mean of $\rv{u_a}$ to remove this degree of freedom.

\Cref{fig:timeseries_alignment:wind_joint_model} shows the joint model learned from the data in which $a_1$ is chosen to be the identity function.
The possible alignments identified match the physical conditions of the wind farm.
For the given turbines, time offsets of up to six minutes are plausible and for most wind conditions, the offset is expected to be close to zero.
For areas where the alignment is quite certain however, the two time series are explained with comparable detail.
The model is able to recover unambiguous associations well and successfully places high uncertainty on the alignment in areas where multiple explanations are plausible due to the noisy signal.

As expected, the uncertainty about the alignment also grows where data for the second time series is missing.
This uncertainty is propagated through the shared function and results in higher predictive variances for the second time series.
Because of the factorization in the model however, we can recover the uncertainties about the alignment and the shared latent function separately.
\Cref{fig:timeseries_alignment:wind_samples} compares samples drawn from our model with samples drawn from a GP, a MO-GP and a DGP.
The GP reverts to their respective priors when data is missing, while the MO-GP does not handle short-term dynamics and smoothens the signal enough such that the nonlinear alignment can be approximated as constant.
Samples drawn from a DGP model showcase the complexity of a DGP prior.
Unconstrained composite GPs are hard to reason about and make the model very flexible in terms of representable functions.
Since the model's evidence is very broad, the posterior is uninformed and inference is hard.
Additionally, as discussed in~\cref{toc:timeseries_alignment:variational_approximation} and~\parencite{hensman_nested_2014}, the nested variational compression bound tends to loosen with high uncertainties.
AMO-GP shows richer structure:
Due to the constraints imposed by the model, more robust inference leads to a more informed model.
Samples show that it has learned that a maximum which is missing in the training data has to exist somewhere, but the uncertainty about the correct alignment due to the local turbulence means that different samples place the maximum at different locations in $\mat{X}$-direction.


\subsection{Conclusion}
\label{toc:timeseries_alignment:conclusion}
We have proposed the warped and aligned multi-output Gaussian process (AMO-GP), in which MO-GPs are embedded in a hierarchy to find shared structure in latent spaces.
We extended convolution processes \parencite{boyle_dependent_2004} with conditionally independent Gaussian processes on both the input and output sides, giving rise to a highly structured deep GP model.
This structure can be used to both regularize the model and encode expert knowledge about specific parts of the system.
By applying nested variational compression \parencite{hensman_nested_2014} to inference in these models, we presented a variational lower bound which combines Bayesian treatment of all parts of the model with scalability via stochastic optimization.

We compared the model with GPs, deep GPs and multi-output GPs on an artificial data set and showed how the richer model-structure allows the AMO-GP to pick up on latent structure which the other approaches cannot model.
We then applied the AMO-GP to real world data of two wind turbines and used the proposed hierarchy to model wind propagation in a wind farm and recover information about the latent non homogeneous wind field.
With uncertainties decomposed along the hierarchy, our approach handles ambiguities introduced by the stochasticity of the wind in a principled manner.
This indicates the AMO-GP is a good approach for these kinds of dynamical system, where multiple misaligned sensors measure the same latent effect.


\section{Interpretable Reinforcement Learning}
\label{toc:interpretable_reinforcement_learning}
In this paper, we present a model-based reinforcement learning system where the transition model is treated in a Bayesian manner.
The approach naturally lends itself to exploit expert knowledge by introducing priors to impose structure on the underlying learning task.
The additional information introduced to the system means that we can learn from small amounts of data, recover an interpretable model and, importantly, provide predictions with an associated uncertainty.
To show the benefits of the approach, we use a challenging data set where the dynamics of the underlying system exhibit both operational phase shifts and heteroscedastic noise.
Comparing our model to NFQ and BNN+LV, we show how our approach yields human-interpretable insight about the underlying dynamics while also increasing data-efficiency.


\subsection{Introduction}
\label{toc:interpretable_reinforcement_learning:introduction}
Machine learning methods \parencite{shalev-shwartz_understanding_2014} are designed to solve tasks where the underlying system we want to model is only partly known or understood.
The hope when using machine learning methods is that we can reduce this uncertainty by exploiting statistical patterns in data generated from the underlying system.

Reinforcement learning (RL)~\parencite{sutton_reinforcement_2018} is a machine learning paradigm designed to learn in a dynamic environment where we can specify a goal or have a notion of what a desirable behaviour is.
The goal of RL is to learn a policy which dynamically chooses actions in the environment in order to achieve a goal or a behaviour.
Specifically, an RL agent's task is to learn a policy $\pi$ which, given the current state $\mat{s}$ of an environment, chooses an action $\mat{a}$ to achieve the goal specified by a reward function $r$ mapping system states to numerical rewards.
To learn a policy, any RL system needs to understand the underlying dynamics governing the system, how the transition between states is effected by the actions taken.
The next state $\mat{s}^\prime$ is determined by the latent and possibly stochastic transition function $\mat{s}^\prime = \Fun*{f}{\mat{s}, \mat{a}}$.
How the dynamical system is treated is one of the main distinctions among different approaches to RL.
In model-based RL, the dynamic model is an explicit part of the system, while in the model-free counterpart the transition dynamics are implicit and cannot be disentangled from the system.

Applying RL in an industrial setting often implies trying to derive an alternative more efficient controller for an already existing system.
In a critical application it is unlikely that we will be able to deploy an untested policy as this can lead to safety issues.
This means that in practice we are often limited by previously collected data created by a different, possibly known, control mechanism in order to learn our model.
In the literature, this scenario is referred to as batch RL~\parencite{lange_batch_2012}, where we are presented with a set of state transitions $\Dc = \Set{(\mat{s}_n, \mat{a}_n, \mat{s}_n^\prime)}_{n=1}^N$ and are unable to interact with the original system to find a policy.
In order to be able to derive an efficient policy in this scenario, we need to use the available data as efficiently as possible.
Data efficiency in machine learning comes from reducing the search space of solutions.
In other words, data efficiency arises from being able to exploit as much prior knowledge of the system as possible \parencite{shalev-shwartz_understanding_2014}.

To be able to use the available data as efficiently as possible we therefore need a model which provides explicit and interpretable handles such that we can easily introduce priors.
The model-based approach to RL describes each component of the system in a modular fashion thereby providing an interface to incorporate prior knowledge.
The challenge is how these priors should be specified and how they should be included such that the hypothesis space can be limited in a manner coherent with our knowledge of the system.

Gaussian processes (GPs) are stochastic processes that can be used to specify probability distributions over the space of functions.
While a GP specifies a distribution with support for all functions, it efficiently concentrates its probability mass to functions with specific characteristics.
These characteristics make GPs well suited for RL as they do not impose hard constraints while still placing a significant structure on the space of functions.
In \parencite{deisenroth_pilco_2011} the authors propose a model-based RL method where Gaussian processes are used as priors for the dynamics.
They provide a principled approach of taking model uncertainty into account when evaluating the performance of a policy, thereby reducing the impact of model-bias.
However, the approach has several restrictions, transition dynamics are modelled as standard Gaussian processes (GPs) and policies and rewards must be of specific forms.

In this work, we will show how we can alleviate some of the limitations of \parencite{deisenroth_pilco_2011} to provide a richer and more efficient RL model.
We will show how we can introduce additional constraints on the dynamic model allowing for multiple transitional signatures to be active simultaneously.
Incorporating this knowledge facilitates learning by allowing us to more precisely state what we want to learn thereby significantly reducing the data requirements.
Furthermore, decomposing the transition model into several parts allows us to use reward shaping \parencite{sutton_reinforcement_2018} in order to discourage policies based on dynamic characteristics.

Introducing constraints on the dynamic model based on abstract knowledge is an inherently problem-dependent process.
This work explores this process for the heteroscedastic and bimodal Wet-Chicken benchmark~\parencite{tresp_wet_1994,hans_efficient_2009} which is both easy to understand and challenging to model.
A central challenge in this benchmark is how to formulate a model which can represent bimodalities.
One approach is presented in~\parencite{bishop_mixture_1994}, where multimodal regression tasks are interpreted as a density estimation problem.
A high number of candidate distributions is reweighed to match the observed data without modeling the underlying generative process.
Reformulated in a Bayesian framework using latent variables, this approach has been applied to the Wet-Chicken benchmark in~\parencite{depeweg_learning_2016,depeweg_decomposition_2018}.
However, such models are hard to interpret as they do not yield explicit models for the different modalities or their relative importance.
In this work, we are interested in formulating a dynamics model which yields new interpretable insights about the underlying system.
We formulate a probabilistic model which contains such explicit models by interpreting the Wet-Chicken benchmark as a data association problem~\parencite{barshalom_tracking_1990,cox_review_1993}.
While many probabilistic interpretations of this problem assume that the relative importance of different modes is constant~\parencite{lazaro-gredilla_overlapping_2012,bodin_latent_2017}, we base our formulation on the DAGP model~\parencite{kaiser_data_2018} which learns a non-parametric model of each mode and where the associations between the modes themselves is further controlled by a non-stationary stochastic process.

The paper is outlined as follows.
After introducing the Wet-Chicken benchmark, we show how high-level knowledge about this system can be used to impose Bayesian structure.
We derive an efficient inference scheme for both the dynamics model and for probabilistic policy search based on variational inference.
We show that this approach yields interpretable models and policies and is significantly more data-efficient than less interpretable alternatives.


\subsection{The Wet-Chicken Benchmark}
\label{toc:interpretable_reinforcement_learning:wetchicken}
In the Wet-Chicken problem~\parencite{tresp_wet_1994,hans_efficient_2009}, a canoeist is paddling in a two-dimensional river.
The canoeist's position at time $t$ is given by $\mat{s}_t = (x_t, y_t) \in \Rb^2$, where $x_t$ denotes the position along the river and $y_t$ the position across it.
The river is bounded by its length $l = 5$ and width $w = 5$.
There is a waterfall at the end of the river at $x = l$.
The canoeist wants to get close to the waterfall to maximize the reward $\Fun*{r}{\mat{s}_t} = r_t = x_t$.
However, if the canoeist falls down the waterfall, he has to start over at the initial position $(0, 0)$.

The river's flow consists of a deterministic velocity $v_t = y_t \cdot \sfrac{3}{w}$ and stochastic turbulence $b_t = 3.5 - v_t$, both of which depend on the position on the $y$-axis.
The higher $y_t$ is, the faster the river flows but also the less turbulent it becomes.
The canoeist chooses his paddle direction and intensity via an action $\mat{a}_t = (a_{t,x}, a_{t,y}) \in [-1, 1]^2$.
The transition function $f : (\mat{s}_t, \mat{a}_t) \mapsto \mat{s}_{t+1} = (x_{t+1}, y_{t+1})$ is given by
\begin{align}
    x_{t+1} & = \begin{cases}
        0             & \text{if } \hat{x}_{t+1} > l \\
        0             & \text{if } \hat{x}_{t+1} < 0 \\
        \hat{x}_{t+1} & \text{otherwise}
    \end{cases} &
    y_{t+1} & = \begin{cases}
        0             & \text{if } \hat{x}_{t+1} > l \text{ or } \hat{y}_{t+1} < 0 \\
        w             & \text{if } \hat{y}_{t+1} > w                               \\
        \hat{y}_{t+1} & \text{otherwise}
    \end{cases}
\end{align}
where
\begin{align}
    \begin{split}
        \hat{x}_{t+1} &= x_t + (1.5 \cdot a_{t, x} - 0.5)  + v_t + b_t \cdot \tau_t, \\
        \hat{y}_{t+1} &= y_t + a_{t, y},
    \end{split}
\end{align}
and $\tau_t \sim \Fun*{\Uni}{-1, 1}$ is a uniform random variable that represents the turbulence.

There is almost no turbulence at $y = w$, but the velocity is too high to paddle back.
Similarly, the velocity is zero at $y = 0$, but the canoeist can fall down the waterfall unpredictably due to the high turbulence.
A successful canoeist must find a balance between handling the stochasticity and velocities within the capabilities of the canoeist to get as close to the waterfall as possible.
However, as the canoeist moves closer to the waterfall, the distribution over the next states become increasingly more bi-modal as the probability of falling down increases.
Together with the heteroscedasticity introduced by the turbulence dependent on the current position, these properties make the Wet-Chicken problem especially difficult for model-based reinforcement learning problems.


\subsection{Probabilistic Policy Search}
\label{toc:interpretable_reinforcement_learning:probabilistic_policy_search}
\begin{figure}[t]
    \centering
    \includestandalone{wetchicken_graphical_model_rl}
    \caption{
        \label{fig:interpretable_reinforcement_learning:graphical_model:rl}
        The generative process for the return $J^\pi$, where violet nodes are observed and parameters are shown in yellow.
        It shows how starting from $\mat{s}_0$, a trajectory of length $T$ is generated with the policy parameterized by $\mat{\theta}_\pi$.
        The return is generated by the rewards which depend on their respective states only.
    }
\end{figure}
We are interested in finding a policy specified by the parameters $\mat{\theta}_\pi$ which maximizes the discounted return
$J^\pi(\mat{\theta}_\pi) = \sum_{t=0}^T \gamma^t \Fun*{r}{\mat{s}_t} = \sum_{t=0}^T \gamma^t r_t$ with a constant discount factor $\gamma \in [0, 1]$.
Starting from an initial state $\mat{s}_0$ we generate a trajectory of states $\mat{s}_0, \ldots, \mat{s}_T$ obtained by applying the action $\mat{a}_t = \Fun*{\pi}{\mat{s}_t}$ at every time step $t$.
The next state is generated using the (latent) transition function $f$, yielding $\mat{s}_{t+1} = \Fun*{f}{\mat{s}_t, \mat{a}_t}$.

Many environments have stochastic elements, such as the random drift in the Wet-Chicken benchmark from \cref{toc:interpretable_reinforcement_learning:wetchicken}.
We take this stochasticity into account by interpreting the problem from a Bayesian perspective where the discounted return specifies a generative model whose graphical model is shown in \cref{fig:interpretable_reinforcement_learning:graphical_model:rl}.
Because of the Markov property assumed in RL, conditional independences between the states yield a recursive definition of the state probabilities given by
\begin{align}
    \begin{split}
        \Prob{\mat{s}_{t+1} \given f, \mat{\theta}_\pi} &= \int \Prob{\Fun{f}{\mat{s}_t, \mat{a}_t} \given \mat{s}_t, \mat{a}_t} \Prob{\mat{a}_t \given \mat{s}_t, \mat{\theta}_\pi} \Prob{\mat{s}_t} \diff \mat{a}_t \diff \mat{s}_t, \\
        \Prob{r_t \given \mat{\theta}_\pi} &= \int \Prob{\Fun*{r}{\mat{s}_t} \given \mat{s}_t} \Prob{\mat{s}_t \given \mat{\theta}_\pi} \diff \mat{s}_t.
    \end{split}
\end{align}
With stochasticity or an uncertain transition model, the discounted return becomes uncertain and the goal can be reformulated to optimize the expected return
\begin{align}
    \Moment*{\E}{\Fun*{J^\pi}{\mat{\theta}_\pi}} = \sum_{t=0}^T \gamma^t \Moment*{\E_{\Prob{\mat{s}_t \given \mat{\theta}_\pi}}}{r_t}.
\end{align}

A model-based policy search method consists of two key parts~\parencite{deisenroth_pilco_2011}.
First, a dynamics model is learned from state transition data.
Second, this dynamics model is used to learn the parameters $\mat{\theta}_\pi$ of the policy $\pi$ which maximize the expected return $\Moment*{\E}{\Fun*{J^\pi}{\mat{\theta}_\pi}}$.
We discuss both steps in the following.


\subsubsection{An Interpretable Transition Model}
\label{toc:interpretable_reinforcement_learning:mdgp}
\begin{figure}[t]
    \centering
    \includestandalone{wetchicken_graphical_model_mdgp}
    \caption{
        \label{fig:interpretable_reinforcement_learning:graphical_model:mdgp}
        The graphical model for the DAGP-based transition model, where violet nodes are observed and variational parameters are blue.
        This model separates the flow-behaviour of the river $\mat{f}_t$, the heteroscedastic noise process $\mat{\sigma}_t$ and the possibility of falling down $\mat{\lambda}_t$.
        Latent variables $\mat{l}_t$ represent the belief that the $\nth{t}$ data point is a drop event.
    }
\end{figure}
We formulate a probabilistic transition model-based on high-level knowledge about the Wet-Chicken benchmark.
Importantly, we do not formulate a specific parametric dynamics model as would be required to derive a controller.
Instead, we make assumptions on a level typically available from domain experts.

We encode that given a pair of current state and action $\mat{\hat{s}}_t = \left( \mat{s}_t, \mat{a}_t \right)$, the next state $\mat{s}_{t+1}$ is generated via the combination of three things:
the deterministic flow-behaviour of the river $\mat{f}_t$, some heteroscedastic noise process $\mat{\sigma}_t$ and the possibility of falling down $\mat{\lambda}_t$.
This prior imposes structure which allows us to explicitly state what we want to learn from the data and where we do not assume prior knowledge:
How does the river flow?
What kind of turbulences exist?
When does the canoeist fall down?
How do the actions influence the system?

Each question is explicitly answered by one of the model's components.
In \cref{toc:interpretable_reinforcement_learning:results} we will visualize these components and discuss how they can be used by experts to gain new insights about the system.
Additionally, interpretable transition models help to build trust in derived policies:
Since experts can assess the plausibility of the transition model, successful policies are unlikely to behave unexpectedly on the true system.

We formulate a graphical model in \cref{fig:interpretable_reinforcement_learning:graphical_model:mdgp} using the data association with GPs (DAGP) model~\parencite{kaiser_data_2018}, which allows us to handle the multi-modality introduced by falling down the waterfall.
We specify this separation via the marginal likelihood
\begin{align}
    \begin{split}
        \label{eq:interpretable_reinforcement_learning:true_marginal_likelihood}
        &\Prob*{\mat{s}_{t+1} \given \mat{\hat{s}}_t} =
        \int
        \Prob*{\mat{s}_{t+1} \given \mat{\sigma}_t, \mat{f}_t, \mat{l}_t}
        \Prob*{\mat{l}_t \given \mat{\hat{s}}_t}
        \Prob*{\mat{\sigma}_t \given \mat{\hat{s}}_t}
        \Prob*{\mat{f}_t \given \mat{\hat{s}}_t}
        \diff \mat{\sigma}_t \diff \mat{l}_t \diff \mat{f}_t,
    \end{split}
\end{align}
where $\mat{f}_t = \left( \mat{f}_t^{\pix{1}}, \dots, \mat{f}_t^{\pix{K}} \right)$.
The marginal likelihood consists of the two GPs $\Prob*{\mat{\sigma}_t \given \mat{\hat{s}}_t}$ and $\Prob*{\mat{f}_t \given \mat{\hat{s}}_t}$ and the two likelihoods
\begin{align}
    \begin{split}
        &\Prob*{\mat{s}_{t+1} \given \mat{\sigma}_t, \mat{f}_t, \mat{l}_t} =
        \prod_{k=1}^K
        \Gaussian*{\mat{s}_{t+1} \given \mat{f}_t^{\pix{k}}, \left(\mat{\sigma}_t^{\pix{k}}\right)^2}_{^{\displaystyle,}}^{\Fun{\Ind}{l_t^{\pix{k}} = 1}} \\
        %
        &\qquad\qquad\Prob*{\mat{l}_t \given \mat{\hat{s}}_t} =
        \int \Multinomial*{\mat{l}_t \given \Fun{\softmax}{\mat{\lambda}_t}} \Prob*{\mat{\lambda}_t \given \mat{\hat{s}}_t} \diff \rv{\lambda}_t
    \end{split}
\end{align}
where $\Multi$ denotes a multinomial distribution.
These likelihood describe the regression and classification tasks implied by the problem respectively:
In our case, we use $K = 2$ modes, one for staying in the river and one for falling down the waterfall.
For every data point we infer a posterior belief $\Prob{\mat{l}_t}$ about which mode the data point belongs to, as we assume this separation can not be predetermined using expert knowledge.

We place independent GP priors on the $\mat{f}^{\pix{k}}$, $\mat{\sigma}^{\pix{k}}$ and $\mat{\lambda}^{\pix{k}}$.
Given the data a fixed set of assignments $\mat{L}$, our modelling assumptions imply independence between the $K$ modes.
However, this independence is lost if the assignments are unknown and a discrete optimization problem has to be solved when doing joint inference over the different modes and the association problem.
We approximate the exact posterior via a factorized variational distribution
\begin{align}
    \Variat*{\mat{f}, \mat{\lambda}, \mat{\sigma}, \mat{U}} & =
    \prod_{k=1}^K\prod_{t=1}^T \Variat{\mat{f}_t^{\pix{k}}, \mat{u}^{\pix{k}}} \Variat{\mat{\lambda}_t^{\pix{k}}, \mat{u_\lambda}^{\pix{k}}} \Variat{\mat{\sigma}_t^{\pix{k}}, \mat{u_\sigma}^{\pix{k}}}
\end{align}
which introduces variational inducing inputs and outputs $\mat{U}$ as described in~\parencite{damianou_deep_2013,hensman_scalable_2015,kaiser_data_2018}.
These inducing inputs independently characterize the respective model parts and enable us to do inference via stochastic optimization.

The variational parameters are optimized by minimizing a lower bound to the marginal likelihood which can be efficiently computed via sampling and enables stochastic optimization.
\begin{align}
    \begin{split}
        \label{eq:interpretable_reinforcement_learning:variational_bound}
        \Lc_{\text{DAGP}} &= \Moment*{\E_{\Variat*{\mat{F}, \mat{\lambda}, \mat{\sigma}, \mat{U}}}}{\log\frac{\Prob*{\mat{S}^\prime, \mat{F}, \mat{\lambda}, \mat{\sigma}, \mat{U} \given \mat{\hat{S}}}}{\Variat*{\mat{F}, \mat{\lambda}, \mat{\sigma}, \mat{U}}}} \\
        &=
        \sum_{t=1}^T \Moment*{\E_{\Variat*{\mat{f}_t}}}{\log \Prob*{\mat{s}^\prime_t \given \mat{f}_t, \mat{\lambda}_t, \mat{\sigma}_t}}
        + \sum_{t=1}^T \Moment*{\E_{\Variat*{\mat{\lambda}_t}}}{\log \Prob*{\mat{l}_t \given \mat{\lambda}_t}} \\
        &\quad
        - \sum_{k=1}^K \KL{\Variat*{\mat{u}^{\pix{k}}, \mat{u_\lambda}^{\pix{k}}, \mat{u_\sigma}^{\pix{k}}}}{\Prob*{\mat{u}^{\pix{k}}, \mat{u_\lambda}^{\pix{k}}, \mat{u_\sigma}^{\pix{k}}}}
    \end{split}
\end{align}
To gain informative gradients with respect to the assignments $\mat{l}$ and assignment process $\mat{\lambda}$, we use a continuous relaxation based on Concrete random variables~\parencite{maddison_concrete_2016}.
We represent the belief about $\mat{l}_t$ as a $K$-dimensional discrete distribution $\Variat{\mat{l}_t}$.
Instead of drawing discrete samples from $\Variat{\mat{l}_t}$ when calculating $\Lc_{\text{DAGP}}$, we draw samples $\mat{\hat{l}}_t$ from a concrete random variable.
Based on a temperature parameter, concrete random variables yield samples which are almost discrete but which still yield informative gradients.
For details we refer to~\parencite{kaiser_data_2018}.

We obtain an explicit representation of the GP posteriors during variational inference which allows us to efficiently propagate samples through the model to simulate trajectories used for policy search.
Predictions for an unknown state $\mat{\hat{s}}_\ast$ are mixtures of $K$ independent Gaussians given by,
\begin{align}
    \begin{split}
        \label{eq:interpretable_reinforcement_learning:predictions}
        \Variat{\mat{s}^\prime_\ast \given \mat{\hat{s}}_\ast}
        &= \int \sum_{k=1}^K \Variat{l_\ast^{\pix{k}} \given \mat{\hat{s}}_\ast} \Variat{\mat{s}_\ast^{\prime\pix{k}} \given \mat{\hat{s}}_\ast} \diff \mat{l}_\ast \\
        &= \int \sum_{k=1}^K \Variat{l_\ast^{\pix{k}} \given \mat{\hat{s}}_\ast} \Variat{\mat{s}_\ast^{\prime\pix{k}} \given \mat{f}_\ast^{\pix{k}} \mat{\sigma}_\ast^{\pix{k}}} \Variat{\mat{f}_\ast^{\pix{k}}, \mat{\sigma}_\ast^{\pix{k}} \given \mat{\hat{s}}_\ast} \diff \mat{l}_\ast \diff \mat{f}_\ast \diff \mat{\sigma}_\ast \\
        &\approx \sum_{k=1}^K \tilde{l}_\ast^{\pix{k}} \mat{\tilde{s}}_\ast^{\prime\pix{k}}.
    \end{split}
\end{align}
We sample from the assignment process $\mat{l}_\ast$ and heteroscedastic noise process $\mat{\sigma}_\ast$.
The $K$ predictive posteriors $\Variat{\mat{s}_\ast^{\prime\pix{k}} \given \mat{f}_\ast^{\pix{k}} \mat{\sigma}_\ast^{\pix{k}}}$ are then given by $K$ independent shallow GPs and can be computed analytically.


\subsubsection{Policy Learning}
\label{toc:interpretable_reinforcement_learning:policy}
After training a transition model, we use the variational posterior $\Variat{\mat{s}_{t+1} \given \mat{\hat{s}}_t}$ to train a policy by sampling roll-outs and optimizing policy parameters via stochastic gradient descent on the expected return $\Moment*{\E}{\Fun*{J^\pi}{\mat{\theta}_\pi}}$.
The expected return is approximated using the variational posterior given by
\begin{align}
    \begin{split}
        \label{eq:interpretable_reinforcement_learning:policy_training}
        \Moment*{\E}{\Fun*{J^\pi}{\mat{\theta}_\pi}}
        &= \sum_{t=0}^T \gamma^t \Moment*{\E_{\Prob{\mat{s}_t \given \mat{\theta}_\pi}}}{\mat{r}_t}
        \approx \sum_{t=0}^T \gamma^t \Moment*{\E_{\Variat{\mat{s}_t \given \mat{\theta}_\pi}}}{\mat{r}_t} \\
        &= \int \sum_{t=0}^T \Bigg[ \gamma^t \Moment*{\E_{\Variat{\mat{s}_t \given \mat{\theta}_\pi}}}{\mat{r}_t} \Bigg] \Prob{\mat{s_0}} \prod_{t=0}^{T-1} \Variat{\mat{s}_{t+1} \given \mat{s}_t, \mat{\theta}_\pi} \diff \mat{s}_0 \dots \diff\mat{s}_T \\
        &\approx \frac{1}{P} \sum_{p=1}^P \sum_{t=0}^T \gamma^t r_t^p.
    \end{split}
\end{align}
We expand the expectation to explicitly show the marginalization of the states in the trajectory.
Due to the Markovian property of the transition dynamics, the integral factorizes along $t$.
The integral is approximated by averaging over $P$ samples propagated through the model starting from a known distribution of initial states $\Prob{\mat{s}_0}$.
State transitions can be efficiently sampled from the variational posterior of the dynamics model by repeatedly taking independent samples of the different GPs.

The expected return in \cref{eq:interpretable_reinforcement_learning:policy_training} can be optimized using stochastic gradient descent via the gradients
\begin{align}
    \label{eq:interpretable_reinforcement_learning:policy_gradients}
    \nabla_{\theta_\pi} \Fun*{J^\pi}{\theta_\pi} \approx \frac{1}{P} \sum_{p=1}^P \sum_{t=0}^T \gamma^t \nabla_{\theta_\pi} r_t^p
\end{align}
of the Monte Carlo approximation as they are an unbiased estimator of the true gradient.
The gradients of the samples can be obtained using automatic differentiation tools such as TensorFlow \parencite{abadi_tensorflow_2015}.
The $P$ roll-outs can be trivially parallelized.
Importantly, we only need a small number of Monte Carlo samples at every iteration, since we use the gradients of the samples directly.


\subsection{Results}
\label{toc:interpretable_reinforcement_learning:results}
\begin{figure}[tp]
    \centering
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalone{wetchicken_speed_mean}
        \caption{
        \label{fig:interpretable_reinforcement_learning:wetchicken:speed}
        % Flow $\Moment{\E}{\Delta\mat{x}_{t+1} \given \text{no drop}}$
        Flow $\Delta\mat{x}^{\pix{1}}$
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalone{wetchicken_hetero_noise}
        \caption{
            \label{fig:interpretable_reinforcement_learning:wetchicken:hetero}
            Heteroscedastic turbulence $\mat{\sigma}_x$
        }
    \end{subfigure}\\[\figureskip]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalone{wetchicken_drop_mean}
        \caption{
        \label{fig:interpretable_reinforcement_learning:wetchicken:drop}
        Drop $\mat{x}^{\prime\pix{2}}$
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalone{wetchicken_falldown_probabilities}
        \caption{
        \label{fig:interpretable_reinforcement_learning:wetchicken:falldown}
        Drop probability $\mat{\lambda}^{\pix{2}}$
        }
    \end{subfigure}
    \caption{
        \label{fig:interpretable_reinforcement_learning:wetchicken:dynamics}
        The separation of different aspects of the Wet-Chicken in DAGP-based transition models benchmark yields new and interpretable information about the underlying dynamics.
        The different parts of the model explicitly show flow speeds (\cref{fig:interpretable_reinforcement_learning:wetchicken:speed}), turbulence (\cref{fig:interpretable_reinforcement_learning:wetchicken:hetero}), drop behaviour (\cref{fig:interpretable_reinforcement_learning:wetchicken:drop}) and drop probabilities (\cref{fig:interpretable_reinforcement_learning:wetchicken:falldown}) with respect to the current position in the river and action $a = 0$.
        The model has learned that the river is turbulent on the left and fast on the right, leading to consistent medium drop probabilities on the left due to stochasticity and a sharp boundary on the right, where the flow speed dominates.
        Note that a drop resets the position to the initial state irrespective of the current state.
        It is therefore correctly learned to be represented by the constant zero function (\cref{fig:interpretable_reinforcement_learning:wetchicken:drop}).
    }
\end{figure}
\begin{figure}[tp]
    \centering
    \includestandalone{wetchicken_cut_0_0}
    \includestandalone{wetchicken_cut_1_0}
    \includestandalone{wetchicken_cut_2_5}
    \includestandalone{wetchicken_cut_4_0}
    \includestandalone{wetchicken_cut_5_0}
    \caption{
        \label{fig:interpretable_reinforcement_learning:wetchicken:cut}
        Linear cuts through the DAGP-based transition model with the waterfall on the right at $x_t = 5$.
        The plots show the dependency between $x_t$ and $x_{t+1}$ with respect to the action $a_t = 0$ and, from top to bottom, $y_t \in \Set{0, 1, 2.5, 4, 5}$.
        The DAGP-based transition model successfully separates the two modes introduced through flow (blue) and drop (green) behaviours and predicts the probability of being assigned to the drop mode (violet).
        While drops can be modelled using a constant noiseless function, the flow speed (gradient and bias) and heteroscedastic noise (variance) varies in the different cuts.
        For low $y_t$, the river flows slowly but is very turbulent, while for high $y_t$, the river flows fast but deterministically.
    }
\end{figure}
To solve the Wet-Chicken problem, we first train the dynamics model on batch data sampled from the true dynamics and then optimize neural policies with respect to this dynamics model.
As the DAGP-based dynamics model is designed to be interpretable, we first discuss how, additionally to a joint posterior, the independent posteriors of its components yield insights about the Wet-Chicken problem.
We then show how successful policies can be found with less data compared to the model-free NFQ~\parencite{riedmiller_neural_2005} and model-based Bayesian Neural Networks with latent variables (BNN+LV)~\parencite{depeweg_learning_2016}, two approaches which do not makes use of high-level expert knowledge.
Thirdly, we show how the human-interpretable components of the dynamics models can be used for reward shaping, allowing us to easily formulate a requirement for conservative policies.


\subsubsection{Dynamics Model}
\label{toc:interpretable_reinforcement_learning:dynamics_model}
The benchmark has two-dimensional state and action spaces from which we sample uniform random transitions with varying $N$ in the range \numrange{100}{5000}.
For $N \geq 250$, our model is able to identify the underlying dynamics.
In \cref{fig:interpretable_reinforcement_learning:wetchicken:cut} we show the joint predictive posterior of a DAGP-based transition model.
The different plots show linear cuts through the Wet-Chicken system with respect to the action $a_t = 0$ and $y_t \in \Set{0, 1, 2.5, 4, 5}$.
The transition model has successfully identified the two modes introduced through flow and drop behaviours and their relative importance.
These cuts require additional examination to recover new knowledge about the system's behaviour.
In contrast, the separation of the learning problem in the DAGP-based dynamics model gives us explicit and separate posteriors about the different system components via the independent GP posteriors shown in \cref{fig:interpretable_reinforcement_learning:wetchicken:dynamics}.
This belief can directly be reasoned about with experts to evaluate the environment in which policies will be trained, raising confidence in their correctness.

While drops can be modelled using a constant noiseless function, the flow speed and heteroscedastic turbulence varies throughout the system.
For low $y$ the river flows slowly but is very turbulent while for high $y$ the river flows fast but deterministically.
In the turbulent regime, falling down is possible but not certain for most $x$, while in the flow dominated regime, a drop becomes highly probable under a certain distance from the waterfall.
Note that even though the turbulence as defined in \cref{toc:interpretable_reinforcement_learning:wetchicken} is independent of $x$, the heteroscedastic noise process has uncovered the implicit dependency for high $x$ as most possible turbulence values lead to falling down and thus assignment to the other mode.
Similarly, the flow speed shown in \cref{fig:interpretable_reinforcement_learning:wetchicken:speed} is negative in the top left corner.
This is due to the fact that the flow mode models the position after one step under the condition of not falling.
As most turbulence into the direction of the waterfall leads to a drop, the posterior mean is further away from the waterfall as the turbulence dominates the low flow speed on the left side of the river.


\subsubsection{Policy Learning}
\label{toc:interpretable_reinforcement_learning:policy_learning}
\begin{figure}[tp]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \sisetup{
            table-format=-1.2(2),
            table-number-alignment=center,
            separate-uncertainty,
            % table-align-uncertainty,
            table-figures-uncertainty=1,
            detect-weight,
        }
        % \newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}
        % \footnotesize
        % \setlength{\tabcolsep}{1pt}
        \begin{tabular}{cSSSS}
            \toprule
            {N}  & {NFQ}                   & {BNN+LV}                & {GP}                    & {DAGP}                  \\
            \midrule
            100  & 0.66 \pm 0.16           & {---}                   & \bfseries 1.41 \pm 0.01 & 1.18 \pm 0.09           \\
            250  & 1.71 \pm 0.07           & 1.62 \pm 0.20           & 1.54 \pm 0.01           & \bfseries 2.33 \pm 0.01 \\
            500  & 1.60 \pm 0.10           & 2.18 \pm 0.07           & 1.56 \pm 0.01           & \bfseries 2.25 \pm 0.01 \\
            1000 & 1.99 \pm 0.06           & 2.27 \pm 0.01           & 2.13 \pm 0.01           & \bfseries 2.32 \pm 0.01 \\
            2500 & 2.26 \pm 0.02           & \bfseries 2.30 \pm 0.01 & 1.91 \pm 0.01           & 2.28 \pm 0.01           \\
            5000 & \bfseries 2.33 \pm 0.01 & 2.30 \pm 0.01           & 1.91 \pm 0.01           & 2.28 \pm 0.01           \\
            \bottomrule
        \end{tabular}
        \vspace*{1ex}
        \caption{
            \label{fig:interpretable_reinforcement_learning:wetchicken:table}
            Comparison of expected returns
        }
    \end{subfigure}\\[\figureskip]
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includestandalone{wetchicken_policy_quiver}
        \caption{
            \label{fig:interpretable_reinforcement_learning:wetchicken:policy}
            A successful Wet-Chicken policy
        }
    \end{subfigure}
    \caption{
        \label{fig:interpretable_reinforcement_learning:wetchicken_policy}
        Using interpretable DAGP-based transition models with structurally informative priors, successful policies can be learned based on 250 observations.
        In contrast, about 2500 observations are needed to find a policy using the model-free NFQ.
        As GP based transition models are not capable of representing bimodal dynamics, training does not yield successful policies.
        The chosen optimal movement direction of a successful policy (right) is denoted by both the arrows and background color.
    }
\end{figure}
\begin{figure}[tp]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \sisetup{
            table-format=-1.2(2),
            table-number-alignment=center,
            separate-uncertainty,
            % table-align-uncertainty,
            table-figures-uncertainty=1,
            detect-weight,
        }
        % \footnotesize
        % \setlength{\tabcolsep}{1pt}
        \begin{tabular}{cSSS[table-format=2.1(1)]}
            \toprule
            {Training}        & {Original}              & {Conservative}           & {Drop}                 \\
            {Reward}          & {Return}                & {Return}                 & {\%}                   \\
            \midrule
            {$r_\text{orig}$} & \bfseries 2.32 \pm 0.01 & -1.22 \pm 0.02           & 21.8 \pm 0.2           \\
            {$r_\text{cons}$} & 2.17 \pm 0.01           & \bfseries -1.00 \pm 0.01 & \bfseries 18.9 \pm 0.1 \\
            \bottomrule
        \end{tabular}
        \vspace*{1ex}
        \caption{
            \label{fig:interpretable_reinforcement_learning:wetchicken:conservative_table}
            Drop risk reduction with reward shaping
        }
    \end{subfigure}\\[\figureskip]
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includestandalone{wetchicken_conservative_policy_quiver}
        \caption{
            \label{fig:interpretable_reinforcement_learning:wetchicken:conservative_policy}
            A conservative Wet-Chicken policy
        }
    \end{subfigure}
    \caption{
        \label{fig:interpretable_reinforcement_learning:conservative_wetchicken_policy}
        As the different components of a DAGP-based transition model are easily interpretable, they can be used for reward shaping.
        We formulate a conservative reward function $r_\text{cons}$ which penalizes drops and can easily be evaluated in the transition model.
        A resulting policy (right) has worse return with respect to the original reward function $r_\text{orig}$ but effectively reduces the risk of falling.
    }
\end{figure}
Given a posterior for the dynamics model, we now train a neural policy using probabilistic model rollouts.
We sample initial states from the training data, use a horizon of $T = 5$ steps and average over $P = 20$ samples with $\gamma = 0.9$.
We use a two-layer neural network with 20 ReLU-activated units each as our policy parametrization.
For every state transition, we sample independently from the different model components to generate a sample for the next state using \cref{eq:interpretable_reinforcement_learning:predictions}.
This incorporates both the stochasticity in the system introduced via heteroscedastic noise and the Bayesian uncertainty about the correct model in the policy search.
During training, the policy thus implicitly learns to consider the stochasticity of the Wet-Chicken benchmark as different sample-trajectories generate gradients with respect to different realizations of the stochasticity of the Wet-Chicken benchmark.
\Cref{fig:interpretable_reinforcement_learning:wetchicken:policy} shows how a successful policy has found a trade-off between the unpredictability on the left and the uncontrollable speed on the right.

In Table~\ref{fig:interpretable_reinforcement_learning:wetchicken:table}, we compare policy search based on the DAGP-based dynamics model with a standard GP dynamics model and NFQ.
We present expected returns for training runs with different amounts of data averaged over 10 experiments together with standard errors.
A policy applying uniformly random actions yields a return of about \num{1.5} and a return above \num{2.2} indicates that a successful policy has been found.
We ran NFQ for 20 full model learning and sampling iterations using a neural network with one 10-unit hidden layer with sigmoid activations.

A standard GP cannot model heteroscedastic noise or multi-modality.
For any point in the input space, the GP can therefore only predict that the agent will always fall down, never fall down or, via very high uncertainties, that any state in system is possible.
None of these possibilities represent the dynamics well enough to allow the policy search to derive a policy, illustrating our need for a more structured model.
For $N \geq 250$, the DAGP-based dynamics model identifies the underlying dynamics well and policies can be found reliably.

BNN+LV is a more expressive model that can represent both heteroscedasticity and multi-modality.
Due to the model's structure however, it is hard to incorporate high-level expert knowledge and therefore, more sturcture has to be learned from the data.
BNN+LV reliably finds good policies for $N \geq 1000$ and sometimes finds good policies for $N = 500$.
As this approach is model-based and formulates a reasonable general-purpose prior on the wfor the dynamics, the results fall between the more informed DAGP, which is successful with less data, and NFQ, which is more uninformed.

NFQ is a model-free approach.
Instead of learning a dynamics model and using rollouts in that model to find a good policy, NFQ directly models the optimal Q-function and thus the optimal policy.
A Q-function represents the expected return after taking a specified action in a specified state.
Since the expected return already takes into account both the heteroscedasticity and multi-modality of the system, the Q-function itself can be modelled with a standard function approximator such as a neural network.
Thus, no special modelling is needed when applying NFQ to the Wet-Chicken benchmark and given enough data, NFQ is able to find successful policies.
However, at the same time, not modelling the dynamics explicitly also prevents us from utilising the high-level expert knowledge we have about the system, thus increasing the required amount of data:
Using DAGP-based dynamics models, a successful policy can be found with about an order of magnitude less data.


\subsubsection{Reward Shaping}
\label{toc:interpretable_reinforcement_learning:reward_shaping}
We have shown how a dynamics model informed by high-level expert knowledge increases data efficiency.
A further advantage of the decomposition of the dynamics model in interpretable components is that the predictions of these components can be used to influence the policy search.
In this example, we want to find a more conservative policy which, when compared to \cref{fig:interpretable_reinforcement_learning:wetchicken:policy}, sacrifices some return in order to avoid falling down the waterfall.

Any successful agent has the implicit incentive to avoid drops as they move the canoeist away from the waterfall.
However, a successful policy still accepts that it will fall down sometimes due to turbulence.
To encourage more conservative behaviour, we use a conservative reward
\begin{align}
    \Fun*{r_\text{cons}}{\mat{s}} =
    \Fun*{r_\text{orig}}{\mat{s}} \cdot \left(1 - \Prob{\text{drop} \given \mat{s}}\right)
    - 5 \cdot \Prob{\text{drop} \given \mat{s}}
\end{align}
which includes the original Wet-Chicken reward function $\Fun*{r_\text{orig}}{(x, y)} = x$.
For every state, the DAGP-based dynamics model yields an explicit drop-probability which can easily be evaluated.
The conservative reward punishes a high drop probability reweighed with the maximum original reward $\max_{\mat{s}} r_\text{orig}(\mat{s}) = 5$.

\Cref{fig:interpretable_reinforcement_learning:conservative_wetchicken_policy} shows a resulting conservative policy.
Such a policy avoids both the turbulent states on the left and the fast flowing states on the right.
It tries to reach a sweet spot, which, compared to \cref{fig:interpretable_reinforcement_learning:wetchicken:policy}, is further away from the waterfall and therefore safer.
We compare 10 runs with $N=1000$ observations using the original reward and the conservative reward.
The resulting conservative policies yield lower return than the more aggressive default policies but reliably reduce drop probabilities as well.
The interpretable nature of the dynamics models have allowed us to easily influence policy behaviours.


\subsubsection{Effects of Model Misspecification}
\label{toc:interpretable_reinforcement_learning:model_misspecification}
\begin{table}[t]
    \centering
    \caption{
        \label{tab:interpretable_reinforcement_learning:wetchicken:mode_table}
        Comparison of expected returns for different settings of $K$
    }
    \sisetup{
        table-format=-1.2(2),
        table-number-alignment=center,
        separate-uncertainty,
        % table-align-uncertainty,
        table-figures-uncertainty=1,
        detect-weight,
    }
    \newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}
    % \footnotesize
    \begin{tabular}{cSSSSS}
        \toprule
        {}   & \multicolumn{5}{c}{DAGP}                                                                           \\
        \cmidrule(lr){2-6}
        {N}  & {$K=1$}                  & {$\mat{K=2}$}           & {$K=3$}       & {$K=4$}       & {$K=5$}       \\
        \midrule
        250  & 1.41 \pm 0.01            & \bfseries 2.33 \pm 0.01 & 1.64 \pm 0.38 & 1.31 \pm 0.25 & 1.65 \pm 0.08 \\
        500  & 1.54 \pm 0.01            & \bfseries 2.25 \pm 0.01 & 1.97 \pm 0.23 & 1.48 \pm 0.21 & 2.14 \pm 0.10 \\
        1000 & 2.13 \pm 0.01            & \bfseries 2.32 \pm 0.01 & 1.99 \pm 0.17 & 2.09 \pm 0.12 & 2.16 \pm 0.09 \\
        2500 & 1.91 \pm 0.01            & \bfseries 2.28 \pm 0.01 & 2.15 \pm 0.03 & 2.06 \pm 0.12 & 2.17 \pm 0.03 \\
        5000 & 1.91 \pm 0.01            & \bfseries 2.28 \pm 0.01 & 2.19 \pm 0.06 & 1.95 \pm 0.16 & 2.08 \pm 0.13 \\
        \bottomrule
    \end{tabular}
\end{table}
\begin{figure}[tp]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includestandalone{wetchicken_misspecification_good_cut_5_0}
        \includestandalone{wetchicken_misspecification_good_assignment_5_0}
        \caption{
            \label{fig:interpretable_reinforcement_learning:wetchicken:good_cut}
            Transition model with $K=4$ and successful policy training
        }
    \end{subfigure}\\[\figureskip]
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includestandalone{wetchicken_misspecification_bad_cut_5_0}
        \includestandalone{wetchicken_misspecification_bad_assignment_5_0}
        \caption{
            \label{fig:interpretable_reinforcement_learning:wetchicken:bad_cut}
            Transition model with $K=4$ and failed policy training
        }
    \end{subfigure}
    \caption{
        \label{fig:interpretable_reinforcement_learning:wetchicken:cut_comparison}
        Comparison of linear cuts through two DAGP-based transition models with $K=4$ at $y_t = 5$ and $a_t=0$.
        The respective upper plots show the predictive posterior of the different modes while the lower plots show assignment probabilities to the different modes.
        For both models, one mode (green, dotted) model drops and two modes (blue and yellow, dashed) represent flow behaviour.
        A third more uninformed mode (violet) is almost irrelevant in the first model but explains some data through a high noise variance in the second model.
        A significant amount of predictions from the second model are uninformed, leading to the failure of the policy search.
    }
\end{figure}
In \cref{toc:interpretable_reinforcement_learning:probabilistic_policy_search} we have formulated a dynamics model informed by high-level expert knowledge.
One important insight we assumed is the bimodal nature of the Wet-Chicken problem introduced by the waterfall.
In \cref{toc:interpretable_reinforcement_learning:policy_learning}, we compared our model to standard GPs and showed that modelling multi-modality is critical to solve Wet-Chicken.
We extend this comparison in this experiment and discuss the effects of model misspecification on our model's performance.
Specifically, we investigate the case where additional modes are available to dynamics model to solve the underlying data association problem.

\Cref{tab:interpretable_reinforcement_learning:wetchicken:mode_table} shows results for $K \in \Set{1, \dots, 5}$, where $K=1$ is equivalent to standard GPs.
All models have been trained for the same number of iterations and, for $K > 1$, all models have comparable marginal likelihoods.
While $250$ data points are enough with $K=2$ to reliably solve the Wet-Chicken problem, more data is needed until working policies can be found with $K>2$ (e.g.\ for $K=5$, double the data was needed until one of the runs found a working policy).
Most notably, performance fluctuates significantly with misspecified models for different repetitions of the same experiment and good policies can not be found reliably.

Using the additional modes available, the model can now find representations of the systems where multiple modes jointly represent the river's flow.
This showcases how data association problems are inherently ill-posed in general~\parencite{barshalom_tracking_1990,cox_review_1993}.
The additional representative power for $K>2$ introduces symmetries in the optimization landscape which both significantly complicate training~\parencite{lazaro-gredilla_overlapping_2012,minka_expectation_2001} and lead to undesired generalization behaviour which is not driven by knowledge about the underlying system.

An example for undesired generalization is shown in \cref{fig:interpretable_reinforcement_learning:wetchicken:cut_comparison} which compares two models trained with $K=4$ and $N=2500$.
While both models explain the overall training data well, the cuts through the system at $y_t = 5$ give an intuition why the first model leads to a successful policy, while the second model does not.
Both models represent drops via one of the modes and share the remaining three modes to jointly explain the flow behaviour.
In the first model, two alternating modes have learned essentially equivalent models and a third more uninformed mode is almost irrelevant.
The second model is similar, but the uninformed mode's model is closer to the RBF prior and more relevant.
Note that due to the high noise variance, this choice of model still explains the data only slightly worse.
Still, the second model does not generalize according to the underlying system.
A significant amount of predictions from the second model are uninformed, leading to the failure of the policy search.

Significantly longer training or specialized optimization schemes may lead to robust inference for $K>2$.
However, this experiment shows the significance of encoding available abstract prior knowledge to avoid pathologic model behaviours.
Models for $K=2$ both reliably identify the system using standard optimization methods and yield immediately interpretable results.


\subsection{Conclusion and Discussion}
\label{toc:interpretable_reinforcement_learning:conclusion}
In this paper we have presented a Bayesian reinforcement learning model-based on non-parametric Gaussian process priors.
The model is motivated by the observation that in real world scenarios high-level prior knowledge of the system dynamics is often available.
We believe that many tasks are characterised by dynamics that can be decomposed into several attributes.
For example, when a physical structure is excited by a force oscillating at its natural frequency its response will change drastically.
The approach we have presented is based on learning a modular dynamic model which decomposes this type of transitional behaviour into separate components.
The model learns both the individual components and the underlying structure of how the components interact within the system.
The use of Gaussian process priors to quantify the uncertainty within components allows us to perform probabilistic policy search.

The interpretable structure of our model facilitates data efficient learning by easily incorporating prior knowledge.
We showed experimentally how this significantly reduces the data requirements compared to a model free approach.
Furthermore, the same knowledge can be used as a means for directing the policy search by discouraging solutions which exhibit a specific dynamic, such as avoiding falling down the waterfall in the Wet-Chicken benchmark.


\section{Discussion}
\label{toc:discussion}
?
