
@article{abadi_tensorflow_2015,
  title = {{{TensorFlow}}: {{Large}}-{{Scale Machine Learning}} on {{Heterogeneous Systems}}},
  author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  date = {2015},
  url = {https://www.tensorflow.org/},
  file = {/home/markus/Zotero/storage/WLKK3HI7/Abadi et al. - 2015 - TensorFlow Large-Scale Machine Learning on Hetero.pdf}
}

@inproceedings{alvarez_efficient_2010,
  title = {Efficient {{Multioutput Gaussian Processes}} through {{Variational Inducing Kernels}}.},
  booktitle = {{{AISTATS}}},
  author = {Alvarez, Mauricio A. and Luengo, David and Titsias, Michalis K. and Lawrence, Neil D.},
  date = {2010},
  volume = {9},
  pages = {25--32},
  url = {http://www.jmlr.org/proceedings/papers/v9/alvarez10a/alvarez10a.pdf},
  urldate = {2017-03-02},
  file = {/home/markus/Zotero/storage/6Q4I9FRF/Alvarez et al. - 2010 - Efficient Multioutput Gaussian Processes through V.pdf}
}

@article{alvarez_kernels_2011,
  title = {Kernels for {{Vector}}-{{Valued Functions}}: A {{Review}}},
  shorttitle = {Kernels for {{Vector}}-{{Valued Functions}}},
  author = {Alvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
  date = {2011-06-30},
  url = {http://arxiv.org/abs/1106.6251},
  urldate = {2017-02-06},
  abstract = {Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.},
  archivePrefix = {arXiv},
  eprint = {1106.6251},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/R6PZ939E/Alvarez et al. - 2011 - Kernels for Vector-Valued Functions a Review.pdf;/home/markus/Zotero/storage/IFE9Z28Q/1106.html},
  keywords = {Computer Science - Artificial Intelligence,Mathematics - Statistics Theory,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@inproceedings{alvarez_sparse_2009,
  title = {Sparse Convolved {{Gaussian}} Processes for Multi-Output Regression},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Alvarez, Mauricio and Lawrence, Neil D.},
  date = {2009},
  pages = {57--64},
  url = {http://papers.nips.cc/paper/3553-sparse-convolved-gaussian-processes-for-multi-output-regression},
  urldate = {2017-07-14},
  file = {/home/markus/Zotero/storage/SIZMYY5F/Alvarez and Lawrence - 2009 - Sparse convolved Gaussian processes for multi-outp.pdf;/home/markus/Zotero/storage/A2QU9XT7/3553-sparse-convolved-gaussian-processes-for-multi-output-regression.html}
}

@book{andrew_gelman_bayesian_2013,
  title = {Bayesian {{Data Analysis}} ({{Chapman}} \& {{Hall}}/{{CRC Texts}} in {{Statistical Science}})},
  author = {{Andrew Gelman}},
  date = {2013-11-27},
  publisher = {{Chapman and Hall/CRC}},
  file = {/home/markus/Zotero/storage/NUG7YY8D/212.pdf},
  keywords = {Textbook},
  langid = {english}
}

@article{andrieu_tutorial_2008,
  title = {A Tutorial on Adaptive {{MCMC}}},
  author = {Andrieu, Christophe and Thoms, Johannes},
  date = {2008-12-01},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {18},
  pages = {343--373},
  issn = {1573-1375},
  doi = {10.1007/s11222-008-9110-y},
  url = {https://doi.org/10.1007/s11222-008-9110-y},
  urldate = {2020-04-16},
  abstract = {We review adaptive Markov chain Monte Carlo algorithms (MCMC) as a mean to optimise their performance. Using simple toy examples we review their theoretical underpinnings, and in particular show why adaptive MCMC algorithms might fail when some fundamental properties are not satisfied. This leads to guidelines concerning the design of correct algorithms. We then review criteria and the useful framework of stochastic approximation, which allows one to systematically optimise generally used criteria, but also analyse the properties of adaptive MCMC algorithms. We then propose a series of novel adaptive algorithms which prove to be robust and reliable in practice. These algorithms are applied to artificial and high dimensional scenarios, but also to the classic mine disaster dataset inference problem.},
  file = {/home/markus/Zotero/storage/6CUISA4W/Andrieu and Thoms - 2008 - A tutorial on adaptive MCMC.pdf},
  langid = {english},
  number = {4}
}

@book{astrom_introduction_1971,
  title = {Introduction to {{Stochastic Control Theory}}},
  author = {Åström, Karl J.},
  date = {1971-02-27},
  publisher = {{Elsevier}},
  abstract = {In this book, we study theoretical and practical aspects of computing methods for mathematical modelling of nonlinear systems. A number of computing techniques are considered, such as methods of operator approximation with any given accuracy; operator interpolation techniques including a non-Lagrange interpolation; methods of system representation subject to constraints associated with concepts of causality, memory and stationarity; methods of system representation with an accuracy that is the best within a given class of models; methods of covariance matrix estimation;methods for low-rank matrix approximations; hybrid methods based on a combination of iterative procedures and best operator approximation; andmethods for information compression and filtering under condition that a filter model should satisfy restrictions associated with causality and different types of memory.As a result, the book represents a blend of new methods in general computational analysis,and specific, but also generic, techniques for study of systems theory ant its particularbranches, such as optimal filtering and information compression.- Best operator approximation,- Non-Lagrange interpolation,- Generic Karhunen-Loeve transform- Generalised low-rank matrix approximation- Optimal data compression- Optimal nonlinear filtering},
  isbn = {978-0-08-095579-7},
  keywords = {Mathematics / Algebra / Linear,Mathematics / Calculus,Mathematics / Mathematical Analysis,Mathematics / Numerical Analysis,Technology & Engineering / Mechanical},
  langid = {english},
  pagetotal = {318}
}

@article{barshalom_tracking_1990,
  title = {Tracking and {{Data Association}}},
  author = {Bar‐Shalom, Yaakov and Fortmann, Thomas E. and Cable, Peter G.},
  date = {1990-02-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {87},
  pages = {918--919},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.398863},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.398863},
  urldate = {2020-03-04},
  file = {/home/markus/Zotero/storage/GIAF2DIW/1.html},
  number = {2}
}

@article{barto_neuronlike_1983,
  title = {Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems},
  author = {Barto, A. G. and Sutton, R. S. and Anderson, C. W.},
  date = {1983-09},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {SMC-13},
  pages = {834--846},
  issn = {0018-9472},
  doi = {10.1109/TSMC.1983.6313077},
  abstract = {It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.},
  file = {/home/markus/Zotero/storage/899XKXEN/Barto et al. - 1983 - Neuronlike adaptive elements that can solve diffic.pdf;/home/markus/Zotero/storage/CL99BSYP/6313077.html;/home/markus/Zotero/storage/MKQ85H6W/6313077.html},
  keywords = {adaptive control,adaptive critic element,Adaptive systems,animal learning studies,associative search element,Biological neural networks,learning control problem,learning systems,movable cart,neural nets,neuronlike adaptive elements,Neurons,Pattern recognition,Problem-solving,Supervised learning,Training},
  number = {5}
}

@article{bernardo_regression_1998,
  title = {Regression and Classification Using {{Gaussian}} Process Priors},
  author = {Bernardo, J. and Berger, J. and Dawid, A. and Smith, A.},
  date = {1998},
  journaltitle = {Bayesian statistics},
  volume = {6},
  pages = {475},
  file = {/home/markus/Zotero/storage/RVPZ5DUT/Bernardo et al. - 1998 - Regression and classification using Gaussian proce.pdf;/home/markus/Zotero/storage/HGZSKINU/books.html}
}

@article{berner_dota_2019,
  title = {Dota 2 with Large Scale Deep Reinforcement Learning},
  author = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dębiak, Przemys\textbackslash law and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris},
  date = {2019},
  journaltitle = {arXiv preprint arXiv:1912.06680},
  file = {/home/markus/Zotero/storage/WHJ63GCZ/Berner et al. - 2019 - Dota 2 with large scale deep reinforcement learnin.pdf;/home/markus/Zotero/storage/GL8ILJJ5/1912.html}
}

@book{bertsekas_stochastic_1978,
  title = {Stochastic {{Optimal Control}}: {{The Discrete}}-{{Time Case}}},
  author = {Bertsekas, Dimitir P. and Shreve, Steven},
  date = {1978},
  publisher = {{Academic press}},
  abstract = {The book is a comprehensive and theoretically sound treatment of the mathematical foundations of stochastic optimal control of discrete-time systems, including the treatment of the intricate measure-theoretic issues.},
  file = {/home/markus/Zotero/storage/5JHEMU42/book.pdf},
  isbn = {1-886529-03-5}
}

@article{betancourt_optimizing_2015,
  title = {Optimizing {{The Integrator Step Size}} for {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, M. J. and Byrne, Simon and Girolami, Mark},
  date = {2015-02-02},
  url = {http://arxiv.org/abs/1411.6669},
  urldate = {2020-04-16},
  abstract = {Hamiltonian Monte Carlo can provide powerful inference in complex statistical problems, but ultimately its performance is sensitive to various tuning parameters. In this paper we use the underlying geometry of Hamiltonian Monte Carlo to construct a universal optimization criteria for tuning the step size of the symplectic integrator crucial to any implementation of the algorithm as well as diagnostics to monitor for any signs of invalidity. An immediate outcome of this result is that the suggested target average acceptance probability of 0.651 can be relaxed to \$0.6 \textbackslash lesssim a \textbackslash lesssim 0.9\$ with larger values more robust in practice.},
  archivePrefix = {arXiv},
  eprint = {1411.6669},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/HD76ILI2/Betancourt et al. - 2015 - Optimizing The Integrator Step Size for Hamiltonia.pdf;/home/markus/Zotero/storage/FJS3N74F/1411.html},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  primaryClass = {math, stat}
}

@report{bishop_mixture_1994,
  title = {Mixture Density Networks},
  author = {Bishop, Christopher M.},
  date = {1994},
  file = {/home/markus/Zotero/storage/HLZBIATM/Bishop - 1994 - Mixture density networks.pdf}
}

@book{bishop_pattern_2007,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher M.},
  date = {2007-04-28},
  file = {/home/markus/Zotero/storage/WEDREPNS/13.pdf},
  keywords = {Textbook}
}

@inproceedings{bitar_coordinated_2013,
  title = {Coordinated Control of a Wind Turbine Array for Power Maximization},
  booktitle = {2013 {{American Control Conference}}},
  author = {Bitar, Eilyan and Seiler, Pete},
  date = {2013-06},
  pages = {2898--2904},
  issn = {2378-5861},
  doi = {10.1109/ACC.2013.6580274},
  abstract = {Wind turbines are currently operated at their peak power extraction efficiency without consideration of the aerodynamic coupling between neighboring turbines. This mode of operation leads to inefficient, sub-optimal power capture at the wind farm level. By explicitly accounting for the aerodynamic wake interactions between neighboring wind turbines within a farm, we aim to characterize optimal control policies that maximize the power captured by a collection of wind turbines operating in quasi-steady wind flow conditions. In this paper, we consider two wake interaction models, termed near-field and far-field, describing wake propagation under densely and sparsely spaced turbine arrays, respectively. Under the near-field model, we derive a closed form expression for the optimal control policy maximizing power capture for a one-dimensional array of wind turbines. Moreover, we show that the optimal control policy is both static and independent of the free stream wind velocity, being thus amenable to a decentralized implementation. We also formulate and solve numerically the problem of jointly optimizing over the control policy and placement of turbines in a one dimensional 3-turbine array under the far-field model.},
  eventtitle = {2013 {{American Control Conference}}},
  file = {/home/markus/Zotero/storage/SEJY5JEY/6580274.html},
  keywords = {aerodynamic wake interactions,aerodynamics,Aerodynamics,Arrays,closed form expression,coordinated wind turbine array control,Couplings,decentralised control,decentralized implementation,densely spaced turbine array,dynamic programming,Equations,far-field wake interaction model,free stream wind velocity-independent optimal control policy,inefficient-suboptimal power capturing,Mathematical model,near-field wake interaction model,numerical analysis,one-dimensional 3-turbine array,one-dimensional wind turbine array,optimal control,Optimal Control,peak power extraction efficiency,power maximization,quasisteady wind flow conditions,sparsely spaced turbine array,static optimal control policy,turbine placement,wake propagation,wakes,Wind Energy,wind farm level,wind turbines,Wind turbines}
}

@article{bodin_latent_2017,
  title = {Latent {{Gaussian Process Regression}}},
  author = {Bodin, Erik and Campbell, Neill D. F. and Ek, Carl Henrik},
  date = {2017-07-18},
  url = {http://arxiv.org/abs/1707.05534},
  urldate = {2017-08-29},
  abstract = {We introduce Latent Gaussian Process Regression which is a latent variable extension allowing modelling of non-stationary processes using stationary GP priors. The approach is built on extending the input space of a regression problem with a latent variable that is used to modulate the covariance function over the input space. We show how our approach can be used to model non-stationary processes but also how multi-modal or non-functional processes can be described where the input signal cannot fully disambiguate the output. We exemplify the approach on a set of synthetic data and provide results on real data from geostatistics.},
  archivePrefix = {arXiv},
  eprint = {1707.05534},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/IWU4IK4P/Bodin et al. - 2017 - Latent Gaussian Process Regression.pdf;/home/markus/Zotero/storage/AZSUGMDS/1707.html},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{bodin_modulating_2020,
  title = {Modulating {{Surrogates}} for {{Bayesian Optimization}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}}) 119},
  author = {Bodin, Erik and Kaiser, Markus and Kazlauskaite, Ieva and Dai, Zhenwen and Campbell, Neill D. F. and Ek, Carl Henrik},
  date = {2020-02-24},
  url = {http://arxiv.org/abs/1906.11152},
  urldate = {2020-07-07},
  abstract = {Bayesian optimization (BO) methods often rely on the assumption that the objective function is well-behaved, but in practice, this is seldom true for real-world objectives even if noise-free observations can be collected. Common approaches, which try to model the objective as precisely as possible, often fail to make progress by spending too many evaluations modeling irrelevant details. We address this issue by proposing surrogate models that focus on the well-behaved structure in the objective function, which is informative for search, while ignoring detrimental structure that is challenging to model from few observations. First, we demonstrate that surrogate models with appropriate noise distributions can absorb challenging structures in the objective function by treating them as irreducible uncertainty. Secondly, we show that a latent Gaussian process is an excellent surrogate for this purpose, comparing with Gaussian processes with standard noise distributions. We perform numerous experiments on a range of BO benchmarks and find that our approach improves reliability and performance when faced with challenging objective functions.},
  archivePrefix = {arXiv},
  eprint = {1906.11152},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/B693TCSH/Bodin et al. - 2020 - Modulating Surrogates for Bayesian Optimization.pdf;/home/markus/Zotero/storage/44DQZUDW/1906.html},
  keywords = {Computer Science - Machine Learning,mrksr,Statistics - Machine Learning}
}

@inproceedings{boyle_dependent_2004,
  title = {Dependent {{Gaussian Processes}}.},
  booktitle = {{{NIPS}}},
  author = {Boyle, Phillip and Frean, Marcus R.},
  date = {2004},
  volume = {17},
  pages = {217--224},
  url = {https://papers.nips.cc/paper/2561-dependent-gaussian-processes.pdf},
  urldate = {2017-01-27},
  file = {/home/markus/Zotero/storage/HJT7BPIT/Boyle and Frean - 2004 - Dependent Gaussian Processes..pdf}
}

@report{boyle_multiple_2005,
  title = {Multiple Output Gaussian Process Regression},
  author = {Boyle, Phillip and Frean, Marcus and Boyle, Phillip and Frean, Marcus},
  date = {2005},
  abstract = {Gaussian processes are usually parameterised in terms of their covariance functions. However, this makes it difficult to deal with multiple outputs, because ensuring that the covariance matrix is positive definite is problematic. An alternative formulation is to treat Gaussian processes as white noise sources convolved with smoothing kernels, and to parameterise the kernel instead. Using this, we extend Gaussian processes to handle multiple, coupled outputs. 1},
  file = {/home/markus/Zotero/storage/STU7NV59/Boyle et al. - 2005 - Multiple output gaussian process regression.pdf;/home/markus/Zotero/storage/ZWMMCM3F/summary.html}
}

@book{bradley_efron_computer_2016,
  title = {Computer {{Age Statistical Inference}}: {{Algorithms}}, {{Evidence}}, and {{Data Science}} ({{Institute}} of {{Mathematical Statistics Monographs}})},
  author = {{Bradley Efron} and {Trevor Hastie}},
  date = {2016-07-20},
  publisher = {{Cambridge University Press}},
  file = {/home/markus/Zotero/storage/NPNW7NJS/175.pdf},
  isbn = {1-107-14989-4},
  keywords = {Textbook},
  langid = {english}
}

@article{breiman_statistical_2001,
  title = {Statistical Modeling: {{The}} Two Cultures (with Comments and a Rejoinder by the Author)},
  shorttitle = {Statistical Modeling},
  author = {Breiman, Leo},
  date = {2001},
  journaltitle = {Statistical science},
  volume = {16},
  pages = {199--231},
  file = {/home/markus/Zotero/storage/LIRPVSKE/Breiman - 2001 - Statistical modeling The two cultures (with comme.pdf;/home/markus/Zotero/storage/VFQ9KBNY/1009213726.html},
  number = {3}
}

@article{briol_probabilistic_2017,
  title = {Probabilistic {{Integration}}: {{A Role}} in {{Statistical Computation}}?},
  shorttitle = {Probabilistic {{Integration}}},
  author = {Briol, François-Xavier and Oates, Chris J. and Girolami, Mark and Osborne, Michael A. and Sejdinovic, Dino},
  date = {2017-10-18},
  url = {http://arxiv.org/abs/1512.00933},
  urldate = {2020-02-20},
  abstract = {A research frontier has emerged in scientific computation, wherein numerical error is regarded as a source of epistemic uncertainty that can be modelled. This raises several statistical challenges, including the design of statistical methods that enable the coherent propagation of probabilities through a (possibly deterministic) computational work-flow. This paper examines the case for probabilistic numerical methods in routine statistical computation. Our focus is on numerical integration, where a probabilistic integrator is equipped with a full distribution over its output that reflects the presence of an unknown numerical error. Our main technical contribution is to establish, for the first time, rates of posterior contraction for these methods. These show that probabilistic integrators can in principle enjoy the "best of both worlds", leveraging the sampling efficiency of Monte Carlo methods whilst providing a principled route to assess the impact of numerical error on scientific conclusions. Several substantial applications are provided for illustration and critical evaluation, including examples from statistical modelling, computer graphics and a computer model for an oil reservoir.},
  archivePrefix = {arXiv},
  eprint = {1512.00933},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/439DVI6X/Briol et al. - 2017 - Probabilistic Integration A Role in Statistical C.pdf;/home/markus/Zotero/storage/PIGM8TG3/1512.html},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{brockman_openai_2016,
  title = {{{OpenAI Gym}}},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  date = {2016-06-05},
  url = {http://arxiv.org/abs/1606.01540},
  urldate = {2018-09-24},
  abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
  archivePrefix = {arXiv},
  eprint = {1606.01540},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/Y7R3KYIX/Brockman et al. - 2016 - OpenAI Gym.pdf;/home/markus/Zotero/storage/IEKJED9E/1606.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{burt_rates_2019,
  title = {Rates of {{Convergence}} for {{Sparse Variational Gaussian Process Regression}}},
  author = {Burt, David R. and Rasmussen, Carl E. and van der Wilk, Mark},
  date = {2019-03-08},
  url = {http://arxiv.org/abs/1903.03571},
  urldate = {2019-07-21},
  abstract = {Excellent variational approximations to Gaussian process posteriors have been developed which avoid the \$\textbackslash mathcal\{O\}\textbackslash left(N\^3\textbackslash right)\$ scaling with dataset size \$N\$. They reduce the computational cost to \$\textbackslash mathcal\{O\}\textbackslash left(NM\^2\textbackslash right)\$, with \$M\textbackslash ll N\$ being the number of inducing variables, which summarise the process. While the computational cost seems to be linear in \$N\$, the true complexity of the algorithm depends on how \$M\$ must increase to ensure a certain quality of approximation. We address this by characterising the behavior of an upper bound on the KL divergence to the posterior. We show that with high probability the KL divergence can be made arbitrarily small by growing \$M\$ more slowly than \$N\$. A particular case of interest is that for regression with normally distributed inputs in D-dimensions with the popular Squared Exponential kernel, \$M=\textbackslash mathcal\{O\}(\textbackslash log\^D N)\$ is sufficient. Our results show that as datasets grow, Gaussian process posteriors can truly be approximated cheaply, and provide a concrete rule for how to increase \$M\$ in continual learning scenarios.},
  archivePrefix = {arXiv},
  eprint = {1903.03571},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/A34JXTCI/Burt et al. - 2019 - Rates of Convergence for Sparse Variational Gaussi.pdf;/home/markus/Zotero/storage/SJ6B5UEK/1903.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  options = {useprefix=true},
  primaryClass = {cs, stat}
}

@inproceedings{calandra_manifold_2016,
  title = {Manifold {{Gaussian Processes}} for Regression},
  booktitle = {2016 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Calandra, Roberto and Peters, Jan and Rasmussen, Carl Edward and Deisenroth, Marc Peter},
  date = {2016-07},
  pages = {3338--3345},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2016.7727626},
  abstract = {Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness assumptions on the structure of the function to be modeled. To model complex and non-differentiable functions, these smoothness assumptions are often too restrictive. One way to alleviate this limitation is to find a different representation of the data by introducing a feature space. This feature space is often learned in an unsupervised way, which might lead to data representations that are not useful for the overall regression task. In this paper, we propose Manifold Gaussian Processes, a novel supervised method that jointly learns a transformation of the data into a feature space and a GP regression from the feature space to observed space. The Manifold GP is a full GP and allows to learn data representations, which are useful for the overall regression task. As a proof-of-concept, we evaluate our approach on complex non-smooth functions where standard GPs perform poorly, such as step functions and robotics tasks with contacts.},
  eventtitle = {2016 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  file = {/home/markus/Zotero/storage/98AZPAP8/Calandra et al. - 2016 - Manifold Gaussian Processes for regression.pdf;/home/markus/Zotero/storage/W8VG2G7H/7727626.html},
  keywords = {Bayes methods,complex nonsmooth functions,Computational modeling,data handling,data representation,Gaussian processes,GP regression,manifold Gaussian processes,Manifolds,regression analysis,Standards,Supervised learning,supervised method,Training}
}

@book{casella_statistical_2002,
  title = {Statistical Inference},
  author = {Casella, George and Berger, Roger L.},
  date = {2002},
  volume = {2},
  publisher = {{Duxbury Pacific Grove, CA}},
  file = {/home/markus/Zotero/storage/GMUW236X/Casella and Berger - 2002 - Statistical inference.pdf}
}

@incollection{cheng_variational_2017,
  title = {Variational {{Inference}} for {{Gaussian Process Models}} with {{Linear Complexity}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Cheng, Ching-An and Boots, Byron},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  pages = {5184--5194},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/7103-variational-inference-for-gaussian-process-models-with-linear-complexity.pdf},
  urldate = {2018-02-20},
  file = {/home/markus/Zotero/storage/HHLXL5ED/Cheng und Boots - 2017 - Variational Inference for Gaussian Process Models .pdf;/home/markus/Zotero/storage/58YU8A3Y/7103-variational-inference-for-gaussian-process-models-with-linear-complexity.html}
}

@incollection{cho_kernel_2009,
  title = {Kernel {{Methods}} for {{Deep Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 22},
  author = {Cho, Youngmin and Saul, Lawrence K.},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
  date = {2009},
  pages = {342--350},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf},
  urldate = {2017-03-28},
  file = {/home/markus/Zotero/storage/87SEPH23/Cho and Saul - 2009 - Kernel Methods for Deep Learning.pdf;/home/markus/Zotero/storage/NVSB5KMR/3628-kernel-methods-for-deep-learning.html}
}

@article{choi_choicenet_2018,
  title = {{{ChoiceNet}}: {{Robust Learning}} by {{Revealing Output Correlations}}},
  shorttitle = {{{ChoiceNet}}},
  author = {Choi, Sungjoon and Hong, Sanghoon and Lim, Sungbin},
  date = {2018-05-16},
  url = {http://arxiv.org/abs/1805.06431},
  urldate = {2018-08-23},
  abstract = {In this paper, we focus on the supervised learning problem with corrupted training data. We assume that the training dataset is generated from a mixture of a target distribution and other unknown distributions. We estimate the quality of each data by revealing the correlation between the generated distribution and the target distribution. To this end, we present a novel framework referred to here as ChoiceNet that can robustly infer the target distribution in the presence of inconsistent data. We demonstrate that the proposed framework is applicable to both classification and regression tasks. ChoiceNet is evaluated in comprehensive experiments, where we show that it constantly outperforms existing baseline methods in the handling of noisy data. Particularly, ChoiceNet is successfully applied to autonomous driving tasks where it learns a safe driving policy from a dataset with mixed qualities. In the classification task, we apply the proposed method to the MNIST and CIFAR-10 datasets and it shows superior performances in terms of robustness to noisy labels.},
  archivePrefix = {arXiv},
  eprint = {1805.06431},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/F886D4JS/Choi et al. - 2018 - ChoiceNet Robust Learning by Revealing Output Cor.pdf;/home/markus/Zotero/storage/M6QSGCFL/1805.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{choi_robust_2016,
  title = {Robust Learning from Demonstration Using Leveraged {{Gaussian}} Processes and Sparse-Constrained Optimization},
  booktitle = {Robotics and {{Automation}} ({{ICRA}}), 2016 {{IEEE International Conference}} On},
  author = {Choi, Sungjoon and Lee, Kyungjae and Oh, Songhwai},
  date = {2016},
  pages = {470--475},
  publisher = {{IEEE}},
  file = {/home/markus/Zotero/storage/LNLKLPLD/Choi et al. - 2016 - Robust learning from demonstration using leveraged.pdf;/home/markus/Zotero/storage/AGU553L3/7487168.html}
}

@article{chorowski_attention-based_2015,
  title = {Attention-{{Based Models}} for {{Speech Recognition}}},
  author = {Chorowski, Jan and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2015-06-24},
  url = {http://arxiv.org/abs/1506.07503},
  urldate = {2020-08-17},
  abstract = {Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7\% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18\% PER in single utterances and 20\% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6\% level.},
  archivePrefix = {arXiv},
  eprint = {1506.07503},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/EIEK83GR/Chorowski et al. - 2015 - Attention-Based Models for Speech Recognition.pdf;/home/markus/Zotero/storage/YJV4U8KZ/1506.html},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{coburn_geostatistics_2000,
  title = {Geostatistics for Natural Resources Evaluation},
  author = {Coburn, Timothy C.},
  date = {2000},
  publisher = {{Taylor \& Francis Group}},
  file = {/home/markus/Zotero/storage/QKNJSFHQ/auD.html}
}

@article{cockayne_bayesian_2019,
  title = {Bayesian {{Probabilistic Numerical Methods}}},
  author = {Cockayne, Jon and Oates, Chris and Sullivan, Tim and Girolami, Mark},
  date = {2019-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {61},
  pages = {756--789},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/17M1139357},
  url = {http://arxiv.org/abs/1702.03673},
  urldate = {2020-02-14},
  abstract = {The emergent field of probabilistic numerics has thus far lacked clear statistical principals. This paper establishes Bayesian probabilistic numerical methods as those which can be cast as solutions to certain inverse problems within the Bayesian framework. This allows us to establish general conditions under which Bayesian probabilistic numerical methods are well-defined, encompassing both non-linear and non-Gaussian models. For general computation, a numerical approximation scheme is proposed and its asymptotic convergence established. The theoretical development is then extended to pipelines of computation, wherein probabilistic numerical methods are composed to solve more challenging numerical tasks. The contribution highlights an important research frontier at the interface of numerical analysis and uncertainty quantification, with a challenging industrial application presented.},
  archivePrefix = {arXiv},
  eprint = {1702.03673},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/IYE9Y4P8/Cockayne et al. - 2019 - Bayesian Probabilistic Numerical Methods.pdf;/home/markus/Zotero/storage/X8T74HX4/1702.html},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Methodology},
  number = {3}
}

@article{cockayne_probabilistic_2017,
  title = {Probabilistic {{Numerical Methods}} for {{Partial Differential Equations}} and {{Bayesian Inverse Problems}}},
  author = {Cockayne, Jon and Oates, Chris and Sullivan, Tim and Girolami, Mark},
  date = {2017-07-11},
  url = {http://arxiv.org/abs/1605.07811},
  urldate = {2020-02-18},
  abstract = {This paper develops a probabilistic numerical method for solution of partial differential equations (PDEs) and studies application of that method to PDE-constrained inverse problems. This approach enables the solution of challenging inverse problems whilst accounting, in a statistically principled way, for the impact of discretisation error due to numerical solution of the PDE. In particular, the approach confers robustness to failure of the numerical PDE solver, with statistical inferences driven to be more conservative in the presence of substantial discretisation error. Going further, the problem of choosing a PDE solver is cast as a problem in the Bayesian design of experiments, where the aim is to minimise the impact of solver error on statistical inferences; here the challenge of non-linear PDEs is also considered. The method is applied to parameter inference problems in which discretisation error in non-negligible and must be accounted for in order to reach conclusions that are statistically valid.},
  archivePrefix = {arXiv},
  eprint = {1605.07811},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/PDK3PR2J/Cockayne et al. - 2017 - Probabilistic Numerical Methods for Partial Differ.pdf;/home/markus/Zotero/storage/MIWRP8CG/1605.html},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Methodology},
  primaryClass = {cs, math, stat},
  version = {3}
}

@book{cormen_introduction_2009,
  title = {Introduction to {{Algorithms}}},
  author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
  date = {2009},
  author_sort = {Cormen, Thomas H. & Leiserson, Charles E. & Rivest, Ronald L. & Stein, Clifford},
  file = {/home/markus/Zotero/storage/2SKMD8UG/Cormen et al. - Introduction to Algorithms.pdf},
  title_sort = {Introduction to Algorithms}
}

@book{cox_planning_1958,
  title = {Planning of Experiments},
  author = {Cox, David Roxbee},
  date = {1958},
  publisher = {{John Wiley \& Sons Inc}},
  file = {/home/markus/Zotero/storage/E689NTFC/Cox - 1958 - Planning of experiments.pdf;/home/markus/Zotero/storage/WHMV5DL6/1959-07233-000.html}
}

@article{cox_review_1993,
  title = {A Review of Statistical Data Association Techniques for Motion Correspondence},
  author = {Cox, Ingemar J.},
  date = {1993-02-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vision},
  volume = {10},
  pages = {53--66},
  issn = {1573-1405},
  doi = {10.1007/BF01440847},
  url = {https://doi.org/10.1007/BF01440847},
  urldate = {2020-03-04},
  abstract = {Motion correspondence is a fundamental problem in computer vision and many other disciplines. This article describes statistical data association techniques originally developed in the context of target tracking and surveillance and now beginning to be used in dynamic motion analysis by the computer vision community. The Mahalanobis distance measure is first introduced before discussing the limitations of nearest neighbor algorithms. Then, the track-splitting, joint likelihood, multiple hypothesis algorithms are described, each method solving an increasingly more complicated optimization. Real-time constraints may prohibit the application of these optimal methods. The suboptimal joint probabilistic data association algorithm is therefore described. The advantages, limitations, and relationships between the approaches are discussed.},
  langid = {english},
  number = {1}
}

@inproceedings{damianou_deep_2013,
  title = {Deep {{Gaussian Processes}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Damianou, Andreas and Lawrence, Neil},
  date = {2013-04-29},
  pages = {207--215},
  url = {http://proceedings.mlr.press/v31/damianou13a.html},
  urldate = {2018-10-02},
  abstract = {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inpu...},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  file = {/home/markus/Zotero/storage/BUXWE2UV/Damianou and Lawrence - 2012 - Deep Gaussian Processes.pdf;/home/markus/Zotero/storage/WKSUAVEY/Damianou und Lawrence - 2013 - Deep Gaussian Processes.pdf;/home/markus/Zotero/storage/K6M9IPNY/damianou13a.html;/home/markus/Zotero/storage/S2KB72DK/1211.html},
  keywords = {60G15; 58E30,Computer Science - Learning,G.1.2,G.3,I.2.6,Mathematics - Probability,Statistics - Machine Learning},
  langid = {english}
}

@thesis{damianou_deep_2015,
  title = {Deep {{Gaussian}} Processes and Variational Propagation of Uncertainty},
  author = {Damianou, Andreas},
  date = {2015},
  institution = {{University of Sheffield}},
  url = {http://etheses.whiterose.ac.uk/id/eprint/9968},
  urldate = {2016-02-01},
  file = {/home/markus/Zotero/storage/JEWANA87/Damianou - 2015 - Deep Gaussian processes and variational propagatio.pdf;/home/markus/Zotero/storage/DKUPAECG/9968.html}
}

@article{damianou_variational_2014,
  title = {Variational {{Inference}} for {{Uncertainty}} on the {{Inputs}} of {{Gaussian Process Models}}},
  author = {Damianou, Andreas C. and Titsias, Michalis K. and Lawrence, Neil D.},
  date = {2014-09-08},
  url = {http://arxiv.org/abs/1409.2287},
  urldate = {2016-09-05},
  abstract = {The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximized over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximizing an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from iid observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the nonlinear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain inputs and semi-supervised Gaussian processes. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data.},
  archivePrefix = {arXiv},
  eprint = {1409.2287},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/37WUG5GD/Damianou et al. - 2014 - Variational Inference for Uncertainty on the Input.pdf;/home/markus/Zotero/storage/UM49BUPD/1409.html},
  keywords = {60G15 (Primary); 58E30,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,G.1.2,G.3,I.2.6,I.5.4,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{daniely_toward_2016,
  title = {Toward {{Deeper Understanding}} of {{Neural Networks}}: {{The Power}} of {{Initialization}} and a {{Dual View}} on {{Expressivity}}},
  shorttitle = {Toward {{Deeper Understanding}} of {{Neural Networks}}},
  author = {Daniely, Amit and Frostig, Roy and Singer, Yoram},
  date = {2016-02-18},
  url = {http://arxiv.org/abs/1602.05897},
  urldate = {2018-03-21},
  abstract = {We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power.},
  archivePrefix = {arXiv},
  eprint = {1602.05897},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/UFNAVBVT/Daniely et al. - 2016 - Toward Deeper Understanding of Neural Networks Th.pdf;/home/markus/Zotero/storage/C2RF4YMN/1602.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{david_barber_bayesian_2012,
  title = {Bayesian {{Reasoning}} and {{Machine Learning}}},
  author = {{David Barber}},
  date = {2012-02-01},
  publisher = {{Cambridge University Press}},
  file = {/home/markus/Zotero/storage/HBZFHA5D/153.pdf},
  isbn = {978-0-521-51814-7},
  keywords = {Computer Vision & Pattern Recognition,Computers,General,Intelligence (AI) & Semantics,keyword1 key2 key3,Mathematics,Probability & Statistics,Subject,Textbook},
  langid = {english}
}

@article{de_freitas_exponential_2012,
  title = {Exponential {{Regret Bounds}} for {{Gaussian Process Bandits}} with {{Deterministic Observations}}},
  author = {de Freitas, Nando and Smola, Alex and Zoghi, Masrour},
  date = {2012-06-27},
  url = {http://arxiv.org/abs/1206.6457},
  urldate = {2020-04-16},
  abstract = {This paper analyzes the problem of Gaussian process (GP) bandits with deterministic observations. The analysis uses a branch and bound algorithm that is related to the UCB algorithm of (Srinivas et al, 2010). For GPs with Gaussian observation noise, with variance strictly greater than zero, Srinivas et al proved that the regret vanishes at the approximate rate of \$O(1/\textbackslash sqrt\{t\})\$, where t is the number of observations. To complement their result, we attack the deterministic case and attain a much faster exponential convergence rate. Under some regularity assumptions, we show that the regret decreases asymptotically according to \$O(e\^\{-\textbackslash frac\{\textbackslash tau t\}\{(\textbackslash ln t)\^\{d/4\}\}\})\$ with high probability. Here, d is the dimension of the search space and tau is a constant that depends on the behaviour of the objective function near its global maximum.},
  archivePrefix = {arXiv},
  eprint = {1206.6457},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/EY9NLBIH/de Freitas et al. - 2012 - Exponential Regret Bounds for Gaussian Process Ban.pdf;/home/markus/Zotero/storage/ZRS7EMFV/1206.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  options = {useprefix=true},
  primaryClass = {cs, stat}
}

@thesis{deisenroth_efficient_2010,
  title = {Efficient {{Reinforcement Learning}} Using {{Gaussian Processes}}},
  author = {Deisenroth, Marc Peter},
  date = {2010},
  institution = {{KIT Scientific Publishing}},
  url = {http://www.cs.washington.edu/research/projects/aiweb/media/papers/tmppqidj5},
  urldate = {2016-04-19},
  file = {/home/markus/Zotero/storage/K5TE3E38/deisenroth.pdf;/home/markus/Zotero/storage/63UCNMSK/books.html}
}

@inproceedings{deisenroth_pilco_2011,
  title = {{{PILCO}}: {{A}} Model-Based and Data-Efficient Approach to Policy Search},
  shorttitle = {{{PILCO}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on Machine Learning ({{ICML}}-11)},
  author = {Deisenroth, Marc and Rasmussen, Carl E.},
  date = {2011},
  pages = {465--472},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Deisenroth_323.pdf},
  urldate = {2016-02-01},
  file = {/home/markus/Zotero/storage/YEDBHXGB/Deisenroth and Rasmussen - 2011 - PILCO A model-based and data-efficient approach t.pdf}
}

@patent{depeweg_computer_2020,
  title = {Computerimplementiertes {{Verfahren Zum Abschätzen Eines Technischen Verhaltens Einer Vorrichtung}}},
  author = {Depeweg, Stefan and Geipel, Markus Michael and Kaiser, Markus and Udluft, Steffen},
  date = {2020-03-18},
  abstract = {Die Erfindung betrifft ein computerimplementiertes Verfahren zum Abschätzen eines technischen Verhaltens einer Vorrichtung, insbesondere einer Vorrichtung zum Erzeugen von Energie, wobei ein Quellmodell einer Quellvorrichtung verwendet wird, wobei das Quellmodell wenigstens zwei Funktionen aufweist, wobei die zwei Funktionen ein technisches Verhalten der Quellvorrichtung mit einer statistischen Ungenauigkeit beschreiben, wobei die Funktionen des Quellmodells mit einem gemessenen technischen Verhalten der Vorrichtung verglichen werden, wobei für die Funktionen aufgrund des Vergleiches Bewertungsfaktoren ermittelt werden, wobei die Funktionen mit den Bewertungsfaktoren bewertet werden, und wobei mithilfe der bewerteten Funktionen des Quellmodells wenigstens ein Wert des technischen Verhaltens der Vorrichtung abgeschätzt wird.},
  holder = {{Siemens Ag}},
  keywords = {patent},
  number = {3623881A1},
  type = {patenteu}
}

@inproceedings{depeweg_decomposition_2018,
  title = {Decomposition of {{Uncertainty}} in {{Bayesian Deep Learning}} for {{Efficient}} and {{Risk}}-Sensitive {{Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Depeweg, Stefan and Hernandez-Lobato, Jose-Miguel and Doshi-Velez, Finale and Udluft, Steffen},
  date = {2018},
  pages = {1192--1201},
  file = {/home/markus/Zotero/storage/IQV8Z9UK/Depeweg et al. - Decomposition of Uncertainty in Bayesian Deep Lear.pdf;/home/markus/Zotero/storage/TK4ZWLEU/Depeweg et al. - Decomposition of Uncertainty in Bayesian Deep Lear.pdf;/home/markus/Zotero/storage/FR49N2TD/auD.html}
}

@article{depeweg_learning_2016,
  title = {Learning and {{Policy Search}} in {{Stochastic Dynamical Systems}} with {{Bayesian Neural Networks}}},
  author = {Depeweg, Stefan and Hernández-Lobato, José Miguel and Doshi-Velez, Finale and Udluft, Steffen},
  date = {2016-05-23},
  url = {http://arxiv.org/abs/1605.07127},
  urldate = {2019-02-19},
  abstract = {We present an algorithm for policy search in stochastic dynamical systems using model-based reinforcement learning. The system dynamics are described with Bayesian neural networks (BNNs) that include stochastic input variables. These input variables allow us to capture complex statistical patterns in the transition dynamics (e.g. multi-modality and heteroskedasticity), which are usually missed by alternative modeling approaches. After learning the dynamics, our BNNs are then fed into an algorithm that performs random roll-outs and uses stochastic optimization for policy learning. We train our BNNs by minimizing α-divergences with α = 0.5, which usually produces better results than other techniques such as variational Bayes. We illustrate the performance of our method by solving a challenging problem where model-based approaches usually fail and by obtaining promising results in real-world scenarios including the control of a gas turbine and an industrial benchmark.},
  archivePrefix = {arXiv},
  eprint = {1605.07127},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/PDDB5NVV/Depeweg et al. - 2016 - Learning and Policy Search in Stochastic Dynamical.pdf;/home/markus/Zotero/storage/X5C6XVZT/Depeweg et al. - 2016 - Learning and Policy Search in Stochastic Dynamical.pdf;/home/markus/Zotero/storage/MWFZ45PK/1605.html},
  keywords = {Computer Science - Learning,Computer Science - Machine Learning,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@article{duane_hybrid_1987,
  title = {Hybrid Monte Carlo},
  author = {Duane, Simon and Kennedy, Anthony D. and Pendleton, Brian J. and Roweth, Duncan},
  date = {1987},
  journaltitle = {Physics letters B},
  volume = {195},
  pages = {216--222},
  publisher = {{Elsevier}},
  file = {/home/markus/Zotero/storage/5TI3ZYJ4/037026938791197X.html},
  number = {2}
}

@article{duvenaud_additive_2011,
  title = {Additive {{Gaussian Processes}}},
  author = {Duvenaud, David and Nickisch, Hannes and Rasmussen, Carl Edward},
  date = {2011-12-19},
  url = {http://arxiv.org/abs/1112.4394},
  urldate = {2016-09-06},
  abstract = {We introduce a Gaussian process model of functions which are additive. An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels. Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but tractable parameterization of the kernel function, which allows efficient evaluation of all input interaction terms, whose number is exponential in the input dimension. The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks.},
  archivePrefix = {arXiv},
  eprint = {1112.4394},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/AN4KGPFF/uw_additive_gp_slides.pdf;/home/markus/Zotero/storage/UATB9DCB/Duvenaud et al. - 2011 - Additive Gaussian Processes.pdf;/home/markus/Zotero/storage/UKGJTHI6/1112.html},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{duvenaud_avoiding_2014,
  title = {Avoiding Pathologies in Very Deep Networks},
  author = {Duvenaud, David and Rippel, Oren and Adams, Ryan P. and Ghahramani, Zoubin},
  date = {2014-02-24},
  url = {http://arxiv.org/abs/1402.5836},
  urldate = {2016-09-21},
  abstract = {Choosing appropriate architectures and regularization strategies for deep networks is crucial to good predictive performance. To shed light on this problem, we analyze the analogous problem of constructing useful priors on compositions of functions. Specifically, we study the deep Gaussian process, a type of infinitely-wide, deep neural network. We show that in standard architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, retaining only a single degree of freedom in the limit. We propose an alternate network architecture which does not suffer from this pathology. We also examine deep covariance functions, obtained by composing infinitely many feature transforms. Lastly, we characterize the class of models obtained by performing dropout on Gaussian processes.},
  archivePrefix = {arXiv},
  eprint = {1402.5836},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/C8B8H3ZW/Duvenaud et al. - 2014 - Avoiding pathologies in very deep networks.pdf;/home/markus/Zotero/storage/ZRSN74T4/1402.html},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{duvenaud_structure_2013,
  title = {Structure {{Discovery}} in {{Nonparametric Regression}} through {{Compositional Kernel Search}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author = {Duvenaud, David and Lloyd, James Robert and Grosse, Roger and Tenenbaum, Joshua B and Ghahramani, Zoubin},
  date = {2013},
  pages = {9},
  location = {{Atlanta, Georgia, USA}},
  abstract = {Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.},
  eventtitle = {{{ICML}}},
  file = {/home/markus/Zotero/storage/MPWHNS3L/Duvenaud et al. - Structure Discovery in Nonparametric Regression th.pdf},
  langid = {english}
}

@article{efron_modern_2005,
  title = {Modern Science and the {{Bayesian}}-Frequentist Controversy},
  author = {Efron, Bradley},
  date = {2005},
  file = {/home/markus/Zotero/storage/DCBD2FG9/Efron - 2005 - Modern science and the Bayesian-frequentist contro.pdf}
}

@patent{egedal_verfahren_2019,
  title = {Verfahren {{Und Vorrichtung Zur Kooperativen Steuerung Von Windturbinen Eines Windparks}}},
  author = {Egedal, Per and Enevoldsen, Peder Bay and Hentschel, Alexander and Kaiser, Markus and Otte, Clemens and Sterzing, Volkmar and Udluft, Steffen and Weber, Marc Christian},
  date = {2019-07-31},
  abstract = {The invention relates to a method for cooperative controlling wind turbines (10, 20) of a wind farm, wherein the wind farm comprises at least one pair of turbines (10, 20) aligned along a common axis approximately parallel to a current wind direction and consisting of an upstream turbine (10) and a downstream turbine (20). The method comprises the steps of:a) providing a data driven model trained with a machine learning method and stored in a database (51), the data driven model providing a correlation between time series data obtained from the pair of turbines (10, 20) in parallel, the time series data being aligned in time to the same wind front and a ratio of the current power production of the upstream and the downstream turbine (20) related to the aligned time series data;b) determining a decision parameter for controlling at least one of the upstream turbine (10) and the downstream turbine (20) by feeding the data driven model with- the current power production of the upstream turbine (10) which returns a prediction value indicating whether the downstream turbine (20) will be affected by wake, and/or- the temporal evolvement of the current power production of the upstream turbine (10) which returns a prediction of the probable development of the future power production of the downstream turbine (20);c) based on the decision parameter, determining control parameters for the upstream turbine (10) and/or the downstream turbine (20).},
  holder = {{Siemens Gamesa Renewable Energy As}},
  keywords = {patent},
  number = {3517774A1},
  type = {patenteu}
}

@article{friedman_data_1998,
  title = {Data {{Mining}} and {{Statistics}}: {{What}}'s the Connection?},
  shorttitle = {Data {{Mining}} and {{Statistics}}},
  author = {Friedman, Jerome H.},
  date = {1998},
  journaltitle = {Computing science and statistics},
  volume = {29},
  pages = {3--9},
  publisher = {{PROCEEDINGS PUBLISHED BY VARIOUS PUBLISHERS}},
  file = {/home/markus/Zotero/storage/C5I5RWKK/Friedman - 1998 - Data Mining and Statistics What's the connection.pdf},
  number = {1}
}

@article{fubini_sugli_1907,
  title = {Sugli integrali multipli},
  author = {Fubini, G.},
  date = {1907},
  journaltitle = {Accademia dei Lincei, Rendiconti, V. Serie},
  shortjournal = {Rom. Acc. L. Rend. (5)},
  volume = {16},
  pages = {608--614},
  publisher = {{Reale Accademia dei Lincei, Rome}},
  issn = {0001-4435},
  url = {https://zbmath.org/?q=an%3A38.0343.02},
  urldate = {2020-07-10},
  file = {/home/markus/Zotero/storage/PEY3J8X9/zbmath.org.html},
  langid = {italian},
  number = {1},
  zmnumber = {JFM 38.0343.02}
}

@inproceedings{gardner_discovering_2017,
  title = {Discovering and {{Exploiting Additive Structure}} for {{Bayesian Optimization}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Gardner, Jacob and Guo, Chuan and Weinberger, Kilian and Garnett, Roman and Grosse, Roger},
  date = {2017-04-10},
  pages = {1311--1319},
  issn = {1938-7228},
  url = {http://proceedings.mlr.press/v54/gardner17a.html},
  urldate = {2020-04-16},
  abstract = {Bayesian optimization has proven invaluable for black-box optimization of expensive functions. Its main limitation is its exponential complexity with respect to the dimensionality of the search spa...},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  file = {/home/markus/Zotero/storage/6G2KPTRG/Gardner et al. - 2017 - Discovering and Exploiting Additive Structure for .pdf;/home/markus/Zotero/storage/YMF5CKU3/gardner17a.html},
  langid = {english}
}

@book{gareth_james_introduction_2013,
  title = {An {{Introduction}} to {{Statistical Learning}}: {{With Applications}} in {{R}} ({{Springer Texts}} in {{Statistics}})},
  author = {{Gareth James} and {Daniela Witten} and {Trevor Hastie} and {Robert Tibshirani}},
  date = {2013-08-11},
  publisher = {{Springer}},
  file = {/home/markus/Zotero/storage/HZZMH9RE/155.pdf},
  isbn = {978-1-4614-7137-0},
  keywords = {Textbook},
  langid = {english}
}

@book{gauss_theoria_1809,
  title = {Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientium},
  author = {Gauss, Carl Friedrich},
  date = {1809},
  publisher = {{sumtibus Frid. Perthes et IH Besser}},
  url = {https://books.google.de/books?id=VKhu8yPcat8C},
  urldate = {2016-06-06},
  file = {/home/markus/Zotero/storage/NEPIE6G8/books.html}
}

@patent{geipel_transferlernen_2020,
  title = {Transferlernen {{Von Modellen Des Maschinellen Lernens Unter Verwendung Einer Wissensgraphdatenbank}}},
  author = {Geipel, Markus Michael and Hubauer, Thomas and Kaiser, Markus and Von, Beuningen Anja},
  date = {2020-03-11},
  abstract = {The invention relates to a method of building a machine-learning model (301-304) for operational monitoring of an ensemble (200) of field devices (201-204). The method comprises organizing the various field devices (201-204) of the ensemble (200) in a knowledge graph database (501-503), and, for a given field device (201-204) of the ensemble (200) of field devices (201-204), accessing the knowledge graph database (501-503) to identify one or more related field devices (201-204). The method further comprises performing a training of the machine-learning model (301-304) of the given field device (201-204) based on information (603) associated with the one or more related field devices (201-204).},
  holder = {{Siemens Ag}},
  keywords = {patent},
  number = {3620997A1},
  type = {patenteu}
}

@thesis{girard_approximate_2004,
  title = {Approximate Methods for Propagation of Uncertainty with {{Gaussian}} Process Models},
  author = {Girard, Agathe},
  date = {2004},
  institution = {{Citeseer}},
  file = {/home/markus/Zotero/storage/9WRXUG9Y/Girard - 2004 - Approximate methods for propagation of uncertainty.pdf},
  type = {PhD Thesis}
}

@incollection{girard_gaussian_2003,
  title = {Gaussian {{Process Priors}} with {{Uncertain Inputs Application}} to {{Multiple}}-{{Step Ahead Time Series Forecasting}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 15},
  author = {Girard, Agathe and Rasmussen, Carl Edward and Candela, Joaquin Quiñonero and Murray-Smith, Roderick},
  editor = {Becker, S. and Thrun, S. and Obermayer, K.},
  date = {2003},
  pages = {545--552},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/2313-gaussian-process-priors-with-uncertain-inputs-application-to-multiple-step-ahead-time-series-forecasting.pdf},
  urldate = {2020-04-16},
  file = {/home/markus/Zotero/storage/RB426LIY/Girard et al. - 2003 - Gaussian Process Priors with Uncertain Inputs Appl.pdf;/home/markus/Zotero/storage/ND38MTP4/2313-gaussian-process-priors-with-uncertain-inputs-application-to-multiple-step-ahead-time-seri.html}
}

@book{giulio_d._agostini_bayesian_2003,
  title = {Bayesian {{Reasoning}} in {{Data Analysis}}: {{A Critical Introduction}}},
  author = {{Giulio D. Agostini}},
  date = {2003-08-01},
  publisher = {{World Scientific Pub Co Inc}},
  file = {/home/markus/Zotero/storage/5CU4Q2ZA/174.pdf},
  isbn = {978-981-238-356-3},
  keywords = {Bayesian Analysis,General,Mathematics,Measurement,Physics,Probability & Statistics,Science,Textbook},
  langid = {english}
}

@incollection{goldberg_regression_1998,
  title = {Regression with {{Input}}-Dependent {{Noise}}: {{A Gaussian Process Treatment}}},
  shorttitle = {Regression with {{Input}}-Dependent {{Noise}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 10},
  author = {Goldberg, Paul W. and Williams, Christopher K. I. and Bishop, Christopher M.},
  editor = {Jordan, M. I. and Kearns, M. J. and Solla, S. A.},
  date = {1998},
  pages = {493--499},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/1444-regression-with-input-dependent-noise-a-gaussian-process-treatment.pdf},
  urldate = {2020-04-16},
  file = {/home/markus/Zotero/storage/VKXXSLLV/Goldberg et al. - 1998 - Regression with Input-dependent Noise A Gaussian .pdf;/home/markus/Zotero/storage/P5ZQYIQC/1444-regression-with-input-dependent-noise-a-gaussian-process-treatment.html}
}

@inproceedings{gonzalez_preferential_2017,
  title = {Preferential {{Bayesian}} Optimization},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}} - {{Volume}} 70},
  author = {González, Javier and Dai, Zhenwen and Damianou, Andreas and Lawrence, Neil D.},
  date = {2017-08-06},
  pages = {1282--1291},
  publisher = {{JMLR.org}},
  location = {{Sydney, NSW, Australia}},
  abstract = {Bayesian optimization (BO) has emerged during the last few years as an effective approach to optimizing black-box functions where direct queries of the objective are expensive. In this paper we consider the case where direct access to the function is not possible, but information about user preferences is. Such scenarios arise in problems where human preferences are modeled, such as A/B tests or recommender systems. We present a new framework for this scenario that we call Preferential Bayesian Optimization (PBO) which allows us to find the optimum of a latent function that can only be queried through pairwise comparisons, the so-called duels. PBO extends the applicability of standard BO ideas and generalizes previous discrete dueling approaches by modeling the probability of the winner of each duel by means of a Gaussian process model with a Bernoulli likelihood. The latent preference function is used to define a family of acquisition functions that extend usual policies used in BO. We illustrate the benefits of PBO in a variety of experiments, showing that PBO needs drastically fewer comparisons for finding the optimum. According to our experiments, the way of modeling correlations in PBO is key in obtaining this advantage.},
  file = {/home/markus/Zotero/storage/6PCKI4SY/González et al. - 2017 - Preferential Bayesian optimization.pdf},
  series = {{{ICML}}'17}
}

@book{goodfellow_deep_2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {{MIT Press}},
  url = {https://www.deeplearningbook.org/},
  urldate = {2020-08-14},
  file = {/home/markus/Zotero/storage/V9QILZ2Q/Goodfellow et al. - 2016 - Deep Learning.pdf;/home/markus/Zotero/storage/EIXUGP4J/www.deeplearningbook.org.html}
}

@article{gpyopt_gpyopt_2016,
  title = {{{GPyOpt}}: {{A}} Bayesian Optimization Framework in Python},
  author = {{GPyOpt}},
  date = {2016},
  url = {http://github.com/SheffieldML/GPyOpt}
}

@article{gramacy_cases_2010,
  title = {Cases for the Nugget in Modeling Computer Experiments},
  author = {Gramacy, Robert B. and Lee, Herbert K. H.},
  date = {2010-11-21},
  url = {http://arxiv.org/abs/1007.4580},
  urldate = {2019-11-28},
  abstract = {Most surrogate models for computer experiments are interpolators, and the most common interpolator is a Gaussian process (GP) that deliberately omits a small-scale (measurement) error term called the nugget. The explanation is that computer experiments are, by definition, "deterministic", and so there is no measurement error. We think this is too narrow a focus for a computer experiment and a statistically inefficient way to model them. We show that estimating a (non-zero) nugget can lead to surrogate models with better statistical properties, such as predictive accuracy and coverage, in a variety of common situations.},
  archivePrefix = {arXiv},
  eprint = {1007.4580},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/XSWFF7FY/Gramacy and Lee - 2010 - Cases for the nugget in modeling computer experime.pdf;/home/markus/Zotero/storage/NKB9EUZE/1007.html},
  keywords = {Statistics - Computation},
  primaryClass = {stat}
}

@article{grosse_exploiting_2012,
  title = {Exploiting Compositionality to Explore a Large Space of Model Structures},
  author = {Grosse, Roger and Salakhutdinov, Ruslan R. and Freeman, William T. and Tenenbaum, Joshua B.},
  date = {2012-10-16},
  url = {http://arxiv.org/abs/1210.4856},
  urldate = {2020-04-16},
  abstract = {The recent proliferation of richly structured probabilistic models raises the question of how to automatically determine an appropriate model for a dataset. We investigate this question for a space of matrix decomposition models which can express a variety of widely used models from unsupervised learning. To enable model selection, we organize these models into a context-free grammar which generates a wide variety of structures through the compositional application of a few simple rules. We use our grammar to generically and efficiently infer latent components and estimate predictive likelihood for nearly 2500 structures using a small toolbox of reusable algorithms. Using a greedy search over our grammar, we automatically choose the decomposition structure from raw data by evaluating only a small fraction of all models. The proposed method typically finds the correct structure for synthetic data and backs off gracefully to simpler models under heavy noise. It learns sensible structures for datasets as diverse as image patches, motion capture, 20 Questions, and U.S. Senate votes, all using exactly the same code.},
  archivePrefix = {arXiv},
  eprint = {1210.4856},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/JZ9LEAXX/Grosse et al. - 2012 - Exploiting compositionality to explore a large spa.pdf;/home/markus/Zotero/storage/3G7PV25Q/1210.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{hans_efficient_2009,
  title = {Efficient Uncertainty Propagation for Reinforcement Learning with Limited Data},
  booktitle = {International {{Conference}} on {{Artificial Neural Networks}}},
  author = {Hans, Alexander and Udluft, Steffen},
  date = {2009},
  pages = {70--79},
  publisher = {{Springer}},
  file = {/home/markus/Zotero/storage/28GBNUPD/Hans und Udluft - 2009 - Efficient uncertainty propagation for reinforcemen.pdf;/home/markus/Zotero/storage/WGKEVCHC/978-3-642-04274-4_8.html}
}

@article{havasi_inference_2018,
  title = {Inference in {{Deep Gaussian Processes}} Using {{Stochastic Gradient Hamiltonian Monte Carlo}}},
  author = {Havasi, Marton and Hernández-Lobato, José Miguel and Murillo-Fuentes, Juan José},
  date = {2018-06-14},
  url = {http://arxiv.org/abs/1806.05490},
  urldate = {2018-06-21},
  abstract = {Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian Processes that combine well calibrated uncertainty estimates with the high flexibility of multilayer models. One of the biggest challenges with these models is that exact inference is intractable. The current state-of-the-art inference method, Variational Inference (VI), employs a Gaussian approximation to the posterior distribution. This can be a potentially poor unimodal approximation of the generally multimodal posterior. In this work, we provide evidence for the non-Gaussian nature of the posterior and we apply the Stochastic Gradient Hamiltonian Monte Carlo method to directly sample from it. To efficiently optimize the hyperparameters, we introduce the Moving Window MCEM algorithm. This results in significantly better predictions at a lower computational cost than its VI counterpart. Thus our method establishes a new state-of-the-art for inference in DGPs.},
  archivePrefix = {arXiv},
  eprint = {1806.05490},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/YZ3JDPEU/Havasi et al. - 2018 - Inference in Deep Gaussian Processes using Stochas.pdf;/home/markus/Zotero/storage/LU4HGI5F/1806.html},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@software{head_scikit-optimize_2018,
  title = {Scikit-Optimize: V0.5.2},
  shorttitle = {Scikit-Optimize/Scikit-Optimize},
  author = {Head, Tim and MechCoder and Louppe, Gilles and Shcherbatyi, Iaroslav and {fcharras} and Vinícius, Zé and {cmmalone} and Schröder, Christopher and {nel215} and Campos, Nuno and Young, Todd and Cereda, Stefano and Fan, Thomas and rene- {rex} and Shi, Kejia (KJ) and Schwabedal, Justus and {carlosdanielcsantos} and Hvass-Labs and Pak, Mikhail and SoManyUsernamesTaken and Callaway, Fred and Estève, Loïc and Besson, Lilian and Cherti, Mehdi and Pfannschmidt, Karlson and Linzberger, Fabian and Cauet, Christophe and Gut, Anna and Mueller, Andreas and Fabisch, Alexander},
  date = {2018-03-25},
  doi = {10.5281/zenodo.1207017},
  url = {https://zenodo.org/record/1207017},
  urldate = {2020-04-16},
  abstract = {Version 0.5.2 Bug fixes Separated n\_points from n\_jobs in BayesSearchCV. Dimensions now support boolean np.arrays. Maintenance matplotlib is now an optional requirement (install with pip install 'scikit-optimize[plots]')},
  file = {/home/markus/Zotero/storage/5HN4UWEE/1207017.html},
  options = {useprefix=true},
  organization = {{Zenodo}}
}

@inproceedings{hein_benchmark_2017,
  title = {A Benchmark Environment Motivated by Industrial Control Problems},
  booktitle = {2017 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  author = {Hein, Daniel and Depeweg, Stefan and Tokic, Michel and Udluft, Steffen and Hentschel, Alexander and Runkler, Thomas A. and Sterzing, Volkmar},
  date = {2017-11},
  pages = {1--8},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/SSCI.2017.8280935},
  url = {http://ieeexplore.ieee.org/document/8280935/},
  urldate = {2019-02-19},
  abstract = {In the research area of reinforcement learning (RL), frequently novel and promising methods are developed and introduced to the RL community. However, although many researchers are keen to apply their methods on real-world problems, implementing such methods in real industry environments often is a frustrating and tedious process. Generally, academic research groups have only limited access to real industrial data and applications. For this reason, new methods are usually developed, evaluated and compared by using artificial software benchmarks. On one hand, these benchmarks are designed to provide interpretable RL training scenarios and detailed insight into the learning process of the method on hand. On the other hand, they usually do not share much similarity with industrial real-world applications. For this reason we used our industry experience to design a benchmark which bridges the gap between freely available, documented, and motivated artificial benchmarks and properties of real industrial problems. The resulting industrial benchmark (IB) has been made publicly available to the RL community by publishing its Java and Python code, including an OpenAI Gym wrapper, on Github. In this paper we motivate and describe in detail the IB’s dynamics and identify prototypic experimental settings that capture common situations in real-world industry control problems.},
  eventtitle = {2017 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  file = {/home/markus/Zotero/storage/P9Z3ASIR/Hein et al. - 2017 - A benchmark environment motivated by industrial co.pdf;/home/markus/Zotero/storage/QTA4H4W9/Hein et al. - 2017 - A benchmark environment motivated by industrial co.pdf;/home/markus/Zotero/storage/XMHQWZRL/8280935.html},
  isbn = {978-1-5386-2726-6},
  keywords = {academic research groups,artificial software benchmarks,Automobiles,benchmark environment,Benchmark testing,frustrating process,Games,Helicopters,industrial control,Industrial control,industrial data,Industries,industry environments,industry experience,interpretable RL training scenarios,learning (artificial intelligence),learning process,motivated artificial benchmarks,public domain software,real-world applications,real-world industry control problems,reinforcement learning,research area,RL community,tedious process,Wind turbines},
  langid = {english}
}

@inproceedings{hensman_gaussian_2013,
  title = {Gaussian {{Processes}} for {{Big Data}}},
  booktitle = {Uncertainty in {{Artificial Intelligence}}},
  author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D.},
  date = {2013},
  pages = {282},
  publisher = {{Citeseer}},
  file = {/home/markus/Zotero/storage/EU8WZFR4/Hensman et al. - 2013 - Gaussian Processes for Big Data.pdf;/home/markus/Zotero/storage/XV3VH9PJ/Hensman et al. - 2013 - Gaussian Processes for Big Data.pdf;/home/markus/Zotero/storage/2JAR4BNM/1309.html;/home/markus/Zotero/storage/9GMK8QF5/auD.html;/home/markus/Zotero/storage/ISZ4Z86Q/1309.html},
  keywords = {Computer Science - Learning,Statistics - Machine Learning}
}

@incollection{hensman_mcmc_2015,
  title = {{{MCMC}} for {{Variationally Sparse Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Hensman, James and Matthews, Alexander G and Filippone, Maurizio and Ghahramani, Zoubin},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  date = {2015},
  pages = {1648--1656},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/5875-mcmc-for-variationally-sparse-gaussian-processes.pdf},
  urldate = {2020-05-19},
  file = {/home/markus/Zotero/storage/AMDCZYAJ/Hensman et al. - 2015 - MCMC for Variationally Sparse Gaussian Processes.pdf;/home/markus/Zotero/storage/BMPCGTGE/5875-mcmc-for-variationally-sparse-gaussian-processes.html}
}

@article{hensman_nested_2014,
  title = {Nested {{Variational Compression}} in {{Deep Gaussian Processes}}},
  author = {Hensman, James and Lawrence, Neil D.},
  date = {2014-12-03},
  url = {http://arxiv.org/abs/1412.1370},
  urldate = {2017-07-19},
  abstract = {Deep Gaussian processes provide a flexible approach to probabilistic modelling of data using either supervised or unsupervised learning. For tractable inference approximations to the marginal likelihood of the model must be made. The original approach to approximate inference in these models used variational compression to allow for approximate variational marginalization of the hidden variables leading to a lower bound on the marginal likelihood of the model [Damianou and Lawrence, 2013]. In this paper we extend this idea with a nested variational compression. The resulting lower bound on the likelihood can be easily parallelized or adapted for stochastic variational inference.},
  archivePrefix = {arXiv},
  eprint = {1412.1370},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/ZKNA6NYN/Hensman and Lawrence - 2014 - Nested Variational Compression in Deep Gaussian Pr.pdf;/home/markus/Zotero/storage/UMQ96R94/1412.html},
  keywords = {Statistics - Machine Learning},
  primaryClass = {stat}
}

@article{hensman_scalable_2015,
  title = {Scalable Variational {{Gaussian}} Process Classification},
  author = {Hensman, James and Matthews, Alexander G. de G. and Ghahramani, Zoubin},
  date = {2015},
  journaltitle = {Journal of Machine Learning Research},
  volume = {38},
  pages = {351--360},
  abstract = {Gaussian process classification is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, outperforming the state of the art on benchmark datasets. Importantly, the variational formulation can be exploited to allow classification in problems with millions of data points, as we demonstrate in experiments. Copyright 2015 by the authors.},
  file = {/home/markus/Zotero/storage/T4WFAQPK/Hensman et al. - 2014 - Scalable Variational Gaussian Process Classificati.pdf;/home/markus/Zotero/storage/5GEKF8R7/1411.html;/home/markus/Zotero/storage/FX4I5R8Q/display.html},
  keywords = {Statistics - Machine Learning}
}

@book{higham_accuracy_2002,
  title = {Accuracy and Stability of Numerical Algorithms},
  author = {Higham, Nicholas J.},
  date = {2002},
  edition = {2nd ed},
  publisher = {{Society for Industrial and Applied Mathematics}},
  location = {{Philadelphia}},
  file = {/home/markus/Zotero/storage/DS4B7YM9/Higham - 2002 - Accuracy and stability of numerical algorithms.pdf},
  isbn = {978-0-89871-521-7},
  keywords = {Computer algorithms,Data processing,Numerical analysis},
  langid = {english},
  pagetotal = {680}
}

@article{hodge_survey_2004,
  title = {A Survey of Outlier Detection Methodologies},
  author = {Hodge, Victoria and Austin, Jim},
  date = {2004},
  journaltitle = {Artificial intelligence review},
  volume = {22},
  pages = {85--126},
  file = {/home/markus/Zotero/storage/2KWBFKJ4/Hodge und Austin - 2004 - A survey of outlier detection methodologies.pdf;/home/markus/Zotero/storage/M35HYIRV/BAIRE.0000045502.10941.html},
  number = {2}
}

@article{jacobs_adaptive_1991,
  title = {Adaptive Mixtures of Local Experts},
  author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  date = {1991},
  journaltitle = {Neural computation},
  volume = {3},
  pages = {79--87},
  file = {/home/markus/Zotero/storage/T4BQHV93/Jacobs et al. - 1991 - Adaptive mixtures of local experts.pdf;/home/markus/Zotero/storage/APX3N2YA/neco.1991.3.1.html;/home/markus/Zotero/storage/WZAY9YTW/auD.html},
  number = {1}
}

@inproceedings{jenatton_bayesian_2017,
  title = {Bayesian Optimization with Tree-Structured Dependencies},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}} - {{Volume}} 70},
  author = {Jenatton, Rodolphe and Archambeau, Cedric and Gonzalez, Javier and Seeger, Matthias},
  date = {2017-08-06},
  pages = {1655--1664},
  publisher = {{JMLR.org}},
  location = {{Sydney, NSW, Australia}},
  abstract = {Bayesian optimization has been successfully used to optimize complex black-box functions whose evaluations are expensive. In many applications, like in deep learning and predictive analytics, the optimization domain is itself complex and structured. In this work, we focus on use cases where this domain exhibits a known dependency structure. The benefit of leveraging this structure is twofold: we explore the search space more efficiently and posterior inference scales more favorably with the number of observations than Gaussian Process-based approaches published in the literature. We introduce a novel surrogate model for Bayesian optimization which combines independent Gaussian Processes with a linear model that encodes a tree-based dependency structure and can transfer information between overlapping decision sequences. We also design a specialized two-step acquisition function that explores the search space more effectively. Our experiments on synthetic tree-structured objectives and on the tuning of feedforward neural networks show that our method compares favorably with competing approaches.},
  file = {/home/markus/Zotero/storage/8GPE8F8W/Jenatton et al. - 2017 - Bayesian optimization with tree-structured depende.pdf},
  series = {{{ICML}}'17}
}

@article{johnson_googles_2017,
  title = {Google’s Multilingual Neural Machine Translation System: {{Enabling}} Zero-Shot Translation},
  shorttitle = {Google’s Multilingual Neural Machine Translation System},
  author = {Johnson, Melvin and Schuster, Mike and Le, Quoc V. and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Viégas, Fernanda and Wattenberg, Martin and Corrado, Greg},
  date = {2017},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {339--351},
  publisher = {{MIT Press}},
  file = {/home/markus/Zotero/storage/VBTXWAD7/Johnson et al. - 2017 - Google’s multilingual neural machine translation s.pdf;/home/markus/Zotero/storage/PIM2GQJK/tacl_a_00065.html}
}

@book{journel_mining_1978,
  title = {Mining Geostatistics},
  author = {Journel, Andre G. and Huijbregts, Ch J.},
  date = {1978},
  publisher = {{Academic press}},
  file = {/home/markus/Zotero/storage/T8DUJGCJ/27687.pdf}
}

@article{jylanki_robust_2011,
  title = {Robust {{Gaussian Process Regression}} with a {{Student}}-{\emph{t}} {{Likelihood}}},
  author = {Jylänki, Pasi and Vanhatalo, Jarno and Vehtari, Aki},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  pages = {3227--3257},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v12/jylanki11a.html},
  urldate = {2020-04-16},
  file = {/home/markus/Zotero/storage/PM8I457Z/Jylänki et al. - 2011 - Robust Gaussian Process Regression with a Student-.pdf;/home/markus/Zotero/storage/G3MDWNCR/jylanki11a.html},
  number = {99}
}

@inproceedings{kaelbling_learning_2017,
  title = {Learning Composable Models of Parameterized Skills},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Kaelbling, Leslie Pack and Lozano-Pérez, Tomás},
  date = {2017-05},
  pages = {886--893},
  doi = {10.1109/ICRA.2017.7989109},
  abstract = {There has been a great deal of work on learning new robot skills, but very little consideration of how these newly acquired skills can be integrated into an overall intelligent system. A key aspect of such a system is compositionality: newly learned abilities have to be characterized in a form that will allow them to be flexibly combined with existing abilities, affording a (good!) combinatorial explosion in the robot's abilities. In this paper, we focus on learning models of the preconditions and effects of new parameterized skills, in a form that allows those actions to be combined with existing abilities by a generative planning and execution system.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  file = {/home/markus/Zotero/storage/5GPH9MT9/7989109.html},
  keywords = {composable models,compositionality,control engineering computing,Explosions,generative planning and execution system,Generators,intelligent robots,intelligent system,Intelligent systems,learning (artificial intelligence),learning models,parameterized skills,Planning,robot programming,robot skills,Robots,Training,Uncertainty}
}

@incollection{kaiser_bayesian_2018,
  title = {Bayesian {{Alignments}} of {{Warped Multi}}-{{Output Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Kaiser, Markus and Otte, Clemens and Runkler, Thomas and Ek, Carl Henrik},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {6995--7004},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/7931-bayesian-alignments-of-warped-multi-output-gaussian-processes.pdf},
  urldate = {2019-01-23},
  archivePrefix = {arXiv},
  eprint = {1710.02766},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/MJQDUDFP/Kaiser et al. - 2017 - Bayesian Alignments of Warped Multi-Output Gaussia.pdf;/home/markus/Zotero/storage/N8FXLQBZ/Kaiser et al. - 2018 - Bayesian Alignments of Warped Multi-Output Gaussia.pdf;/home/markus/Zotero/storage/P98SK2MP/Kaiser et al. - 2017 - Bayesian Alignments of Warped Multi-Output Gaussia.pdf;/home/markus/Zotero/storage/5KMTSPFI/7931-bayesian-alignments-of-warped-multi-output-gaussian-processes.html;/home/markus/Zotero/storage/AHX4Q83I/1710.html;/home/markus/Zotero/storage/UKGW6CEX/1710.html},
  keywords = {Computer Science - Learning,Computer Science - Machine Learning,mrksr,Statistics - Machine Learning}
}

@article{kaiser_bayesian_2020,
  title = {Bayesian Decomposition of Multi-Modal Dynamical Systems for Reinforcement Learning},
  author = {Kaiser, Markus and Otte, Clemens and Runkler, Thomas A. and Ek, Carl Henrik},
  date = {2020-04-10},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.12.132},
  url = {http://www.sciencedirect.com/science/article/pii/S0925231220305026},
  urldate = {2020-05-11},
  abstract = {In this paper, we present a model-based reinforcement learning system where the transition model is treated in a Bayesian manner. The approach naturally lends itself to exploit expert knowledge by introducing priors to impose structure on the underlying learning task. The additional information introduced to the system means that we can learn from small amounts of data, recover an interpretable model and, importantly, provide predictions with an associated uncertainty. To show the benefits of the approach, we use a challenging data set where the dynamics of the underlying system exhibit both operational phase shifts and heteroscedastic noise. Comparing our model to NFQ and BNN+LV, we show how our approach yields human-interpretable insight about the underlying dynamics while also increasing data-efficiency.},
  file = {/home/markus/Zotero/storage/SH9SR2MA/Kaiser et al. - 2020 - Bayesian decomposition of multi-modal dynamical sy.pdf;/home/markus/Zotero/storage/LNL9XGQZ/S0925231220305026.html},
  keywords = {Bayesian machine learning,Data-efficiency,Gaussian processes,Hierarchical gaussian processes,Model-based reinforcement learning,mrksr,Reinforcement learning,Stochastic policy search},
  langid = {english}
}

@inproceedings{kaiser_data_2019,
  title = {Data {{Association}} with {{Gaussian Processes}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Machine Learning}} and {{Knowledge Discovery}} in {{Databases}} ({{ECML PKDD}}) 2019},
  author = {Kaiser, Markus and Otte, Clemens and Runkler, Thomas and Ek, Carl Henrik},
  date = {2019-09},
  url = {http://arxiv.org/abs/1810.07158},
  urldate = {2019-02-14},
  abstract = {The data association problem is concerned with separating data coming from different generating processes, for example when data come from different data sources, contain significant noise, or exhibit multimodality. We present a fully Bayesian approach to this problem. Our model is capable of simultaneously solving the data association problem and the induced supervised learning problems. Underpinning our approach is the use of Gaussian process priors to encode the structure of both the data and the data associations. We present an efficient learning scheme based on doubly stochastic variational inference and discuss how it can be applied to deep Gaussian process priors.},
  archivePrefix = {arXiv},
  eprint = {1810.07158},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/EC5P3VNZ/Kaiser et al. - Data Association with Gaussian Processes.pdf;/home/markus/Zotero/storage/ZB65CUNK/Kaiser et al. - 2018 - Data Association with Gaussian Processes.pdf;/home/markus/Zotero/storage/S7VSJTCS/1810.html},
  keywords = {Computer Science - Machine Learning,mrksr,Statistics - Machine Learning}
}

@article{kaiser_interpretable_2019,
  title = {Interpretable {{Dynamics Models}} for {{Data}}-{{Efficient Reinforcement Learning}}},
  author = {Kaiser, Markus and Otte, Clemens and Runkler, Thomas and Ek, Carl Henrik},
  date = {2019},
  journaltitle = {Computational Intelligence and Machine Learning},
  volume = {ESANN 2019 proceedings},
  pages = {6},
  abstract = {In this paper, we present a Bayesian view on model-based reinforcement learning. We use expert knowledge to impose structure on the transition model and present an efficient learning scheme based on variational inference. This scheme is applied to a heteroskedastic and bimodal benchmark problem on which we compare our results to NFQ and show how our approach yields human-interpretable insight about the underlying dynamics while also increasing data-efficiency.},
  file = {/home/markus/Zotero/storage/NNT3NS2A/Kaiser et al. - 2019 - Interpretable Dynamics Models for Data-Efficient R.pdf},
  keywords = {mrksr},
  langid = {english}
}

@patent{kaiser_verfahren_2019,
  title = {Verfahren {{Und Vorrichtungen Zur Automatischen Ermittlung Und}}/{{Oder Kompensation Des Einflusses Einer Wirbelschleppe Auf Eine Windkraftanlage}}},
  author = {Kaiser, Markus and Weber, Marc Christian},
  date = {2019-08-21},
  abstract = {Die Erfindung betrifft ein Ermittlungs-Verfahren zur automatischen Ermittlung des Einflusses einer ersten Windkraftanlage (2) auf eine zweite Windkraftanlage (3) eines Windkraftsystems (1), wobei die ersten Windkraftanlage (2) bezüglich der Windrichtung vor der zweiten Windkraftanlage (3) steht, umfassend die Schritte:- Bereitstellung einer Ermittlungseinheit (6) umfassend eine trainierte Zustandsvorhersage-Einheit (fE'),- Bereitstellung eines Datensatzes (D) umfassend Zustands-Zeitreihendaten (Z) zum Zustand zumindest der ersten Windkraftanlage (2),- Voraussage des Zustandes der hinteren, zweiten Windkraftanlage (3) mittels der trainierten Zustandsvorhersage-Einheit (fE') auf Basis der Zustands-Zeitreihendaten (Z) des Datensatzes (D).Die Erfindung betrifft des Weiteren ein Verfahren zum Erstellen einer solchen Ermittlungseinheit (6) sowie eine solche Ermittlungseinheit (6) mit einer Zustandsvorhersage-Einheit (fE') und ggf. einer Zeitkorrelations-Einheit (LE), eine Lern-Rechenvorrichtung zum Training einer Ermittlungseinheit (6), ein Steuerverfahren sowie eine Steuereinrichtung und ein entsprechendes Windkraftsystem.},
  holder = {{Siemens Ag}},
  keywords = {patent},
  number = {3527817A1},
  type = {patenteu}
}

@inproceedings{kingma_variational_2015,
  title = {Variational {{Dropout}} and the {{Local Reparameterization Trick}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Kingma, Diederik P and Salimans, Tim and Welling, Max},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  date = {2015},
  pages = {2575--2583},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick.pdf},
  urldate = {2018-09-12},
  file = {/home/markus/Zotero/storage/89SIZL5F/Kingma et al. - 2015 - Variational Dropout and the Local Reparameterizati.pdf;/home/markus/Zotero/storage/VE5EGL5C/5666-variational-dropout-and-the-local-reparameterization-trick.html}
}

@thesis{kuss_gaussian_2006,
  title = {Gaussian Process Models for Robust Regression, Classification, and Reinforcement Learning},
  author = {Kuss, Malte},
  date = {2006},
  institution = {{Technische Universität Darmstadt Darmstadt, Germany}},
  file = {/home/markus/Zotero/storage/SGCVK5X6/Kuss - 2006 - Gaussian process models for robust regression, cla.pdf},
  type = {PhD Thesis}
}

@incollection{lange_batch_2012,
  title = {Batch Reinforcement Learning},
  booktitle = {Reinforcement Learning},
  author = {Lange, Sascha and Gabel, Thomas and Riedmiller, Martin},
  date = {2012},
  pages = {45--73},
  publisher = {{Springer}},
  file = {/home/markus/Zotero/storage/9AQ568ZU/978-3-642-27645-3_2.html}
}

@inproceedings{lawrence_hierarchical_2007,
  title = {Hierarchical {{Gaussian}} Process Latent Variable Models},
  booktitle = {Proceedings of the 24th International Conference on {{Machine}} Learning},
  author = {Lawrence, Neil D. and Moore, Andrew J.},
  date = {2007},
  pages = {481--488},
  file = {/home/markus/Zotero/storage/QFQRFWNM/1273496.html}
}

@inproceedings{lazaro-gredilla_bayesian_2012,
  title = {Bayesian Warped {{Gaussian}} Processes},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lázaro-Gredilla, Miguel},
  date = {2012},
  pages = {1619--1627},
  url = {http://papers.nips.cc/paper/4494-bayesian-warped-gaussian-processes},
  urldate = {2016-12-06},
  file = {/home/markus/Zotero/storage/NTS9SDBA/Lázaro-Gredilla - 2012 - Bayesian warped Gaussian processes.pdf;/home/markus/Zotero/storage/HFAKAI4X/4494-bayesian-warped-gaussian-processes.html}
}

@article{lazaro-gredilla_overlapping_2012,
  title = {Overlapping Mixtures of {{Gaussian}} Processes for the Data Association Problem},
  author = {Lázaro-Gredilla, Miguel and Van Vaerenbergh, Steven and Lawrence, Neil D.},
  date = {2012},
  journaltitle = {Pattern Recognition},
  volume = {45},
  pages = {1386--1395},
  file = {/home/markus/Zotero/storage/9PWMT93W/Lázaro-Gredilla et al. - 2011 - Overlapping Mixtures of Gaussian Processes for the.pdf;/home/markus/Zotero/storage/GSDXF73X/Lázaro-Gredilla et al. - 2012 - Overlapping mixtures of Gaussian processes for the.pdf;/home/markus/Zotero/storage/HZYC3SEH/S0031320311004109.html;/home/markus/Zotero/storage/WXY7PLDG/1108.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  number = {4}
}

@inproceedings{lazaro-gredilla_variational_2011,
  title = {Variational {{Heteroscedastic Gaussian Process Regression}}},
  author = {Lazaro-Gredilla, Miguel and Titsias, Michalis K.},
  date = {2011-01-01},
  url = {https://openreview.net/forum?id=BJNsBoWdWB},
  urldate = {2020-04-16},
  abstract = {Standard Gaussian processes (GPs) model observations' noise as constant throughout input space. This is often a too restrictive assumption, but one that is needed for GP inference to be tractable....},
  eventtitle = {{{ICML}}},
  file = {/home/markus/Zotero/storage/8TRDD8SX/L&#xE1 et al. - 2011 - Variational Heteroscedastic Gaussian Process Regre.pdf;/home/markus/Zotero/storage/273HEBAR/forum.html}
}

@article{levine_reinforcement_2018,
  title = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}: {{Tutorial}} and {{Review}}},
  shorttitle = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}},
  author = {Levine, Sergey},
  date = {2018-05-20},
  url = {http://arxiv.org/abs/1805.00909},
  urldate = {2020-02-18},
  abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
  archivePrefix = {arXiv},
  eprint = {1805.00909},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/N8D2BMI9/Levine - 2018 - Reinforcement Learning and Control as Probabilisti.pdf;/home/markus/Zotero/storage/FN6PF6J5/1805.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{maddison_concrete_2016,
  title = {The {{Concrete Distribution}}: {{A Continuous Relaxation}} of {{Discrete Random Variables}}},
  shorttitle = {The {{Concrete Distribution}}},
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  date = {2016-11-02},
  url = {http://arxiv.org/abs/1611.00712},
  urldate = {2018-09-12},
  abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
  archivePrefix = {arXiv},
  eprint = {1611.00712},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/4JCGSNBV/Maddison et al. - 2016 - The Concrete Distribution A Continuous Relaxation.pdf;/home/markus/Zotero/storage/M4WZ8CC4/1611.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@incollection{malkomes_automating_2018,
  title = {Automating {{Bayesian}} Optimization with {{Bayesian}} Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Malkomes, Gustavo and Garnett, Roman},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {5984--5994},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/7838-automating-bayesian-optimization-with-bayesian-optimization.pdf},
  urldate = {2019-05-17},
  file = {/home/markus/Zotero/storage/PKD8HNJT/Malkomes and Garnett - 2018 - Automating Bayesian optimization with Bayesian opt.pdf;/home/markus/Zotero/storage/8KYIW8NX/7838-automating-bayesian-optimization-with-bayesian-optimization.html}
}

@incollection{malkomes_bayesian_2016,
  title = {Bayesian Optimization for Automated Model Selection},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Malkomes, Gustavo and Schaff, Charles and Garnett, Roman},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  date = {2016},
  pages = {2900--2908},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/6466-bayesian-optimization-for-automated-model-selection.pdf},
  urldate = {2020-04-16},
  file = {/home/markus/Zotero/storage/A83HUD22/Malkomes et al. - 2016 - Bayesian optimization for automated model selectio.pdf;/home/markus/Zotero/storage/QQ5KIZ92/6466-bayesian-optimization-for-automated-model-selection.html}
}

@article{martinez-cantin_practical_2017,
  title = {Practical {{Bayesian}} Optimization in the Presence of Outliers},
  author = {Martinez-Cantin, Ruben and Tee, Kevin and McCourt, Michael},
  date = {2017-12-12},
  url = {http://arxiv.org/abs/1712.04567},
  urldate = {2020-04-16},
  abstract = {Inference in the presence of outliers is an important field of research as outliers are ubiquitous and may arise across a variety of problems and domains. Bayesian optimization is method that heavily relies on probabilistic inference. This allows outstanding sample efficiency because the probabilistic machinery provides a memory of the whole optimization process. However, that virtue becomes a disadvantage when the memory is populated with outliers, inducing bias in the estimation. In this paper, we present an empirical evaluation of Bayesian optimization methods in the presence of outliers. The empirical evidence shows that Bayesian optimization with robust regression often produces suboptimal results. We then propose a new algorithm which combines robust regression (a Gaussian process with Student-t likelihood) with outlier diagnostics to classify data points as outliers or inliers. By using an scheduler for the classification of outliers, our method is more efficient and has better convergence over the standard robust regression. Furthermore, we show that even in controlled situations with no expected outliers, our method is able to produce better results.},
  archivePrefix = {arXiv},
  eprint = {1712.04567},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/TBJFKFD3/Martinez-Cantin et al. - 2017 - Practical Bayesian optimization in the presence of.pdf;/home/markus/Zotero/storage/DSTDHJ5J/1712.html},
  keywords = {90C26; 62K25; 62F35,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{martinez-cantin_robust_2017,
  title = {Robust {{Bayesian Optimization}} with {{Student}}-t {{Likelihood}}},
  author = {Martinez-Cantin, Ruben and McCourt, Michael and Tee, Kevin},
  date = {2017-07-18},
  url = {http://arxiv.org/abs/1707.05729},
  urldate = {2020-04-16},
  abstract = {Bayesian optimization has recently attracted the attention of the automatic machine learning community for its excellent results in hyperparameter tuning. BO is characterized by the sample efficiency with which it can optimize expensive black-box functions. The efficiency is achieved in a similar fashion to the learning to learn methods: surrogate models (typically in the form of Gaussian processes) learn the target function and perform intelligent sampling. This surrogate model can be applied even in the presence of noise; however, as with most regression methods, it is very sensitive to outlier data. This can result in erroneous predictions and, in the case of BO, biased and inefficient exploration. In this work, we present a GP model that is robust to outliers which uses a Student-t likelihood to segregate outliers and robustly conduct Bayesian optimization. We present numerical results evaluating the proposed method in both artificial functions and real problems.},
  archivePrefix = {arXiv},
  eprint = {1707.05729},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/XXLDPBP9/Martinez-Cantin et al. - 2017 - Robust Bayesian Optimization with Student-t Likeli.pdf;/home/markus/Zotero/storage/9XYIP4MH/1707.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{matthews_gpflow_2017,
  title = {{{GPflow}}: {{A Gaussian}} Process Library Using {{TensorFlow}}},
  shorttitle = {{{GPflow}}},
  author = {Matthews, Alexander G. de G. and van der Wilk, Mark and Nickson, Tom and Fujii, Keisuke and Boukouvalas, Alexis and León-Villagrá, Pablo and Ghahramani, Zoubin and Hensman, James},
  date = {2017},
  journaltitle = {Journal of Machine Learning Research},
  volume = {18},
  pages = {1--6},
  url = {http://www.jmlr.org/papers/volume18/16-537/16-537.pdf},
  urldate = {2017-09-27},
  file = {/home/markus/Zotero/storage/X6QGAFR8/Matthews et al. - 2017 - GPflow A Gaussian process library using TensorFlo.pdf},
  number = {40},
  options = {useprefix=true}
}

@article{mccourt_optimization_2016,
  title = {Optimization Test Functions},
  author = {McCourt, Michael},
  date = {2016},
  url = {https://github.com/sigopt/evalset}
}

@incollection{mchutchon_gaussian_2011,
  title = {Gaussian {{Process Training}} with {{Input Noise}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  author = {Mchutchon, Andrew and Rasmussen, Carl E.},
  editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  date = {2011},
  pages = {1341--1349},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/4295-gaussian-process-training-with-input-noise.pdf},
  urldate = {2020-04-16},
  file = {/home/markus/Zotero/storage/AVV5UV4J/Mchutchon and Rasmussen - 2011 - Gaussian Process Training with Input Noise.pdf;/home/markus/Zotero/storage/BU9B6UCP/4295-gaussian-process-training-with-input-noise.html}
}

@inproceedings{minka_expectation_2001,
  title = {Expectation {{Propagation}} for Approximate {{Bayesian}} Inference},
  booktitle = {Proceedings of the {{Seventeenth}} Conference on {{Uncertainty}} in Artificial Intelligence},
  author = {Minka, Thomas P.},
  date = {2001},
  pages = {362--369},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  url = {http://arxiv.org/abs/1301.2294},
  urldate = {2016-09-06},
  abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, "Expectation Propagation", unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. All three algorithms try to recover an approximate distribution which is close in KL divergence to the true distribution. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining certain expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Expectation Propagation also extends belief propagation in the opposite direction - it can propagate richer belief states that incorporate correlations between nodes. Experiments with Gaussian mixture models show Expectation Propagation to be convincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.},
  archivePrefix = {arXiv},
  eprint = {1301.2294},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/KNSHS9MI/Minka - 2013 - Expectation Propagation for approximate Bayesian i.pdf;/home/markus/Zotero/storage/3UKPDVIC/1301.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning}
}

@book{mitchell_machine_1997,
  title = {Machine Learning},
  author = {Mitchell, Tom M.},
  date = {1997},
  publisher = {{McGraw-hill New York}},
  file = {/home/markus/Zotero/storage/HN2MHPEE/Mitchell - 1997 - Machine learning.pdf}
}

@article{moore_efficient_1990,
  title = {Efficient Memory-Based Learning for Robot Control},
  author = {Moore, Andrew William},
  date = {1990},
  publisher = {{Citeseer}},
  file = {/home/markus/Zotero/storage/I5EUT4AB/summary.html}
}

@incollection{morgan_generalization_1990,
  title = {Generalization and {{Parameter Estimation}} in {{Feedforward Nets}}: {{Some Experiments}}},
  shorttitle = {Generalization and {{Parameter Estimation}} in {{Feedforward Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 2},
  author = {Morgan, N. and Bourlard, H.},
  editor = {Touretzky, D. S.},
  date = {1990},
  pages = {630--637},
  publisher = {{Morgan-Kaufmann}},
  url = {http://papers.nips.cc/paper/275-generalization-and-parameter-estimation-in-feedforward-nets-some-experiments.pdf},
  urldate = {2020-05-11},
  file = {/home/markus/Zotero/storage/YG7CAQGL/Morgan and Bourlard - 1990 - Generalization and Parameter Estimation in Feedfor.pdf;/home/markus/Zotero/storage/IX46EIFR/275-generalization-and-parameter-estimation-in-feedforward-nets-some-experiments.html}
}

@book{murphy_machine_2012,
  title = {Machine {{Learning}}: {{A Probabilistic Perspective}}},
  shorttitle = {Machine {{Learning}}},
  author = {Murphy, Kevin P.},
  date = {2012-08-24},
  publisher = {{MIT Press}},
  abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package -- PMTK (probabilistic modeling toolkit) -- that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
  file = {/home/markus/Zotero/storage/FERQXL3E/17.pdf},
  isbn = {978-0-262-01802-9},
  keywords = {Computers / Machine Theory,Textbook},
  langid = {english},
  pagetotal = {1098}
}

@incollection{mutny_efficient_2018,
  title = {Efficient {{High Dimensional Bayesian Optimization}} with {{Additivity}} and {{Quadrature Fourier Features}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Mutny, Mojmir and Krause, Andreas},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {9005--9016},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/8115-efficient-high-dimensional-bayesian-optimization-with-additivity-and-quadrature-fourier-features.pdf},
  urldate = {2020-04-16},
  file = {/home/markus/Zotero/storage/T4L8KKW7/Mutny and Krause - 2018 - Efficient High Dimensional Bayesian Optimization w.pdf;/home/markus/Zotero/storage/8CICUQKN/8115-efficient-high-dimensional-bayesian-optimization-with-additivity-and-quadrature-fourier-fe.html}
}

@incollection{naish-guzman_robust_2008,
  title = {Robust {{Regression}} with {{Twinned Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 20},
  author = {Naish-guzman, Andrew and Holden, Sean},
  editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
  date = {2008},
  pages = {1065--1072},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/3346-robust-regression-with-twinned-gaussian-processes.pdf},
  urldate = {2020-04-16},
  file = {/home/markus/Zotero/storage/SL7R5W86/Naish-guzman and Holden - 2008 - Robust Regression with Twinned Gaussian Processes.pdf;/home/markus/Zotero/storage/MAKFLUVF/3346-robust-regression-with-twinned-gaussian-processes.html}
}

@article{oates_modern_2019,
  title = {A {{Modern Retrospective}} on {{Probabilistic Numerics}}},
  author = {Oates, C. J. and Sullivan, T. J.},
  date = {2019-01-14},
  url = {http://arxiv.org/abs/1901.04457},
  urldate = {2019-10-02},
  abstract = {This article attempts to place the emergence of probabilistic numerics as a mathematical-statistical research field within its historical context and to explore how its gradual development can be related both to applications and to a modern formal treatment. We highlight in particular the parallel contributions of Sul'din and Larkin in the 1960s and how their pioneering early ideas have reached a degree of maturity in the intervening period, mediated by paradigms such as average-case analysis and information-based complexity. We provide a subjective assessment of the state of research in probabilistic numerics and highlight some difficulties to be addressed by future works.},
  archivePrefix = {arXiv},
  eprint = {1901.04457},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/D67KFS6E/Oates and Sullivan - 2019 - A Modern Retrospective on Probabilistic Numerics.pdf;/home/markus/Zotero/storage/HUTII4YQ/1901.html},
  keywords = {62-03; 65-03; 01A60; 01A65; 01A67,Mathematics - History and Overview,Mathematics - Numerical Analysis,Mathematics - Probability,Statistics - Machine Learning},
  primaryClass = {math, stat}
}

@article{oh_bock_2018,
  title = {{{BOCK}} : {{Bayesian Optimization}} with {{Cylindrical Kernels}}},
  shorttitle = {{{BOCK}}},
  author = {Oh, ChangYong and Gavves, Efstratios and Welling, Max},
  date = {2018-06-05},
  url = {http://arxiv.org/abs/1806.01619},
  urldate = {2018-12-04},
  abstract = {A major challenge in Bayesian Optimization is the boundary issue (Swersky, 2017) where an algorithm spends too many evaluations near the boundary of its search space. In this paper, we propose BOCK, Bayesian Optimization with Cylindrical Kernels, whose basic idea is to transform the ball geometry of the search space using a cylindrical transformation. Because of the transformed geometry, the Gaussian Process-based surrogate model spends less budget searching near the boundary, while concentrating its efforts relatively more near the center of the search region, where we expect the solution to be located. We evaluate BOCK extensively, showing that it is not only more accurate and efficient, but it also scales successfully to problems with a dimensionality as high as 500. We show that the better accuracy and scalability of BOCK even allows optimizing modestly sized neural network layers, as well as neural network hyperparameters.},
  archivePrefix = {arXiv},
  eprint = {1806.01619},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/KE83S5VM/Oh et al. - 2018 - BOCK  Bayesian Optimization with Cylindrical Kern.pdf;/home/markus/Zotero/storage/DGAXPTWZ/1806.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{petersen_matrix_2008,
  title = {The Matrix Cookbook},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind and others},
  date = {2008},
  journaltitle = {Technical University of Denmark},
  volume = {7},
  pages = {15},
  url = {http://www.cim.mcgill.ca/~dudek/417/Papers/matrixOperations.pdf},
  urldate = {2016-02-01},
  file = {/home/markus/Zotero/storage/N4W86FFD/imm3274.pdf}
}

@inproceedings{pfingsten_nonstationary_2006,
  title = {Nonstationary Gaussian Process Regression Using a Latent Extension of the Input Space},
  booktitle = {Eighth {{World Meeting}} of the {{International Society}} for {{Bayesian Analysis}} ({{ISBA}} 2006)},
  author = {Pfingsten, Tobias and Kuss, Malte and Rasmussen, Carl Edward},
  date = {2006},
  file = {/home/markus/Zotero/storage/GGRSD5P6/Pfingsten et al. - 2006 - Nonstationary gaussian process regression using a .pdf}
}

@book{rasmussen_gaussian_2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward},
  date = {2006},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.86.3414},
  urldate = {2016-02-01},
  file = {/home/markus/Zotero/storage/X7VMYWR8/Rasmussen - 2006 - Gaussian processes for machine learning.pdf;/home/markus/Zotero/storage/TFXHMXDE/summary.html}
}

@inproceedings{rasmussen_infinite_2002,
  title = {Infinite {{Mixtures}} of {{Gaussian Process Experts}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 14},
  author = {Rasmussen, Carl E. and Ghahramani, Zoubin},
  editor = {Dietterich, T. G. and Becker, S. and Ghahramani, Z.},
  date = {2002},
  pages = {881--888},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/2055-infinite-mixtures-of-gaussian-process-experts.pdf},
  urldate = {2018-08-23},
  file = {/home/markus/Zotero/storage/VHKBGGWV/Rasmussen und Ghahramani - 2002 - Infinite Mixtures of Gaussian Process Experts.pdf;/home/markus/Zotero/storage/3YP3XGSE/2055-infinite-mixtures-of-gaussian-process-experts.html}
}

@article{rezende_stochastic_2014,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  date = {2014-01-16},
  url = {https://arxiv.org/abs/1401.4082},
  urldate = {2018-09-12},
  file = {/home/markus/Zotero/storage/N244K3J2/Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf;/home/markus/Zotero/storage/LEJMJNS8/1401.html},
  langid = {english}
}

@inproceedings{riedmiller_neural_2005,
  title = {Neural Fitted {{Q}} Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method},
  booktitle = {European {{Conference}} on {{Machine Learning}}},
  author = {Riedmiller, Martin},
  date = {2005},
  pages = {317--328},
  publisher = {{Springer}},
  file = {/home/markus/Zotero/storage/3JSSKLJS/Riedmiller - 2005 - Neural fitted Q iteration–first experiences with a.pdf;/home/markus/Zotero/storage/SCF964Y7/11564096_32.html}
}

@article{rossi_rethinking_2020,
  title = {Rethinking {{Sparse Gaussian Processes}}: {{Bayesian Approaches}} to {{Inducing}}-{{Variable Approximations}}},
  shorttitle = {Rethinking {{Sparse Gaussian Processes}}},
  author = {Rossi, Simone and Heinonen, Markus and Bonilla, Edwin V. and Shen, Zheyang and Filippone, Maurizio},
  date = {2020-03-09},
  url = {http://arxiv.org/abs/2003.03080},
  urldate = {2020-03-14},
  abstract = {Variational inference techniques based on inducing variables provide an elegant framework for scalable posterior estimation in Gaussian process (GP) models. Most previous works treat the locations of the inducing variables, i.e. the inducing inputs, as variational hyperparameters, and these are then optimized together with GP covariance hyper-parameters. While some approaches point to the benefits of a Bayesian treatment of GP hyper-parameters, this has been largely overlooked for the inducing inputs. In this work, we show that treating both inducing locations and GP hyper-parameters in a Bayesian way, by inferring their full posterior, further significantly improves performance. Based on stochastic gradient Hamiltonian Monte Carlo, we develop a fully Bayesian approach to scalable GP and deep GP models, and demonstrate its competitive performance through an extensive experimental campaign across several regression and classification problems.},
  archivePrefix = {arXiv},
  eprint = {2003.03080},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/LUTJTXQK/Rossi et al. - 2020 - Rethinking Sparse Gaussian Processes Bayesian App.pdf;/home/markus/Zotero/storage/3DMKCMFF/2003.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{rousseeuw_robust_2005,
  title = {Robust Regression and Outlier Detection},
  author = {Rousseeuw, Peter J. and Leroy, Annick M.},
  date = {2005},
  volume = {589},
  publisher = {{John wiley \& sons}},
  file = {/home/markus/Zotero/storage/X4BEMJQI/books.html}
}

@book{runkler_data_2016,
  title = {Data {{Analytics}}},
  author = {Runkler, Thomas A.},
  date = {2016},
  publisher = {{Springer Fachmedien Wiesbaden}},
  location = {{Wiesbaden}},
  doi = {10.1007/978-3-658-14075-5},
  url = {http://link.springer.com/10.1007/978-3-658-14075-5},
  urldate = {2020-04-28},
  file = {/home/markus/Zotero/storage/UPP5KXSX/Runkler - 2016 - Data Analytics.pdf},
  isbn = {978-3-658-14075-5},
  langid = {english}
}

@article{russakovsky_imagenet_2015,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  date = {2015-01-29},
  url = {http://arxiv.org/abs/1409.0575},
  urldate = {2020-08-17},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
  archivePrefix = {arXiv},
  eprint = {1409.0575},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/E5HW9ZJT/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf;/home/markus/Zotero/storage/YPML5PSG/1409.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,I.4.8,I.5.2},
  primaryClass = {cs}
}

@inproceedings{salimbeni_doubly_2017,
  title = {Doubly {{Stochastic Variational Inference}} for {{Deep Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Salimbeni, Hugh and Deisenroth, Marc},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  pages = {4588--4599},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.pdf},
  urldate = {2018-10-02},
  file = {/home/markus/Zotero/storage/FTCRG5BC/Salimbeni und Deisenroth - 2017 - Doubly Stochastic Variational Inference for Deep G.pdf;/home/markus/Zotero/storage/TX47KKYE/Salimbeni und Deisenroth - 2017 - Doubly Stochastic Variational Inference for Deep G.pdf;/home/markus/Zotero/storage/AP6UXDGD/1705.html;/home/markus/Zotero/storage/FNPVJ7AA/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.html;/home/markus/Zotero/storage/GVUNMT8K/display.html},
  keywords = {Statistics - Machine Learning}
}

@article{salimbeni_orthogonally_2018,
  title = {Orthogonally {{Decoupled Variational Gaussian Processes}}},
  author = {Salimbeni, Hugh and Cheng, Ching-An and Boots, Byron and Deisenroth, Marc},
  date = {2018-09-24},
  url = {http://arxiv.org/abs/1809.08820},
  urldate = {2018-10-16},
  abstract = {Gaussian processes (GPs) provide a powerful non-parametric framework for reasoning over functions. Despite appealing theory, its superlinear computational and memory complexities have presented a long-standing challenge. State-of-the-art sparse variational inference methods trade modeling accuracy against complexity. However, the complexities of these methods still scales cubically still scale superlinearly in the number of basis functions, implying that that sparse GP methods are able to learn from large datasets only when a small model is used. Recently, a decoupled approach was proposed that removes the unnecessary coupling between the complexities of modeling the mean and the covariance functions of a GP. It achieves a linear complexity in the number of mean parameters, so an expressive posterior mean function can be modeled. While promising, this approach suffers from optimization difficulties due to ill-conditioning and non-convexity. In this work, we propose an alternative decoupled parametrization. It adopts an orthogonal basis in the mean function to model the residues that cannot be learned by the standard coupled approach. Therefore, our method extends, rather than replaces, the coupled approach to achieve strictly better performance. This construction admits a straightforward natural gradient update rule, so the structure of the information manifold that is lost during decoupling can be leveraged to speed up learning. Empirically, our algorithm demonstrates significantly faster convergence in multiple experiments.},
  archivePrefix = {arXiv},
  eprint = {1809.08820},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/6N72XAS2/Salimbeni et al. - 2018 - Orthogonally Decoupled Variational Gaussian Proces.pdf;/home/markus/Zotero/storage/KFI66IL2/1809.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{schepers_improved_2007,
  title = {Improved Modelling of Wake Aerodynamics and Assessment of New Farm Control Strategies},
  author = {Schepers, J G and van der Pijl, S P},
  date = {2007-07-01},
  journaltitle = {Journal of Physics: Conference Series},
  shortjournal = {J. Phys.: Conf. Ser.},
  volume = {75},
  pages = {012039},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/75/1/012039},
  url = {http://stacks.iop.org/1742-6596/75/i=1/a=012039?key=crossref.77f9d84ce31d5b802648696d4d6f1f03},
  urldate = {2020-03-05}
}

@book{scholkopf_learning_2002,
  title = {Learning with {{Kernels}}: {{Support Vector Machines}}, {{Regularization}}, {{Optimization}}, and {{Beyond}}},
  shorttitle = {Learning with {{Kernels}}},
  author = {Schölkopf, Bernhard and Smola, Alexander J.},
  date = {2002-01},
  publisher = {{MIT Press}},
  abstract = {In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -- -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.},
  isbn = {978-0-262-19475-4},
  keywords = {Computers / Intelligence (AI) & Semantics,Computers / Programming / General},
  langid = {english},
  pagetotal = {658}
}

@article{shahriari_taking_2016,
  title = {Taking the {{Human Out}} of the {{Loop}}: {{A Review}} of {{Bayesian Optimization}}},
  shorttitle = {Taking the {{Human Out}} of the {{Loop}}},
  author = {Shahriari, B. and Swersky, K. and Wang, Z. and Adams, R. P. and de Freitas, N.},
  date = {2016-01},
  journaltitle = {Proceedings of the IEEE},
  volume = {104},
  pages = {148--175},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2015.2494218},
  abstract = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  file = {/home/markus/Zotero/storage/65PRW2XH/Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Baye.pdf;/home/markus/Zotero/storage/8KFSWMW2/7352306.html},
  keywords = {Bayes methods,Bayesian Optimization,Big Data,Big data application,Decision making,Design of experiments,Genomes,genomic medicine,human productivity,large-scale heterogeneous computing,Linear programming,massive complex software system,optimisation,Optimization,product quality,response surface methodology,Statistical analysis,statistical learning,storage allocation,storage architecture},
  number = {1}
}

@book{shalev-shwartz_understanding_2014,
  title = {Understanding {{Machine Learning}}},
  author = {Shalev-Shwartz, Shai and Ben-David, Shai},
  date = {2014},
  file = {/home/markus/Zotero/storage/6NSW4FQU/Shalev-Shwartz and Ben-David - Understanding Machine Learning.pdf},
  langid = {english}
}

@article{shi_sparse_2020,
  title = {Sparse {{Orthogonal Variational Inference}} for {{Gaussian Processes}}},
  author = {Shi, Jiaxin and Titsias, Michalis K. and Mnih, Andriy},
  date = {2020-02-29},
  url = {http://arxiv.org/abs/1910.10596},
  urldate = {2020-03-03},
  abstract = {We introduce a new interpretation of sparse variational approximations for Gaussian processes using inducing points, which can lead to more scalable algorithms than previous methods. It is based on decomposing a Gaussian process as a sum of two independent processes: one spanned by a finite basis of inducing points and the other capturing the remaining variation. We show that this formulation recovers existing approximations and at the same time allows to obtain tighter lower bounds on the marginal likelihood and new stochastic variational inference algorithms. We demonstrate the efficiency of these algorithms in several Gaussian process models ranging from standard regression to multi-class classification using (deep) convolutional Gaussian processes and report state-of-the-art results on CIFAR-10 among purely GP-based models.},
  archivePrefix = {arXiv},
  eprint = {1910.10596},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/T9AP38QL/Shi et al. - 2020 - Sparse Orthogonal Variational Inference for Gaussi.pdf;/home/markus/Zotero/storage/QXXQ3NDR/1910.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{shmueli_explain_2010,
  title = {To Explain or to Predict?},
  author = {Shmueli, Galit},
  date = {2010},
  journaltitle = {Statistical science},
  volume = {25},
  pages = {289--310},
  publisher = {{Institute of Mathematical Statistics}},
  file = {/home/markus/Zotero/storage/G3UFM4ET/Shmueli - 2010 - To explain or to predict.pdf;/home/markus/Zotero/storage/CS34YRF4/1294167961.html},
  number = {3}
}

@article{silver_mastering_2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  date = {2017-10},
  journaltitle = {Nature},
  volume = {550},
  pages = {354--359},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  url = {https://www.nature.com/articles/nature24270},
  urldate = {2019-04-04},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
  file = {/home/markus/Zotero/storage/9VNUJ3GJ/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf;/home/markus/Zotero/storage/DLDP8I3H/nature24270.html;/home/markus/Zotero/storage/RE8QUYV6/nature24270.html;/home/markus/Zotero/storage/SBYNLYP2/display.html;/home/markus/Zotero/storage/ZW2KG8K4/nature24270.html},
  langid = {english},
  number = {7676},
  options = {useprefix=true}
}

@thesis{snelson_flexible_2007,
  title = {Flexible and Efficient {{Gaussian}} Process Models for Machine Learning},
  author = {Snelson, Edward Lloyd},
  date = {2007},
  institution = {{Citeseer}},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.4041},
  urldate = {2016-02-01},
  file = {/home/markus/Zotero/storage/T34X6F3F/Snelson - 2007 - Flexible and efficient Gaussian process models for.pdf}
}

@inproceedings{snelson_sparse_2005,
  title = {Sparse {{Gaussian}} Processes Using Pseudo-Inputs},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Snelson, Edward and Ghahramani, Zoubin},
  date = {2005},
  pages = {1257--1264},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2005_543.pdf},
  urldate = {2016-02-01},
  file = {/home/markus/Zotero/storage/6U2PZBFH/Snelson and Ghahramani - 2005 - Sparse Gaussian processes using pseudo-inputs.pdf}
}

@article{snelson_warped_2004,
  title = {Warped Gaussian Processes},
  author = {Snelson, Edward and Rasmussen, Carl Edward and Ghahramani, Zoubin},
  date = {2004},
  journaltitle = {Advances in neural information processing systems},
  volume = {16},
  pages = {337--344},
  url = {https://books.google.de/books?hl=de&lr=&id=0F-9C7K8fQ8C&oi=fnd&pg=PA337&dq=bayesian+warped+gaussian+processes&ots=THLrkTTe95&sig=-m3ipE2FEwJhMaXgmZmnzNQdRnE},
  urldate = {2016-12-06},
  file = {/home/markus/Zotero/storage/SDTMP9GR/Snelson et al. - 2004 - Warped gaussian processes.pdf;/home/markus/Zotero/storage/AP3BH28T/books.html}
}

@article{snoek_input_2014,
  title = {Input {{Warping}} for {{Bayesian Optimization}} of {{Non}}-Stationary {{Functions}}},
  author = {Snoek, Jasper and Swersky, Kevin and Zemel, Richard S. and Adams, Ryan P.},
  date = {2014-02-04},
  url = {http://arxiv.org/abs/1402.0929},
  urldate = {2017-07-31},
  abstract = {Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions. The ability to accurately model distributions over functions is critical to the effectiveness of Bayesian optimization. Although Gaussian processes provide a flexible prior over functions which can be queried efficiently, there are various classes of functions that remain difficult to model. One of the most frequently occurring of these is the class of non-stationary functions. The optimization of the hyperparameters of machine learning algorithms is a problem domain in which parameters are often manually transformed a priori, for example by optimizing in "log-space," to mitigate the effects of spatially-varying length scale. We develop a methodology for automatically learning a wide family of bijective transformations or warpings of the input space using the Beta cumulative distribution function. We further extend the warping framework to multi-task Bayesian optimization so that multiple tasks can be warped into a jointly stationary space. On a set of challenging benchmark optimization tasks, we observe that the inclusion of warping greatly improves on the state-of-the-art, producing better results faster and more reliably.},
  archivePrefix = {arXiv},
  eprint = {1402.0929},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/ELZ9CMFF/Snoek et al. - 2014 - Input Warping for Bayesian Optimization of Non-sta.pdf;/home/markus/Zotero/storage/5ECB3EET/1402.html},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@incollection{snoek_practical_2012,
  title = {Practical {{Bayesian Optimization}} of {{Machine Learning Algorithms}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  date = {2012},
  pages = {2951--2959},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf},
  urldate = {2019-05-18},
  file = {/home/markus/Zotero/storage/3VJUYQWP/Snoek et al. - 2012 - Practical Bayesian Optimization of Machine Learnin.pdf;/home/markus/Zotero/storage/9EQZHPHL/4522-practical-bayesian-optimization-of-machine-learning-algorithms.html}
}

@article{soleimanzadeh_controller_2011,
  title = {Controller Design for a Wind Farm, Considering Both Power and Load Aspects},
  author = {Soleimanzadeh, Maryam and Wisniewski, Rafael},
  date = {2011-06-01},
  journaltitle = {Mechatronics},
  shortjournal = {Mechatronics},
  volume = {21},
  pages = {720--727},
  issn = {0957-4158},
  doi = {10.1016/j.mechatronics.2011.02.008},
  url = {http://www.sciencedirect.com/science/article/pii/S0957415811000328},
  urldate = {2020-03-05},
  abstract = {In this paper, a wind farm controller is developed that distributes power references among wind turbines while it reduces their structural loads. The proposed controller is based on a spatially discrete model of the farm, which delivers an approximation of wind speed in the vicinity of each wind turbine. The control algorithm determines the reference signals for each individual wind turbine controller in two scenarios based on low and high wind speed. In low wind speed, the reference signals for rotor speed are adjusted, taking the trade-off between power maximization and load minimization into account. In high wind speed, the power and pitch reference signals are determined while structural loads are minimized. To the best of authors’ knowledge, the proposed dynamical model is a suitable framework for control, since it provides a dynamic structure for behavior of the flow in wind farms. Moreover, the controller has been proven exceptionally useful in solving the problem of both power and load optimization on the basis of this model.},
  file = {/home/markus/Zotero/storage/VSDM6SDF/S0957415811000328.html},
  keywords = {Load control,Wind farm control,Wind flow model,Wind turbine},
  langid = {english},
  number = {4}
}

@article{stegle_gaussian_2008,
  title = {Gaussian {{Process Robust Regression}} for {{Noisy Heart Rate Data}}},
  author = {Stegle, O. and Fallert, S.V. and MacKay, D.J. and Brage, S.},
  date = {2008-09},
  journaltitle = {IEEE Transactions on Biomedical Engineering},
  shortjournal = {IEEE Trans. Biomed. Eng.},
  volume = {55},
  pages = {2143--2151},
  issn = {0018-9294, 1558-2531},
  doi = {10.1109/TBME.2008.923118},
  url = {http://ieeexplore.ieee.org/document/4487100/},
  urldate = {2020-04-16},
  abstract = {Heart rate data collected during nonlaboratory conditions present several data-modeling challenges. First, the noise in such data is often poorly described by a simple Gaussian; it has outliers and errors come in bursts. Second, in large-scale studies the ECG waveform is usually not recorded in full, so one has to deal with missing information. In this paper, we propose a robust postprocessing model for such applications. Our model to infer the latent heart rate time series consists of two main components: unsupervised clustering followed by Bayesian regression. The clustering component uses auxiliary data to learn the structure of outliers and noise bursts. The subsequent Gaussian process regression model uses the cluster assignments as prior information and incorporates expert knowledge about the physiology of the heart. We apply the method to a wide range of heart rate data and obtain convincing predictions along with uncertainty estimates. In a quantitative comparison with existing postprocessing methodology, our model achieves a significant increase in performance.},
  file = {/home/markus/Zotero/storage/3GEKDLBT/Stegle et al. - 2008 - Gaussian Process Robust Regression for Noisy Heart.pdf},
  langid = {english},
  number = {9}
}

@article{stone_cross-validatory_1974,
  title = {Cross-Validatory Choice and Assessment of Statistical Predictions},
  author = {Stone, Mervyn},
  date = {1974},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {36},
  pages = {111--133},
  publisher = {{Wiley Online Library}},
  file = {/home/markus/Zotero/storage/9IKMQD3C/Stone - 1974 - Cross-validatory choice and assessment of statisti.pdf;/home/markus/Zotero/storage/ZPRLAPQT/j.2517-6161.1974.tb00994.html},
  number = {2}
}

@article{sun_functional_2019,
  title = {Functional {{Variational Bayesian Neural Networks}}},
  author = {Sun, Shengyang and Zhang, Guodong and Shi, Jiaxin and Grosse, Roger},
  date = {2019-03-13},
  url = {http://arxiv.org/abs/1903.05779},
  urldate = {2019-12-13},
  abstract = {Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors entailing rich structures, including Gaussian processes and implicit stochastic processes. Empirically, we find fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and scale to large datasets.},
  archivePrefix = {arXiv},
  eprint = {1903.05779},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/XVIVRS9W/Sun et al. - 2019 - Functional Variational Bayesian Neural Networks.pdf;/home/markus/Zotero/storage/BQFH56UL/1903.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018},
  edition = {2},
  publisher = {{MIT Press}},
  location = {{Cambridge, Mass}},
  file = {/home/markus/Zotero/storage/6EQF4BV3/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf},
  isbn = {978-0-262-19398-6},
  keywords = {Reinforcement learning},
  langid = {english},
  pagetotal = {322},
  series = {Adaptive Computation and Machine Learning}
}

@article{thorburn_occams_1915,
  title = {Occam's Razor},
  author = {Thorburn, William M.},
  date = {1915},
  journaltitle = {Mind},
  volume = {24},
  pages = {287--288},
  publisher = {{Narnia}},
  file = {/home/markus/Zotero/storage/YEDMQJDC/946731.html},
  number = {2}
}

@inproceedings{titsias_bayesian_2010,
  title = {Bayesian {{Gaussian}} Process Latent Variable Model},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Titsias, Michalis K. and Lawrence, Neil D.},
  date = {2010},
  pages = {844--851},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_TitsiasL10.pdf},
  urldate = {2016-02-01},
  file = {/home/markus/Zotero/storage/5HPG3ZG9/Titsias and Lawrence - 2010 - Bayesian Gaussian process latent variable model.pdf}
}

@inproceedings{titsias_variational_2009,
  title = {Variational {{Learning}} of {{Inducing Variables}} in {{Sparse Gaussian Processes}}.},
  booktitle = {{{AISTATS}}},
  author = {Titsias, Michalis K.},
  date = {2009},
  volume = {5},
  pages = {567--574},
  url = {http://www.jmlr.org/proceedings/papers/v5/titsias09a/titsias09a.pdf},
  urldate = {2017-04-06},
  file = {/home/markus/Zotero/storage/UTMCPPXS/Titsias - 2009 - Variational Learning of Inducing Variables in Spar.pdf}
}

@inproceedings{tresp_mixtures_2001,
  title = {Mixtures of {{Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 13},
  author = {Tresp, Volker},
  editor = {Leen, T. K. and Dietterich, T. G. and Tresp, V.},
  date = {2001},
  pages = {654--660},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/1900-mixtures-of-gaussian-processes.pdf},
  urldate = {2018-09-26},
  file = {/home/markus/Zotero/storage/C7SSU5TS/Tresp - 2001 - Mixtures of Gaussian Processes.pdf;/home/markus/Zotero/storage/7GPFZX3C/1900-mixtures-of-gaussian-processes.html}
}

@article{tresp_wet_1994,
  title = {The Wet Game of Chicken},
  author = {Tresp, Volker},
  date = {1994},
  journaltitle = {Siemens AG, CT IC 4, Technical Report}
}

@book{trevor_hastie_elements_2013,
  title = {The {{Elements}} of {{Statistical Learning}}: {{Data Mining}}, {{Inference}}, and {{Prediction}}},
  author = {{Trevor Hastie} and {Robert Tibshirani} and {Jerome Friedman}},
  date = {2013-11-10},
  publisher = {{Springer Science \& Business Media}},
  file = {/home/markus/Zotero/storage/TDPNF8U7/152.pdf},
  isbn = {978-0-387-21606-5},
  keywords = {Biology,Computers,Databases,Discrete Mathematics,General,Intelligence (AI) & Semantics,Life Sciences,Mathematical & Statistical Software,Mathematics,Probability & Statistics,Science,Stochastic Processes,Textbook},
  langid = {english}
}

@inproceedings{ustyuzhaninov_compositional_2020,
  title = {Compositional Uncertainty in Deep {{Gaussian}} Processes},
  booktitle = {Proceedings of the 36th {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}} ({{UAI}})},
  author = {Ustyuzhaninov, Ivan and Kazlauskaite, Ieva and Kaiser, Markus and Bodin, Erik and Campbell, Neill D. F. and Ek, Carl Henrik},
  date = {2020-02-25},
  url = {http://arxiv.org/abs/1909.07698},
  urldate = {2020-05-19},
  abstract = {Gaussian processes (GPs) are nonparametric priors over functions. Fitting a GP implies computing a posterior distribution of functions consistent with the observed data. Similarly, deep Gaussian processes (DGPs) should allow us to compute a posterior distribution of compositions of multiple functions giving rise to the observations. However, exact Bayesian inference is intractable for DGPs, motivating the use of various approximations. We show that the application of simplifying mean-field assumptions across the hierarchy leads to the layers of a DGP collapsing to near-deterministic transformations. We argue that such an inference scheme is suboptimal, not taking advantage of the potential of the model to discover the compositional structure in the data. To address this issue, we examine alternative variational inference schemes allowing for dependencies across different layers and discuss their advantages and limitations.},
  archivePrefix = {arXiv},
  eprint = {1909.07698},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/VA356SXW/Ustyuzhaninov et al. - 2020 - Compositional uncertainty in deep Gaussian process.pdf;/home/markus/Zotero/storage/YLUZ86WU/1909.html},
  keywords = {Computer Science - Machine Learning,mrksr,Statistics - Machine Learning}
}

@incollection{vapnik_principles_1992,
  title = {Principles of {{Risk Minimization}} for {{Learning Theory}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 4},
  author = {Vapnik, V.},
  editor = {Moody, J. E. and Hanson, S. J. and Lippmann, R. P.},
  date = {1992},
  pages = {831--838},
  publisher = {{Morgan-Kaufmann}},
  url = {http://papers.nips.cc/paper/506-principles-of-risk-minimization-for-learning-theory.pdf},
  urldate = {2020-05-11},
  file = {/home/markus/Zotero/storage/7FFGSJ8X/Vapnik - 1992 - Principles of Risk Minimization for Learning Theor.pdf;/home/markus/Zotero/storage/TKYDCS4J/506-principles-of-risk-minimization-for-learning-theory.html}
}

@article{wang_gaussian_2012,
  title = {Gaussian {{Process Regression}} with {{Heteroscedastic}} or {{Non}}-{{Gaussian Residuals}}},
  author = {Wang, Chunyi and Neal, Radford M.},
  date = {2012-12-26},
  url = {http://arxiv.org/abs/1212.6246},
  urldate = {2020-04-16},
  abstract = {Gaussian Process (GP) regression models typically assume that residuals are Gaussian and have the same variance for all observations. However, applications with input-dependent noise (heteroscedastic residuals) frequently arise in practice, as do applications in which the residuals do not have a Gaussian distribution. In this paper, we propose a GP Regression model with a latent variable that serves as an additional unobserved covariate for the regression. This model (which we call GPLC) allows for heteroscedasticity since it allows the function to have a changing partial derivative with respect to this unobserved covariate. With a suitable covariance function, our GPLC model can handle (a) Gaussian residuals with input-dependent variance, or (b) non-Gaussian residuals with input-dependent variance, or (c) Gaussian residuals with constant variance. We compare our model, using synthetic datasets, with a model proposed by Goldberg, Williams and Bishop (1998), which we refer to as GPLV, which only deals with case (a), as well as a standard GP model which can handle only case (c). Markov Chain Monte Carlo methods are developed for both modelsl. Experiments show that when the data is heteroscedastic, both GPLC and GPLV give better results (smaller mean squared error and negative log-probability density) than standard GP regression. In addition, when the residual are Gaussian, our GPLC model is generally nearly as good as GPLV, while when the residuals are non-Gaussian, our GPLC model is better than GPLV.},
  archivePrefix = {arXiv},
  eprint = {1212.6246},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/BFHPVHIZ/Wang and Neal - 2012 - Gaussian Process Regression with Heteroscedastic o.pdf;/home/markus/Zotero/storage/GAU5E772/1212.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{waring_vii._1779,
  title = {Vii. Problems Concerning Interpolations},
  author = {Waring, Edward},
  date = {1779},
  journaltitle = {Philosophical transactions of the royal society of London},
  pages = {59--67},
  publisher = {{The Royal Society London}},
  file = {/home/markus/Zotero/storage/LDGGEG9A/rstl.1779.html},
  number = {69}
}

@article{wilson_efficiently_2020,
  title = {Efficiently {{Sampling Functions}} from {{Gaussian Process Posteriors}}},
  author = {Wilson, James T. and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc Peter},
  date = {2020-07-01},
  url = {http://arxiv.org/abs/2002.09309},
  urldate = {2020-07-24},
  abstract = {Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model's success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes' statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.},
  archivePrefix = {arXiv},
  eprint = {2002.09309},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/6X3MSHVJ/Wilson et al. - 2020 - Efficiently Sampling Functions from Gaussian Proce.pdf;/home/markus/Zotero/storage/7FYS5KS9/2002.html},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{wilson_kernel_2015,
  title = {Kernel {{Interpolation}} for {{Scalable Structured Gaussian Processes}} ({{KISS}}-{{GP}})},
  author = {Wilson, Andrew Gordon and Nickisch, Hannes},
  date = {2015-03-03},
  url = {http://arxiv.org/abs/1503.01057},
  urldate = {2017-09-18},
  abstract = {We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling.},
  archivePrefix = {arXiv},
  eprint = {1503.01057},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/WUTMYZE9/Wilson and Nickisch - 2015 - Kernel Interpolation for Scalable Structured Gauss.pdf;/home/markus/Zotero/storage/IPZFBX7C/1503.html},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{yousefi_unsupervised_2016,
  title = {Unsupervised {{Learning}} with {{Imbalanced Data}} via {{Structure Consolidation Latent Variable Model}}},
  author = {Yousefi, Fariba and Dai, Zhenwen and Ek, Carl Henrik and Lawrence, Neil},
  date = {2016-06-30},
  url = {http://arxiv.org/abs/1607.00067},
  urldate = {2016-09-05},
  abstract = {Unsupervised learning on imbalanced data is challenging because, when given imbalanced data, current model is often dominated by the major category and ignores the categories with small amount of data. We develop a latent variable model that can cope with imbalanced data by dividing the latent space into a shared space and a private space. Based on Gaussian Process Latent Variable Models, we propose a new kernel formulation that enables the separation of latent space and derives an efficient variational inference method. The performance of our model is demonstrated with an imbalanced medical image dataset.},
  archivePrefix = {arXiv},
  eprint = {1607.00067},
  eprinttype = {arxiv},
  file = {/home/markus/Zotero/storage/2FD2NKPS/Yousefi et al. - 2016 - Unsupervised Learning with Imbalanced Data via Str.pdf;/home/markus/Zotero/storage/XE6V6UJN/1607.html},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{zhou_generalized_2012,
  title = {Generalized Time Warping for Multi-Modal Alignment of Human Motion},
  booktitle = {Computer {{Vision}} and {{Pattern Recognition}} ({{CVPR}}), 2012 {{IEEE Conference}} On},
  author = {Zhou, Feng and De la Torre, Fernando},
  date = {2012},
  pages = {1282--1289},
  publisher = {{IEEE}},
  file = {/home/markus/Zotero/storage/H9KUZQZQ/Zhou und De la Torre - 2012 - Generalized time warping for multi-modal alignment.pdf;/home/markus/Zotero/storage/A5RQ27H7/auD.html}
}

@article{ziebart_modeling_2010,
  title = {Modeling {{Interaction}} via the {{Principle}} of {{Maximum Causal Entropy}}},
  author = {Ziebart, Brian D and Bagnell, J Andrew and Dey, Anind K},
  date = {2010},
  pages = {8},
  abstract = {The principle of maximum entropy provides a powerful framework for statistical models of joint, conditional, and marginal distributions. However, there are many important distributions with elements of interaction and feedback where its applicability has not been established. This work presents the principle of maximum causal entropy—an approach based on causally conditioned probabilities that can appropriately model the availability and influence of sequentially revealed side information. Using this principle, we derive models for sequential data with revealed information, interaction, and feedback, and demonstrate their applicability for statistically framing inverse optimal control and decision prediction tasks.},
  file = {/home/markus/Zotero/storage/KJJBGJPC/Ziebart et al. - Modeling Interaction via the Principle of Maximum .pdf},
  langid = {english}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

