\chapter{Introduction}
\label{toc:introduction}
In many machine learning problems, the task is to derive a model for observational data that has been generated by some process in nature that is not fully understood.
Starting with an input-vector~$\mat{x}$ describing its initial state, nature produces an output-vector~$\mat{y}$.
Given a set of observational pairs $(\mat{x}, \mat{y})$, we want to learn about the underlying, possibly stochastic, functional dependency $\mat{y} = f(\mat{x})$.
The goal of learning is often to either gain new knowledge about nature by describing the data or to be able to predict outputs $\mat{y}_\ast$ for previously unseen inputs $\mat{x}_\ast$.
While traditional statistics favor what \textcite{breiman_statistical_2001,shmueli_explain_2010} call the white-box modeling culture that focuses on explaining observational data using causal theoretic models, machine learning is more closely associated with the black-box culture.
When formulating a black-box model, we accept that we cannot model nature directly and instead focus on the functional dependency $f$ between inputs and outputs.
Using general function approximators like neural networks or Gaussian processes, any smooth dependency can be approximated given enough observations.
With growing computational power and the availability of large data sets, black-box approaches have led to impressive advancements in domains that are hard to formalize.
Successes have been seen in a wide range of digital tasks such as speech recognition~\parencite{hochreiter_long_1997,chorowski_attention-based_2015,bahdanau_neural_2014,chorowski_attention-based_2015}, computer vision~\parencite{russakovsky_imagenet_2015,lecun_backpropagation_1989}, video games~\parencite{berner_dota_2019,silver_mastering_2016}, or machine translation~\parencite{johnson_googles_2017}.
In such domains, data is often abundant, which shifts the focus away from traditional statistical models to approaches that are highly adaptable and scalable.
At the same time, the consequences of mistakes tend to be mild, enabling fast prototyping and heuristic design choices that can be validated in the deployed systems using the performance metrics of interest.

However, bridging the gap to applications in critical systems in the physical world has proved challenging.
Problem domains like robotics or industrial control introduce a new set of requirements.
In industrial control, machine learning is used to find more efficient or safer control strategies not obvious to the engineers designing a machine~\parencite{hein_benchmark_2017}.
In such scenarios, relevant data are often scarce.
The most valuable data, such as when a machine will fail or how it will behave in new situations, is never produced.
As applications become more safety-relevant, gathering data through exploration can be problematic due to the adverse consequences of failure.
If they are to operate in safety-critical areas, interact with people, or carry responsibility, machine learning systems must be robust, trustworthy, and assessable.
Besides optimizing for good average-case performance, a responsible system needs to reason about plausible worst-cases to be able to avoid them.
To ensure trust, ML-systems often need to be verified by domain experts before deployment, emphasizing a need for interpretability.
Models are required to incorporate and reproduce expert knowledge and make consistent predictions, while at the same time yielding new insights from data.

The highly adaptable and scalable models that lead to success in digital domains are not well-suited to be deployed in safety-critical environments.
Large neural networks generally suffer from low interpretability~\parencite{rudin_stop_2019}, the need for large amounts of data and their inability to quantify uncertainties~\parencite{goodfellow_deep_2016}.
In this work, we explore how to formulate machine learning models that can satisfy the requirements of physical environments.


\section{Bayesian structured models with Gaussian processes}
A key technique that allows us to cope with the requirements of critical systems in the physical world is probabilistic modeling.
Bayesian inference offers a framework to formalize expert knowledge in prior assumptions and combine them with data.
% Using Bayes' rule, a distribution of plausible models is reweighed by their ability to generate data to yield a posterior distribution over plausible explanations.
To make predictions, all plausible models are taken into account, allowing us to both quantify confidence in predictions and take more unlikely but relevant scenarios into consideration.
% Bayesian reasoning was pioneered by Pierre-Simon Laplace in the eighteenth century to extend the prevalent mechanistic world-view with the idea that knowledge is imperfect~\parencite{pulskamp_laplace_2020}.
In modern science, probabilistic models are used in large-scale experiments such as in particle physics~\parencite{khachatryan_observation_2015} or astronomy~\parencite{collaboration_first_2019,the_ligo_scientific_collaboration_observation_2016}, where randomness plays a major role.
These scientific models are generally formulated from a hypothesis testing perspective and follow the white-box philosophy.
Causal theoretic models are extended with statistical theory to handle uncertain data and lead to conclusions in terms of effect sizes.
The focus is on explaining the available observations instead of predicting new situations.

For many applications of machine learning in the physical world, the situation is different.
Since complete theoretic models are not available, the goal of modeling is to infer some aspects of a system's behavior from data while still yielding physically plausible explanations.
Take wind power production as an example:
To optimize the efficiency of a wind-turbine, the speed and pitch have to be controlled according to the local wind conditions.
In a wind-farm, turbines are typically equipped with sensors for wind speed and direction.
The goal is to use these sensor data to produce accurate estimates and forecasts of the wind conditions at every turbine in the farm.
For the ideal case of a homogeneous and very slowly changing wind field, wind conditions can be estimated using the propagation times computed from geometry, wind speed and direction.
In the real world, however, wind fields are not homogeneous, exhibit global and local turbulence, and interfere with the turbines and the terrain inside and outside the farm.
This makes it extremely difficult to construct accurate analytical models of wind propagation in a farm.
At the same time, data-driven black-box approaches struggle due to identifiability issues introduced by the noise of the unpredictable turbulence.
While we cannot formulate analytical models, domain experts do have an intuitive understanding of the underlying physics of wind propagation.
A successful model needs to combine the white-box and black-box approaches:
It needs to reproduce expectations like slowly changing prevailing wind conditions or gusts that travel through the system while still being able to infer new knowledge about specific turbine behavior or the influence of terrain from data.
Domain experts need to be able to verify that they can trust the model's predictions for unseen situations.

In this work, we study how to formulate structured models that fullfill these requirements.
We use information about the structure of a learning problem to formulate machine learning models that reproduce knowledge, are understandable for domain-experts, make physically plausible predictions in unseen situations and can quantify their own uncertainty.
First, we explore how to embed general function approximators in Bayesian probabilistic models to enforce structure and learn from data.
We discuss how to formulate inference schemes based on composite and hierarchical Gaussian process models that yield informative uncertainties.
Second, we study how to use structured models to reproduce expert knowledge and factorize uncertainties to achieve interpretability.
We explore how rich internal structure helps models to generalize to unseen situations.
And third, we discuss how to evaluate structured models.
As evaluation data in critical parts of the system is often scarce in physical environments, performance measures such as the generalization error are not enough to identify a desirable structured model.
To formalize the subjectiveness in the description of desirable models, we will explore how downstream tasks can be taken into account for model selection.


\section{Thesis outline}
The content of this thesis is structured as follows.
In \cref{toc:gp}, we provide a short introduction to Bayesian machine learning and Bayesian nonparametrics with a focus on Gaussian processes (GPs).
We discuss the connection between statistical learning theory and Bayesian inference and their application to hierarchical models.
After introducing GPs, we discuss variational approaches to sparse GPs, their extension to hierarchical GP models, and efficient inference schemes.

In \cref{toc:data_association}, we explore how to formulate structured hierarchical models using the data association problem as an example.
We separate the data association problem into hierarchical components and derive an efficient joint inference scheme, resulting in a fully Bayesian model.
We show that this model is able to factorize multi-modal data into independent processes, providing explicit models for both the processes and assignment probabilities.
Comparisons with previous models show the additional qualitative model capabilities of the structured approach and competitive black-box performance.
However, experiments also show that standard measures are not enough to identify desirable models.

In \cref{toc:alignment}, we formulate a structured Bayesian model that reproduces expert knowledge about wind propagation in a wind-farm.
We interpret the problem of modeling the power production of multiple wind-turbines as a nonlinear alignment problem and derive an efficient inference scheme.
Based on expert knowledge about the underlying latent and turbulent wind field, we can derive a structured model that enforces a physically plausible dependency structure between multiple time-series.
We discuss how these constraints help to solve an otherwise highly ambiguous learning problem and show that the imposed structure leads to a rich internal model-structure that can be interpreted by experts.

In \cref{toc:interpretable_rl}, we consider how to formalize the subjectiveness introduced via expert knowledge and model ambiguities to evalute structured models.
We include the task a model will be used to solve into model selection, which allows us to distinguish between qualitatively different models showing similar performance metrics.
We revisit the data association problem and embed it into a reinforcement learning problem, where identifying the correct underlying dynamics, and therefore a desirable model, is critical to finding a successful policy.
We show that semantic hierarchical structure increases data efficiency and allows domain experts to influence agent behavior through detailed insights into the dynamics of a system.

\Cref{toc:discussion} concludes the work presented in this thesis and interprets the results in a broader context by further exploring the properties and evaluation of structured hierarchical models.
In \cref{toc:discussion:composition}, we present an intuitive argument for why inference schemes based on factorizations between layers cannot represent heterogeneous posteriors.
In \cref{toc:discussion:bo}, we argue why models with suboptimal marginal likelihoods can perform well in hierarchical systems.
In \cref{toc:discussion:mountaincar}, we explore this idea further and consider how tasks can be included in the inference problem directly.
Finally, we discuss possible further directions of research.


\section{Contributions}
The main contributions presented in this thesis appeared in a number of peer-reviewed publications and patents we outline below.
This work presents novel Bayesian structured models inspired by industrial applications.
In \cref{contrib:data_association}, we formulate a fully Bayesian interpretation of the data association problem and derive an efficient inference scheme.
\Cref{contrib:patent_depeweg,contrib:patent_geipel} discuss an application of this model to the prediction of the combustion dynamics of a gas-turbine, where the structured model allows us to distinguish between different dynamical regimes.
In~\cref{contrib:windpower}, we formulate a hierarchical Bayesian model and inference-scheme for a nonlinear time-series alignment problem and show that this model is capable of representing the complex interactions between turbines in a wind farm.
We discuss how this model can be used to derive more efficient controllers for wind-turbines in~\cref{contrib:patent_egedal,contrib:patent_kaiser}.

Using our methods as examples, we investigate the behavior of Bayesian structured models and discuss evaluation strategies.
In \cref{contrib:interpretable_rl,contrib:multi_modal_rl}, we apply our data association model to a reinforcement learning task and we show how a semantic decomposition of the dynamics reduces the data requirements and produces interpretable solutions.
In \cref{contrib:surrogates}, we argue why models with suboptimal marginal likelihoods can perform well in hierarchical systems such as Bayesian optimization problems.
And in \cref{contrib:composition}, we present an argument for why inference schemes based on factorizations between layers cannot represent heterogeneous posteriors.


\subsubsection{Own publications}
\begin{enumerate}
    \item \label[contribution]{contrib:windpower}\fullfullcite{kaiser_bayesian_2018}
    \item \label[contribution]{contrib:data_association}\fullfullcite{kaiser_data_2019}
    \item \label[contribution]{contrib:interpretable_rl}\fullfullcite{kaiser_interpretable_2019}
    \item \label[contribution]{contrib:multi_modal_rl}\fullfullcite{kaiser_bayesian_2020}
    \item \label[contribution]{contrib:surrogates}\fullfullcite{bodin_modulating_2020}
    \item \label[contribution]{contrib:composition}\fullfullcite{ustyuzhaninov_compositional_2020}
\end{enumerate}

\subsubsection{Patents}
\begin{enumerate}
    \setcounter{enumi}{6}
    \item \label[contribution]{contrib:patent_egedal}\fullfullcite{egedal_verfahren_2019}
    \item \label[contribution]{contrib:patent_kaiser}\fullfullcite{kaiser_verfahren_2019}
    \item \label[contribution]{contrib:patent_geipel}\fullfullcite{geipel_transferlernen_2020}
    \item \label[contribution]{contrib:patent_depeweg}\fullfullcite{depeweg_computer_2020}
\end{enumerate}
