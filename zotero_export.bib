
@book{astrom_introduction_1971,
  title = {Introduction to {{Stochastic Control Theory}}},
  author = {Åström, Karl J.},
  date = {1971-02-27},
  publisher = {{Elsevier}},
  abstract = {In this book, we study theoretical and practical aspects of computing methods for mathematical modelling of nonlinear systems. A number of computing techniques are considered, such as methods of operator approximation with any given accuracy; operator interpolation techniques including a non-Lagrange interpolation; methods of system representation subject to constraints associated with concepts of causality, memory and stationarity; methods of system representation with an accuracy that is the best within a given class of models; methods of covariance matrix estimation;methods for low-rank matrix approximations; hybrid methods based on a combination of iterative procedures and best operator approximation; andmethods for information compression and filtering under condition that a filter model should satisfy restrictions associated with causality and different types of memory.As a result, the book represents a blend of new methods in general computational analysis,and specific, but also generic, techniques for study of systems theory ant its particularbranches, such as optimal filtering and information compression.- Best operator approximation,- Non-Lagrange interpolation,- Generic Karhunen-Loeve transform- Generalised low-rank matrix approximation- Optimal data compression- Optimal nonlinear filtering},
  isbn = {978-0-08-095579-7},
  keywords = {Mathematics / Algebra / Linear,Mathematics / Calculus,Mathematics / Mathematical Analysis,Mathematics / Numerical Analysis,Technology & Engineering / Mechanical},
  langid = {english},
  pagetotal = {318}
}

@article{barshalom_tracking_1990,
  title = {Tracking and {{Data Association}}},
  author = {Bar‐Shalom, Yaakov and Fortmann, Thomas E. and Cable, Peter G.},
  date = {1990-02-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {87},
  pages = {918--919},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.398863},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.398863},
  urldate = {2020-03-04},
  file = {C\:\\Users\\pub\\Zotero\\storage\\GIAF2DIW\\1.html},
  number = {2}
}

@article{barto_neuronlike_1983,
  title = {Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems},
  author = {Barto, A. G. and Sutton, R. S. and Anderson, C. W.},
  date = {1983-09},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {SMC-13},
  pages = {834--846},
  issn = {0018-9472},
  doi = {10.1109/TSMC.1983.6313077},
  abstract = {It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\899XKXEN\\Barto et al. - 1983 - Neuronlike adaptive elements that can solve diffic.pdf;C\:\\Users\\pub\\Zotero\\storage\\CL99BSYP\\6313077.html;C\:\\Users\\pub\\Zotero\\storage\\MKQ85H6W\\6313077.html},
  keywords = {adaptive control,adaptive critic element,Adaptive systems,animal learning studies,associative search element,Biological neural networks,learning control problem,learning systems,movable cart,neural nets,neuronlike adaptive elements,Neurons,Pattern recognition,Problem-solving,Supervised learning,Training},
  number = {5}
}

@book{bertsekas_stochastic_1978,
  title = {Stochastic {{Optimal Control}}: {{The Discrete}}-{{Time Case}}},
  author = {Bertsekas, Dimitir P. and Shreve, Steven},
  date = {1978},
  publisher = {{Academic press}},
  abstract = {The book is a comprehensive and theoretically sound treatment of the mathematical foundations of stochastic optimal control of discrete-time systems, including the treatment of the intricate measure-theoretic issues.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\5JHEMU42\\book.pdf},
  isbn = {1-886529-03-5}
}

@report{bishop_mixture_1994,
  title = {Mixture Density Networks},
  author = {Bishop, Christopher M.},
  date = {1994},
  file = {C\:\\Users\\pub\\Zotero\\storage\\HLZBIATM\\Bishop - 1994 - Mixture density networks.pdf}
}

@article{bodin_latent_2017,
  title = {Latent {{Gaussian Process Regression}}},
  author = {Bodin, Erik and Campbell, Neill D. F. and Ek, Carl Henrik},
  date = {2017-07-18},
  url = {http://arxiv.org/abs/1707.05534},
  urldate = {2017-08-29},
  abstract = {We introduce Latent Gaussian Process Regression which is a latent variable extension allowing modelling of non-stationary processes using stationary GP priors. The approach is built on extending the input space of a regression problem with a latent variable that is used to modulate the covariance function over the input space. We show how our approach can be used to model non-stationary processes but also how multi-modal or non-functional processes can be described where the input signal cannot fully disambiguate the output. We exemplify the approach on a set of synthetic data and provide results on real data from geostatistics.},
  archivePrefix = {arXiv},
  eprint = {1707.05534},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\IWU4IK4P\\Bodin et al. - 2017 - Latent Gaussian Process Regression.pdf;C\:\\Users\\pub\\Zotero\\storage\\AZSUGMDS\\1707.html},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{briol_probabilistic_2017,
  title = {Probabilistic {{Integration}}: {{A Role}} in {{Statistical Computation}}?},
  shorttitle = {Probabilistic {{Integration}}},
  author = {Briol, François-Xavier and Oates, Chris J. and Girolami, Mark and Osborne, Michael A. and Sejdinovic, Dino},
  date = {2017-10-18},
  url = {http://arxiv.org/abs/1512.00933},
  urldate = {2020-02-20},
  abstract = {A research frontier has emerged in scientific computation, wherein numerical error is regarded as a source of epistemic uncertainty that can be modelled. This raises several statistical challenges, including the design of statistical methods that enable the coherent propagation of probabilities through a (possibly deterministic) computational work-flow. This paper examines the case for probabilistic numerical methods in routine statistical computation. Our focus is on numerical integration, where a probabilistic integrator is equipped with a full distribution over its output that reflects the presence of an unknown numerical error. Our main technical contribution is to establish, for the first time, rates of posterior contraction for these methods. These show that probabilistic integrators can in principle enjoy the "best of both worlds", leveraging the sampling efficiency of Monte Carlo methods whilst providing a principled route to assess the impact of numerical error on scientific conclusions. Several substantial applications are provided for illustration and critical evaluation, including examples from statistical modelling, computer graphics and a computer model for an oil reservoir.},
  archivePrefix = {arXiv},
  eprint = {1512.00933},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\439DVI6X\\Briol et al. - 2017 - Probabilistic Integration A Role in Statistical C.pdf;C\:\\Users\\pub\\Zotero\\storage\\PIGM8TG3\\1512.html},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{brockman_openai_2016,
  title = {{{OpenAI Gym}}},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  date = {2016-06-05},
  url = {http://arxiv.org/abs/1606.01540},
  urldate = {2018-09-24},
  abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
  archivePrefix = {arXiv},
  eprint = {1606.01540},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\Y7R3KYIX\\Brockman et al. - 2016 - OpenAI Gym.pdf;C\:\\Users\\pub\\Zotero\\storage\\IEKJED9E\\1606.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{choi_choicenet_2018,
  title = {{{ChoiceNet}}: {{Robust Learning}} by {{Revealing Output Correlations}}},
  shorttitle = {{{ChoiceNet}}},
  author = {Choi, Sungjoon and Hong, Sanghoon and Lim, Sungbin},
  date = {2018-05-16},
  url = {http://arxiv.org/abs/1805.06431},
  urldate = {2018-08-23},
  abstract = {In this paper, we focus on the supervised learning problem with corrupted training data. We assume that the training dataset is generated from a mixture of a target distribution and other unknown distributions. We estimate the quality of each data by revealing the correlation between the generated distribution and the target distribution. To this end, we present a novel framework referred to here as ChoiceNet that can robustly infer the target distribution in the presence of inconsistent data. We demonstrate that the proposed framework is applicable to both classification and regression tasks. ChoiceNet is evaluated in comprehensive experiments, where we show that it constantly outperforms existing baseline methods in the handling of noisy data. Particularly, ChoiceNet is successfully applied to autonomous driving tasks where it learns a safe driving policy from a dataset with mixed qualities. In the classification task, we apply the proposed method to the MNIST and CIFAR-10 datasets and it shows superior performances in terms of robustness to noisy labels.},
  archivePrefix = {arXiv},
  eprint = {1805.06431},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\F886D4JS\\Choi et al. - 2018 - ChoiceNet Robust Learning by Revealing Output Cor.pdf;C\:\\Users\\pub\\Zotero\\storage\\M6QSGCFL\\1805.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{choi_robust_2016,
  title = {Robust Learning from Demonstration Using Leveraged {{Gaussian}} Processes and Sparse-Constrained Optimization},
  booktitle = {Robotics and {{Automation}} ({{ICRA}}), 2016 {{IEEE International Conference}} On},
  author = {Choi, Sungjoon and Lee, Kyungjae and Oh, Songhwai},
  date = {2016},
  pages = {470--475},
  publisher = {{IEEE}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\LNLKLPLD\\Choi et al. - 2016 - Robust learning from demonstration using leveraged.pdf;C\:\\Users\\pub\\Zotero\\storage\\AGU553L3\\7487168.html}
}

@article{cockayne_bayesian_2019,
  title = {Bayesian {{Probabilistic Numerical Methods}}},
  author = {Cockayne, Jon and Oates, Chris and Sullivan, Tim and Girolami, Mark},
  date = {2019-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {61},
  pages = {756--789},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/17M1139357},
  url = {http://arxiv.org/abs/1702.03673},
  urldate = {2020-02-14},
  abstract = {The emergent field of probabilistic numerics has thus far lacked clear statistical principals. This paper establishes Bayesian probabilistic numerical methods as those which can be cast as solutions to certain inverse problems within the Bayesian framework. This allows us to establish general conditions under which Bayesian probabilistic numerical methods are well-defined, encompassing both non-linear and non-Gaussian models. For general computation, a numerical approximation scheme is proposed and its asymptotic convergence established. The theoretical development is then extended to pipelines of computation, wherein probabilistic numerical methods are composed to solve more challenging numerical tasks. The contribution highlights an important research frontier at the interface of numerical analysis and uncertainty quantification, with a challenging industrial application presented.},
  archivePrefix = {arXiv},
  eprint = {1702.03673},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\IYE9Y4P8\\Cockayne et al. - 2019 - Bayesian Probabilistic Numerical Methods.pdf;C\:\\Users\\pub\\Zotero\\storage\\X8T74HX4\\1702.html},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Methodology},
  number = {3}
}

@article{cockayne_probabilistic_2017,
  title = {Probabilistic {{Numerical Methods}} for {{Partial Differential Equations}} and {{Bayesian Inverse Problems}}},
  author = {Cockayne, Jon and Oates, Chris and Sullivan, Tim and Girolami, Mark},
  date = {2017-07-11},
  url = {http://arxiv.org/abs/1605.07811},
  urldate = {2020-02-18},
  abstract = {This paper develops a probabilistic numerical method for solution of partial differential equations (PDEs) and studies application of that method to PDE-constrained inverse problems. This approach enables the solution of challenging inverse problems whilst accounting, in a statistically principled way, for the impact of discretisation error due to numerical solution of the PDE. In particular, the approach confers robustness to failure of the numerical PDE solver, with statistical inferences driven to be more conservative in the presence of substantial discretisation error. Going further, the problem of choosing a PDE solver is cast as a problem in the Bayesian design of experiments, where the aim is to minimise the impact of solver error on statistical inferences; here the challenge of non-linear PDEs is also considered. The method is applied to parameter inference problems in which discretisation error in non-negligible and must be accounted for in order to reach conclusions that are statistically valid.},
  archivePrefix = {arXiv},
  eprint = {1605.07811},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\PDK3PR2J\\Cockayne et al. - 2017 - Probabilistic Numerical Methods for Partial Differ.pdf;C\:\\Users\\pub\\Zotero\\storage\\MIWRP8CG\\1605.html},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Methodology},
  primaryClass = {cs, math, stat},
  version = {3}
}

@inproceedings{damianou_deep_2013,
  title = {Deep {{Gaussian Processes}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Damianou, Andreas and Lawrence, Neil},
  date = {2013-04-29},
  pages = {207--215},
  url = {http://proceedings.mlr.press/v31/damianou13a.html},
  urldate = {2018-10-02},
  abstract = {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inpu...},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\BUXWE2UV\\Damianou and Lawrence - 2012 - Deep Gaussian Processes.pdf;C\:\\Users\\pub\\Zotero\\storage\\WKSUAVEY\\Damianou und Lawrence - 2013 - Deep Gaussian Processes.pdf;C\:\\Users\\pub\\Zotero\\storage\\K6M9IPNY\\damianou13a.html;C\:\\Users\\pub\\Zotero\\storage\\S2KB72DK\\1211.html},
  keywords = {60G15; 58E30,Computer Science - Learning,G.1.2,G.3,I.2.6,Mathematics - Probability,Statistics - Machine Learning},
  langid = {english}
}

@thesis{damianou_deep_2015,
  title = {Deep {{Gaussian}} Processes and Variational Propagation of Uncertainty},
  author = {Damianou, Andreas},
  date = {2015},
  institution = {{University of Sheffield}},
  url = {http://etheses.whiterose.ac.uk/id/eprint/9968},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\JEWANA87\\Damianou - 2015 - Deep Gaussian processes and variational propagatio.pdf;C\:\\Users\\pub\\Zotero\\storage\\DKUPAECG\\9968.html}
}

@thesis{deisenroth_efficient_2010,
  title = {Efficient {{Reinforcement Learning}} Using {{Gaussian Processes}}},
  author = {Deisenroth, Marc Peter},
  date = {2010},
  institution = {{KIT Scientific Publishing}},
  url = {http://www.cs.washington.edu/research/projects/aiweb/media/papers/tmppqidj5},
  urldate = {2016-04-19},
  file = {C\:\\Users\\pub\\Zotero\\storage\\K5TE3E38\\deisenroth.pdf;C\:\\Users\\pub\\Zotero\\storage\\63UCNMSK\\books.html}
}

@inproceedings{deisenroth_pilco_2011,
  title = {{{PILCO}}: {{A}} Model-Based and Data-Efficient Approach to Policy Search},
  shorttitle = {{{PILCO}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on Machine Learning ({{ICML}}-11)},
  author = {Deisenroth, Marc and Rasmussen, Carl E.},
  date = {2011},
  pages = {465--472},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Deisenroth_323.pdf},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\YEDBHXGB\\Deisenroth and Rasmussen - 2011 - PILCO A model-based and data-efficient approach t.pdf}
}

@inproceedings{depeweg_decomposition_2018,
  title = {Decomposition of {{Uncertainty}} in {{Bayesian Deep Learning}} for {{Efficient}} and {{Risk}}-Sensitive {{Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Depeweg, Stefan and Hernandez-Lobato, Jose-Miguel and Doshi-Velez, Finale and Udluft, Steffen},
  date = {2018},
  pages = {1192--1201},
  file = {C\:\\Users\\pub\\Zotero\\storage\\IQV8Z9UK\\Depeweg et al. - Decomposition of Uncertainty in Bayesian Deep Lear.pdf;C\:\\Users\\pub\\Zotero\\storage\\TK4ZWLEU\\Depeweg et al. - Decomposition of Uncertainty in Bayesian Deep Lear.pdf;C\:\\Users\\pub\\Zotero\\storage\\FR49N2TD\\auD.html}
}

@article{depeweg_learning_2016,
  title = {Learning and {{Policy Search}} in {{Stochastic Dynamical Systems}} with {{Bayesian Neural Networks}}},
  author = {Depeweg, Stefan and Hernández-Lobato, José Miguel and Doshi-Velez, Finale and Udluft, Steffen},
  date = {2016-05-23},
  url = {http://arxiv.org/abs/1605.07127},
  urldate = {2019-02-19},
  abstract = {We present an algorithm for policy search in stochastic dynamical systems using model-based reinforcement learning. The system dynamics are described with Bayesian neural networks (BNNs) that include stochastic input variables. These input variables allow us to capture complex statistical patterns in the transition dynamics (e.g. multi-modality and heteroskedasticity), which are usually missed by alternative modeling approaches. After learning the dynamics, our BNNs are then fed into an algorithm that performs random roll-outs and uses stochastic optimization for policy learning. We train our BNNs by minimizing α-divergences with α = 0.5, which usually produces better results than other techniques such as variational Bayes. We illustrate the performance of our method by solving a challenging problem where model-based approaches usually fail and by obtaining promising results in real-world scenarios including the control of a gas turbine and an industrial benchmark.},
  archivePrefix = {arXiv},
  eprint = {1605.07127},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\PDDB5NVV\\Depeweg et al. - 2016 - Learning and Policy Search in Stochastic Dynamical.pdf;C\:\\Users\\pub\\Zotero\\storage\\X5C6XVZT\\Depeweg et al. - 2016 - Learning and Policy Search in Stochastic Dynamical.pdf;C\:\\Users\\pub\\Zotero\\storage\\MWFZ45PK\\1605.html},
  keywords = {Computer Science - Learning,Computer Science - Machine Learning,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@book{gauss_theoria_1809,
  title = {Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientium},
  author = {Gauss, Carl Friedrich},
  date = {1809},
  publisher = {{sumtibus Frid. Perthes et IH Besser}},
  url = {https://books.google.de/books?id=VKhu8yPcat8C},
  urldate = {2016-06-06},
  file = {C\:\\Users\\pub\\Zotero\\storage\\NEPIE6G8\\books.html}
}

@inproceedings{hans_efficient_2009,
  title = {Efficient Uncertainty Propagation for Reinforcement Learning with Limited Data},
  booktitle = {International {{Conference}} on {{Artificial Neural Networks}}},
  author = {Hans, Alexander and Udluft, Steffen},
  date = {2009},
  pages = {70--79},
  publisher = {{Springer}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\28GBNUPD\\Hans und Udluft - 2009 - Efficient uncertainty propagation for reinforcemen.pdf;C\:\\Users\\pub\\Zotero\\storage\\WGKEVCHC\\978-3-642-04274-4_8.html}
}

@inproceedings{hein_benchmark_2017,
  title = {A Benchmark Environment Motivated by Industrial Control Problems},
  booktitle = {2017 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  author = {Hein, Daniel and Depeweg, Stefan and Tokic, Michel and Udluft, Steffen and Hentschel, Alexander and Runkler, Thomas A. and Sterzing, Volkmar},
  date = {2017-11},
  pages = {1--8},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/SSCI.2017.8280935},
  url = {http://ieeexplore.ieee.org/document/8280935/},
  urldate = {2019-02-19},
  abstract = {In the research area of reinforcement learning (RL), frequently novel and promising methods are developed and introduced to the RL community. However, although many researchers are keen to apply their methods on real-world problems, implementing such methods in real industry environments often is a frustrating and tedious process. Generally, academic research groups have only limited access to real industrial data and applications. For this reason, new methods are usually developed, evaluated and compared by using artificial software benchmarks. On one hand, these benchmarks are designed to provide interpretable RL training scenarios and detailed insight into the learning process of the method on hand. On the other hand, they usually do not share much similarity with industrial real-world applications. For this reason we used our industry experience to design a benchmark which bridges the gap between freely available, documented, and motivated artificial benchmarks and properties of real industrial problems. The resulting industrial benchmark (IB) has been made publicly available to the RL community by publishing its Java and Python code, including an OpenAI Gym wrapper, on Github. In this paper we motivate and describe in detail the IB’s dynamics and identify prototypic experimental settings that capture common situations in real-world industry control problems.},
  eventtitle = {2017 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  file = {C\:\\Users\\pub\\Zotero\\storage\\P9Z3ASIR\\Hein et al. - 2017 - A benchmark environment motivated by industrial co.pdf;C\:\\Users\\pub\\Zotero\\storage\\QTA4H4W9\\Hein et al. - 2017 - A benchmark environment motivated by industrial co.pdf;C\:\\Users\\pub\\Zotero\\storage\\XMHQWZRL\\8280935.html},
  isbn = {978-1-5386-2726-6},
  keywords = {academic research groups,artificial software benchmarks,Automobiles,benchmark environment,Benchmark testing,frustrating process,Games,Helicopters,industrial control,Industrial control,industrial data,Industries,industry environments,industry experience,interpretable RL training scenarios,learning (artificial intelligence),learning process,motivated artificial benchmarks,public domain software,real-world applications,real-world industry control problems,reinforcement learning,research area,RL community,tedious process,Wind turbines},
  langid = {english}
}

@inproceedings{hensman_gaussian_2013,
  title = {Gaussian {{Processes}} for {{Big Data}}},
  booktitle = {Uncertainty in {{Artificial Intelligence}}},
  author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D.},
  date = {2013},
  pages = {282},
  publisher = {{Citeseer}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\EU8WZFR4\\Hensman et al. - 2013 - Gaussian Processes for Big Data.pdf;C\:\\Users\\pub\\Zotero\\storage\\XV3VH9PJ\\Hensman et al. - 2013 - Gaussian Processes for Big Data.pdf;C\:\\Users\\pub\\Zotero\\storage\\2JAR4BNM\\1309.html;C\:\\Users\\pub\\Zotero\\storage\\9GMK8QF5\\auD.html;C\:\\Users\\pub\\Zotero\\storage\\ISZ4Z86Q\\1309.html},
  keywords = {Computer Science - Learning,Statistics - Machine Learning}
}

@article{hensman_scalable_2015,
  title = {Scalable Variational {{Gaussian}} Process Classification},
  author = {Hensman, James and Matthews, Alexander G. de G. and Ghahramani, Zoubin},
  date = {2015},
  journaltitle = {Journal of Machine Learning Research},
  volume = {38},
  pages = {351--360},
  abstract = {Gaussian process classification is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, outperforming the state of the art on benchmark datasets. Importantly, the variational formulation can be exploited to allow classification in problems with millions of data points, as we demonstrate in experiments. Copyright 2015 by the authors.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\T4WFAQPK\\Hensman et al. - 2014 - Scalable Variational Gaussian Process Classificati.pdf;C\:\\Users\\pub\\Zotero\\storage\\5GEKF8R7\\1411.html;C\:\\Users\\pub\\Zotero\\storage\\FX4I5R8Q\\display.html},
  keywords = {Statistics - Machine Learning}
}

@article{hodge_survey_2004,
  title = {A Survey of Outlier Detection Methodologies},
  author = {Hodge, Victoria and Austin, Jim},
  date = {2004},
  journaltitle = {Artificial intelligence review},
  volume = {22},
  pages = {85--126},
  file = {C\:\\Users\\pub\\Zotero\\storage\\2KWBFKJ4\\Hodge und Austin - 2004 - A survey of outlier detection methodologies.pdf;C\:\\Users\\pub\\Zotero\\storage\\M35HYIRV\\BAIRE.0000045502.10941.html},
  number = {2}
}

@article{jacobs_adaptive_1991,
  title = {Adaptive Mixtures of Local Experts},
  author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  date = {1991},
  journaltitle = {Neural computation},
  volume = {3},
  pages = {79--87},
  file = {C\:\\Users\\pub\\Zotero\\storage\\T4BQHV93\\Jacobs et al. - 1991 - Adaptive mixtures of local experts.pdf;C\:\\Users\\pub\\Zotero\\storage\\APX3N2YA\\neco.1991.3.1.html;C\:\\Users\\pub\\Zotero\\storage\\WZAY9YTW\\auD.html},
  number = {1}
}

@incollection{kaiser_bayesian_2018,
  title = {Bayesian {{Alignments}} of {{Warped Multi}}-{{Output Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Kaiser, Markus and Otte, Clemens and Runkler, Thomas and Ek, Carl Henrik},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {6995--7004},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/7931-bayesian-alignments-of-warped-multi-output-gaussian-processes.pdf},
  urldate = {2019-01-23},
  file = {C\:\\Users\\pub\\Zotero\\storage\\MJQDUDFP\\Kaiser et al. - 2017 - Bayesian Alignments of Warped Multi-Output Gaussia.pdf;C\:\\Users\\pub\\Zotero\\storage\\N8FXLQBZ\\Kaiser et al. - 2018 - Bayesian Alignments of Warped Multi-Output Gaussia.pdf;C\:\\Users\\pub\\Zotero\\storage\\P98SK2MP\\Kaiser et al. - 2017 - Bayesian Alignments of Warped Multi-Output Gaussia.pdf;C\:\\Users\\pub\\Zotero\\storage\\5KMTSPFI\\7931-bayesian-alignments-of-warped-multi-output-gaussian-processes.html;C\:\\Users\\pub\\Zotero\\storage\\AHX4Q83I\\1710.html;C\:\\Users\\pub\\Zotero\\storage\\UKGW6CEX\\1710.html},
  keywords = {Computer Science - Learning,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{kingma_variational_2015,
  title = {Variational {{Dropout}} and the {{Local Reparameterization Trick}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Kingma, Diederik P and Salimans, Tim and Welling, Max},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  date = {2015},
  pages = {2575--2583},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick.pdf},
  urldate = {2018-09-12},
  file = {C\:\\Users\\pub\\Zotero\\storage\\89SIZL5F\\Kingma et al. - 2015 - Variational Dropout and the Local Reparameterizati.pdf;C\:\\Users\\pub\\Zotero\\storage\\VE5EGL5C\\5666-variational-dropout-and-the-local-reparameterization-trick.html}
}

@article{lazaro-gredilla_overlapping_2012,
  title = {Overlapping Mixtures of {{Gaussian}} Processes for the Data Association Problem},
  author = {Lázaro-Gredilla, Miguel and Van Vaerenbergh, Steven and Lawrence, Neil D.},
  date = {2012},
  journaltitle = {Pattern Recognition},
  volume = {45},
  pages = {1386--1395},
  file = {C\:\\Users\\pub\\Zotero\\storage\\9PWMT93W\\Lázaro-Gredilla et al. - 2011 - Overlapping Mixtures of Gaussian Processes for the.pdf;C\:\\Users\\pub\\Zotero\\storage\\GSDXF73X\\Lázaro-Gredilla et al. - 2012 - Overlapping mixtures of Gaussian processes for the.pdf;C\:\\Users\\pub\\Zotero\\storage\\HZYC3SEH\\S0031320311004109.html;C\:\\Users\\pub\\Zotero\\storage\\WXY7PLDG\\1108.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  number = {4}
}

@article{levine_reinforcement_2018,
  title = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}: {{Tutorial}} and {{Review}}},
  shorttitle = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}},
  author = {Levine, Sergey},
  date = {2018-05-20},
  url = {http://arxiv.org/abs/1805.00909},
  urldate = {2020-02-18},
  abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
  archivePrefix = {arXiv},
  eprint = {1805.00909},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\N8D2BMI9\\Levine - 2018 - Reinforcement Learning and Control as Probabilisti.pdf;C\:\\Users\\pub\\Zotero\\storage\\FN6PF6J5\\1805.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{maddison_concrete_2016,
  title = {The {{Concrete Distribution}}: {{A Continuous Relaxation}} of {{Discrete Random Variables}}},
  shorttitle = {The {{Concrete Distribution}}},
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  date = {2016-11-02},
  url = {http://arxiv.org/abs/1611.00712},
  urldate = {2018-09-12},
  abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
  archivePrefix = {arXiv},
  eprint = {1611.00712},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\4JCGSNBV\\Maddison et al. - 2016 - The Concrete Distribution A Continuous Relaxation.pdf;C\:\\Users\\pub\\Zotero\\storage\\M4WZ8CC4\\1611.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{matthews_gpflow_2017,
  title = {{{GPflow}}: {{A Gaussian}} Process Library Using {{TensorFlow}}},
  shorttitle = {{{GPflow}}},
  author = {Matthews, Alexander G. de G. and van der Wilk, Mark and Nickson, Tom and Fujii, Keisuke and Boukouvalas, Alexis and León-Villagrá, Pablo and Ghahramani, Zoubin and Hensman, James},
  date = {2017},
  journaltitle = {Journal of Machine Learning Research},
  volume = {18},
  pages = {1--6},
  url = {http://www.jmlr.org/papers/volume18/16-537/16-537.pdf},
  urldate = {2017-09-27},
  file = {C\:\\Users\\pub\\Zotero\\storage\\X6QGAFR8\\Matthews et al. - 2017 - GPflow A Gaussian process library using TensorFlo.pdf},
  number = {40},
  options = {useprefix=true}
}

@book{murphy_machine_2012,
  title = {Machine {{Learning}}: {{A Probabilistic Perspective}}},
  shorttitle = {Machine {{Learning}}},
  author = {Murphy, Kevin P.},
  date = {2012-08-24},
  publisher = {{MIT Press}},
  abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package -- PMTK (probabilistic modeling toolkit) -- that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\FERQXL3E\\17.pdf},
  isbn = {978-0-262-01802-9},
  keywords = {Computers / Machine Theory,Textbook},
  langid = {english},
  pagetotal = {1098}
}

@article{oates_modern_2019,
  title = {A {{Modern Retrospective}} on {{Probabilistic Numerics}}},
  author = {Oates, C. J. and Sullivan, T. J.},
  date = {2019-01-14},
  url = {http://arxiv.org/abs/1901.04457},
  urldate = {2019-10-02},
  abstract = {This article attempts to place the emergence of probabilistic numerics as a mathematical-statistical research field within its historical context and to explore how its gradual development can be related both to applications and to a modern formal treatment. We highlight in particular the parallel contributions of Sul'din and Larkin in the 1960s and how their pioneering early ideas have reached a degree of maturity in the intervening period, mediated by paradigms such as average-case analysis and information-based complexity. We provide a subjective assessment of the state of research in probabilistic numerics and highlight some difficulties to be addressed by future works.},
  archivePrefix = {arXiv},
  eprint = {1901.04457},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\D67KFS6E\\Oates and Sullivan - 2019 - A Modern Retrospective on Probabilistic Numerics.pdf;C\:\\Users\\pub\\Zotero\\storage\\HUTII4YQ\\1901.html},
  keywords = {62-03; 65-03; 01A60; 01A65; 01A67,Mathematics - History and Overview,Mathematics - Numerical Analysis,Mathematics - Probability,Statistics - Machine Learning},
  primaryClass = {math, stat}
}

@article{petersen_matrix_2008,
  title = {The Matrix Cookbook},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind and others},
  date = {2008},
  journaltitle = {Technical University of Denmark},
  volume = {7},
  pages = {15},
  url = {http://www.cim.mcgill.ca/~dudek/417/Papers/matrixOperations.pdf},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\N4W86FFD\\imm3274.pdf}
}

@book{rasmussen_gaussian_2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward},
  date = {2006},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.86.3414},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\TFXHMXDE\\summary.html}
}

@inproceedings{rasmussen_infinite_2002,
  title = {Infinite {{Mixtures}} of {{Gaussian Process Experts}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 14},
  author = {Rasmussen, Carl E. and Ghahramani, Zoubin},
  editor = {Dietterich, T. G. and Becker, S. and Ghahramani, Z.},
  date = {2002},
  pages = {881--888},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/2055-infinite-mixtures-of-gaussian-process-experts.pdf},
  urldate = {2018-08-23},
  file = {C\:\\Users\\pub\\Zotero\\storage\\VHKBGGWV\\Rasmussen und Ghahramani - 2002 - Infinite Mixtures of Gaussian Process Experts.pdf;C\:\\Users\\pub\\Zotero\\storage\\3YP3XGSE\\2055-infinite-mixtures-of-gaussian-process-experts.html}
}

@article{rezende_stochastic_2014,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  date = {2014-01-16},
  url = {https://arxiv.org/abs/1401.4082},
  urldate = {2018-09-12},
  file = {C\:\\Users\\pub\\Zotero\\storage\\N244K3J2\\Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf;C\:\\Users\\pub\\Zotero\\storage\\LEJMJNS8\\1401.html},
  langid = {english}
}

@book{rousseeuw_robust_2005,
  title = {Robust Regression and Outlier Detection},
  author = {Rousseeuw, Peter J. and Leroy, Annick M.},
  date = {2005},
  volume = {589},
  publisher = {{John wiley \& sons}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\X4BEMJQI\\books.html}
}

@inproceedings{salimbeni_doubly_2017,
  title = {Doubly {{Stochastic Variational Inference}} for {{Deep Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Salimbeni, Hugh and Deisenroth, Marc},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  pages = {4588--4599},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.pdf},
  urldate = {2018-10-02},
  file = {C\:\\Users\\pub\\Zotero\\storage\\FTCRG5BC\\Salimbeni und Deisenroth - 2017 - Doubly Stochastic Variational Inference for Deep G.pdf;C\:\\Users\\pub\\Zotero\\storage\\TX47KKYE\\Salimbeni und Deisenroth - 2017 - Doubly Stochastic Variational Inference for Deep G.pdf;C\:\\Users\\pub\\Zotero\\storage\\AP6UXDGD\\1705.html;C\:\\Users\\pub\\Zotero\\storage\\FNPVJ7AA\\7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.html;C\:\\Users\\pub\\Zotero\\storage\\GVUNMT8K\\display.html},
  keywords = {Statistics - Machine Learning}
}

@book{scholkopf_learning_2002,
  title = {Learning with {{Kernels}}: {{Support Vector Machines}}, {{Regularization}}, {{Optimization}}, and {{Beyond}}},
  shorttitle = {Learning with {{Kernels}}},
  author = {Schölkopf, Bernhard and Smola, Alexander J.},
  date = {2002-01},
  publisher = {{MIT Press}},
  abstract = {In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -- -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.},
  isbn = {978-0-262-19475-4},
  keywords = {Computers / Intelligence (AI) & Semantics,Computers / Programming / General},
  langid = {english},
  pagetotal = {658}
}

@article{silver_mastering_2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  date = {2017-10},
  journaltitle = {Nature},
  volume = {550},
  pages = {354--359},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  url = {https://www.nature.com/articles/nature24270},
  urldate = {2019-04-04},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\9VNUJ3GJ\\Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf;C\:\\Users\\pub\\Zotero\\storage\\DLDP8I3H\\nature24270.html;C\:\\Users\\pub\\Zotero\\storage\\RE8QUYV6\\nature24270.html;C\:\\Users\\pub\\Zotero\\storage\\SBYNLYP2\\display.html;C\:\\Users\\pub\\Zotero\\storage\\ZW2KG8K4\\nature24270.html},
  langid = {english},
  number = {7676},
  options = {useprefix=true}
}

@thesis{snelson_flexible_2007,
  title = {Flexible and Efficient {{Gaussian}} Process Models for Machine Learning},
  author = {Snelson, Edward Lloyd},
  date = {2007},
  institution = {{Citeseer}},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.4041},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\T34X6F3F\\Snelson - 2007 - Flexible and efficient Gaussian process models for.pdf}
}

@inproceedings{snelson_sparse_2005,
  title = {Sparse {{Gaussian}} Processes Using Pseudo-Inputs},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Snelson, Edward and Ghahramani, Zoubin},
  date = {2005},
  pages = {1257--1264},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2005_543.pdf},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\6U2PZBFH\\Snelson and Ghahramani - 2005 - Sparse Gaussian processes using pseudo-inputs.pdf}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018},
  edition = {2},
  publisher = {{MIT Press}},
  location = {{Cambridge, Mass}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\6EQF4BV3\\Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf},
  isbn = {978-0-262-19398-6},
  keywords = {Reinforcement learning},
  langid = {english},
  pagetotal = {322},
  series = {Adaptive Computation and Machine Learning}
}

@article{tensorflow2015-whitepaper,
  title = {{{TensorFlow}}: {{Large}}-{{Scale Machine Learning}} on {{Heterogeneous Systems}}},
  author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  date = {2015},
  url = {https://www.tensorflow.org/},
  file = {C\:\\Users\\pub\\Zotero\\storage\\WLKK3HI7\\Abadi et al. - 2015 - TensorFlow Large-Scale Machine Learning on Hetero.pdf},
  note = {Software available from tensorflow.org}
}

@inproceedings{titsias_variational_2009,
  title = {Variational {{Learning}} of {{Inducing Variables}} in {{Sparse Gaussian Processes}}.},
  booktitle = {{{AISTATS}}},
  author = {Titsias, Michalis K.},
  date = {2009},
  volume = {5},
  pages = {567--574},
  url = {http://www.jmlr.org/proceedings/papers/v5/titsias09a/titsias09a.pdf},
  urldate = {2017-04-06},
  file = {C\:\\Users\\pub\\Zotero\\storage\\UTMCPPXS\\Titsias - 2009 - Variational Learning of Inducing Variables in Spar.pdf}
}

@inproceedings{tresp_mixtures_2001,
  title = {Mixtures of {{Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 13},
  author = {Tresp, Volker},
  editor = {Leen, T. K. and Dietterich, T. G. and Tresp, V.},
  date = {2001},
  pages = {654--660},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/1900-mixtures-of-gaussian-processes.pdf},
  urldate = {2018-09-26},
  file = {C\:\\Users\\pub\\Zotero\\storage\\C7SSU5TS\\Tresp - 2001 - Mixtures of Gaussian Processes.pdf;C\:\\Users\\pub\\Zotero\\storage\\7GPFZX3C\\1900-mixtures-of-gaussian-processes.html}
}

@article{tresp_wet_1994,
  title = {The Wet Game of Chicken},
  author = {Tresp, Volker},
  date = {1994},
  journaltitle = {Siemens AG, CT IC 4, Technical Report}
}

@article{ziebart_modeling_2010,
  title = {Modeling {{Interaction}} via the {{Principle}} of {{Maximum Causal Entropy}}},
  author = {Ziebart, Brian D and Bagnell, J Andrew and Dey, Anind K},
  date = {2010},
  pages = {8},
  abstract = {The principle of maximum entropy provides a powerful framework for statistical models of joint, conditional, and marginal distributions. However, there are many important distributions with elements of interaction and feedback where its applicability has not been established. This work presents the principle of maximum causal entropy—an approach based on causally conditioned probabilities that can appropriately model the availability and influence of sequentially revealed side information. Using this principle, we derive models for sequential data with revealed information, interaction, and feedback, and demonstrate their applicability for statistically framing inverse optimal control and decision prediction tasks.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\KJJBGJPC\\Ziebart et al. - Modeling Interaction via the Principle of Maximum .pdf},
  langid = {english}
}


