\chapter{Bayesian Machine Learning}
\label{toc:bayesian_ml}

\begin{itemize}
    \item Machine Learning is an extension of algorithmics (and numerics) with different goals
    \item In algorithmics the goal is usually clear and well-defined
    \item Use polynomial interpolation as an example.
          We can use that for curve-fitting, but we generally do not associate it with machine learning because the goal is not to minimize some loss function but rather to find a uniquely defined solution.
          The "ML-variant" of polynomial interpolation would be generalized linear models with polynomial basis functions and there, the correct answer is much less clear. (PCA, "least squares" quadratic fit, etc)
    \item In ML, an essential element is a lack of information and our freedom to handle that lack of information. A subjective \emph{goal} is integral to machine learning. ML is about making the best of the situation and is inherently subjective. And inherently uncertain.
    \item The lack of information is generally introduced via finite information for an infinite object
    \item We can formulate this situation nicely in terms of probability distributions
          \begin{itemize}
              \item A ML problem is an inference problem
              \item We always care about some kind of joint distribution
              \item Depending on which structure we assume, different branches of ML arise
              \item The two main strategies to formulate finiteness (parameters and data) give rise to the (non-)parametric split
          \end{itemize}
    \item Things like learning theory try for objectivity but that does not scale
    \item In practice, ML is about the step from risk minimization to empirical risk minimization and ways to handle the annoying results
    \item We get back to global arguments via encoding knowledge in
          \begin{itemize}
              \item information operator
              \item hypothesis space
              \item loss function
              \item optimization algorithm
          \end{itemize}
    \item Turns out the problem statement and hypothesis space (same thing?) are generally preferred
    \item Bayesian ML then offers a nice language to formulate such hypotheses
    \item Maybe insert the core probabilistic numerics argument about algorithms here?
    \item Most importantly, Bayes' rule offers a consistent and simple way to combine local and global assumptions that does not break probability theory.
          And has a nice semantic interpretation.
          It is not a unique solution though.
    \item With Bayes' rule, a structurally simple \emph{learning algorithm} can be formulated where the complexity is in the hypothesis space (prior) and loss function (likelihood). No optimization needed at first glance.
    \item Turns out this is a nice fit for problems in the physical word which has a few notable properties:
          \begin{itemize}
              \item Inevitability of uncertainty
              \item Availability of knowledge
              \item Need for interpretability
          \end{itemize}
    \item Discuss the differences between Web ML and Industrial ML
    \item Industrial ML motivates our thesis, where we want to formulate models that are inherently interpretable and trustworthy
    \item It will turn out that in this space, judging whether a model is good is actually very tricky.
\end{itemize}

\todo[inline]{Industrial Applications of ML as a start?}

Machine learning methods have seen great success recently in a wide range of digital domains such as speech recognition, computer vision or translation.
However, bridging the gap to applications in the physical world has proved challenging.
Problem domains like robotics, industrial control, decision support systems or the natural sciences introduce a new set of requirements.
ML-systems which operate in safety-critical areas, interact with people or carry responsibility must be robust, trustworthy and assessable.
Besides optimizing for good average-case performance, a responsible system needs to reason about plausible worst-cases and reliably avoid or inform about them.

In this workshop, we will identify a list of properties required in real-world scenarios and discuss how they can be evaluated.
We aim to address challenges arising from the need for interaction of machine learning researchers and domain experts in successful applications.
How do we design models which facilitate communication with experts and how do we evaluate interpretability?
How can we benchmark the trustworthiness and robustness of a model and ensure that it can cope with unobserved situations?
What do uncertainties mean in practice and how do we make use of them to reason about worst-case performance?

We want to bring together computer scientists, mathematicians and statisticians with domain experts to discuss approaches that facilitate interdisciplinary dialogue.
We welcome contributions on explainable and interpretable models, human-in-the-loop learning, uncertainty quantification and learning guarantees.
We hope to introduce open problems to a broader community and discuss benchmarks and success criteria for requirements driven by machine learning problems in the physical world.

\section{Machine learning problems}
\label{toc:bayesian_ml:ml_problems}
One of the roots of machine learning (ML) lies in the study of algorithms in theoretical computer science.
An algorithm is a well-defined sequence of computational steps transforming a set of inputs to a set of outputs.
It is a tool for solving a computational problem, which is defined by an abstract problem of admissable inputs and expected outputs.
An algorithm solves such a problem if for every possible input, the algorithm provably yields the correct output.

Consider the computational problem of sorting a list of numbers in ascending order.
One possible problem~\parencite{cormen_introduction_2009} of the sorting problem is:
\begin{problem}[Sorting]
\label{prob:bayesian_ml:sorting}
\begin{labeling}{Output:}
    \item[Input:] A sequence of $N$ integers $\mat{I} = (i_1, \dots, i_N)$
    \item[Output:] A reordering of $\mat{I}$ called $\mat{O} = (o_1, \dots, o_N)$ such that $o_1 \leq \dots \leq o_N$.
\end{labeling}
\end{problem}
For example, for the input $\mat{\hat{I}}=(12, 8, 23, 4)$ the correct output is $\mat{\hat{O}}=(4, 8, 12, 23)$.
A concrete input $\mat{\hat{I}}$ is called an instance of a problem.
Importantly, such an instance contains all the required information to compute the unique output $\mat{\hat{O}}$.
The correctness of the output can be checked via the formal problem problem.
The various available (correct) sorting algorithms only differ in which and how many computational steps they take to arrive at the output, not in the output itself.

Machine learning can be seen as an extension of algorithmics towards problems where a formal description of a uniquely defined solution does not exist.
Instead, problems in ML are characterized by the observation of finitely many examples or data together with the objective to derive knowledge about how these examples were generated.
This knowledge can then be used to describe common patterns in the data or generate new examples that conform to previous observations.

\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_data}
        \caption{
            Data
            \label{fig:bayesian_ml:polynoms:data}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_lagrange}
        \caption{
            \enquote{Algorithms}
            \label{fig:bayesian_ml:polynoms:lagrange}
        }
    \end{subfigure}
    \caption[Algorithmic Interpolation]{
        ???
        \label{fig:bayesian_ml:polynoms}
    }
\end{figure}

As an example, consider the data shown in \cref{fig:bayesian_ml:polynoms:data}, a set of $N$ pairs of real numbers for which we assume that all $x_i$ are pairwise different.
The knowledge about this data to be uncovered is the functional dependency between the two data dimensions that was used to generate the data.
This is called a regression problem and is strongly under-specified:
There exist uncountably many functions on the real numbers that explain any finite set of observed points.
The problem
\begin{problem}[Regression]
\label{prob:bayesian_ml:regression}
\begin{labeling}{Output:}
    \item[Input:] A set of $N$ pairs of real numbers $\Dc = \Set*{\left(x_n, y_n\right)}_{n=1}^N \subseteq \Xc \times \Yc$
    \item[Output:] The functional dependency $f : \Xc \to \Yc$ used to generate the data.
\end{labeling}
\end{problem}
is therefore not a computational problem and asking for a solution or algorithm for this problem is not a well-posed question.

One can however derive computational problems from the regression problem by making assumptions about nature of the function $f$ that characterize a unique solution.
For example, one could ask for the simplest polynomial that explains the data.
\begin{problem}[Lagrange Polynomial]
\label{prob:bayesian_ml:lagrange}
\begin{labeling}{Output:}
    \item[Input:] A set of $N$ pairs of real numbers $\Dc = \Set*{\left(x_n, y_n\right)}_{n=1}^N \subseteq \Xc \times \Yc$
    \item[Output:] The polynomial of smallest degree for which $f(x_n) = y_n$ holds for all $(x_n, y_n) \in \Dc$.
\end{labeling}
\end{problem}
It can be shown that this is indeed a well-posed computational problem whose unique solution is the Lagrange polynomial $L$~\parencite{waring_vii._1779} given by the explicit form
\begin{align}
    \begin{split}
        L(x) &= \sum_{i=0}^N y_i \ell_i(x)\text{, with} \\
        \ell_i &= \prod_{\substack{j = 0\\j \neq i}}^N \frac{x - x_j}{x_i - x_j}.
    \end{split}
    \label{eq:bayesian_ml:lagrange}
\end{align}
\Cref{fig:bayesian_ml:polynoms:lagrange} shows the Lagrange polynomial interpolating the example dataset.
The figure also shows another derived computational problem of linear interpolation.
Here, the function is defined as being piecewise linear between the different data points, thus also reproducing the data.

Both of these problems and derived algorithms would typically not be identified as ML approaches as it can be argued that they do not derive new insights from data.
Because of the explicit nature of their algorithms as seen in \cref{eq:bayesian_ml:lagrange}, they much more closely resemble classical algorithms such as sorting algorithms.

\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_linear}
        \caption{
            Linear ML
            \label{fig:bayesian_ml:polynoms:linear}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_polynomial}
        \caption{
            Polynomial ML
            \label{fig:bayesian_ml:polynoms:polynomial}
        }
    \end{subfigure}
    \caption[ML Interpolation]{
        ???
        \label{fig:bayesian_ml:polynoms:ml}
    }
\end{figure}
A common approach to formulating a problem that lets the data speak for itself is to formulate more implicit requirements.
Instead of describing exactly one possible function (such as the Lagrange Polynomial), one can choose a broader set of candidate functions or hypotheses $\Hc$ and then select one of the candidates $f \in \Hc$ that is optimal with respect to some measure of performance.
For example, a common assumption about the process used to generate the data is that it separates into two additive components
\begin{align}
    y_n = f(x_n) + \epsilon(x_n).
\end{align}
The first summand $f$ captures the truly informative functional dependency between $x$ and $y$ that applies for all observations while the second term $\epsilon$ captures local error or noise that can be ignored.

A direct consequence of this assumption is that output of the regression problem $f$ need no longer interpolate the data perfectly.
However, this introduces the need for additional assumptions about how far from the data $f$ is allowed to be or, equivalently, about the shape of $\epsilon$.
One can choose $f$ to be a linear function, giving rise to the linear regression problem.
\begin{problem}[Linear regression]
\begin{labeling}{Output:}
    \item[Input:] A set of $N$ pairs of real numbers $\Dc = \Set*{\left(x_n, y_n\right)}_{n=1}^N \subseteq \Xc \times \Yc$ and an error function $e : \Hc \times \Xc \times \Yc \to \Rb$
    \item[Output:] A function $f$ such that
    \begin{align}
        f \in \argmin_{f \in \Hc} e(f, \Dc)
    \end{align}
    with $\Hc$ being the set of linear functions.
\end{labeling}
\end{problem}
\Cref{fig:bayesian_ml:polynoms:linear} shows the different results of three algorithms with different choices of error functions $e$.
With $f(x) = \mat{W}x + b$ those are
\begin{labeling}{Absolute error:}
    \item[Squared error:] $e(f, \Dc) = \sum_{n=1}^N (y_n - f(x_n))^2$
    \item[Absolute error:] $e(f, \Dc) = \sum_{n=1}^N \abs{y_n - f(x_n)}$
    \item[Ridge:] $e(f, \Dc) = \sum_{n=1}^N (y_n - f(x_n))^2 - \norm{\mat{W}}_2 - b^2$.
\end{labeling}
Altering the set of hypotheses $\Hc$ between linear, quadratic and cubic polynomials together with the squared error function results in the functions shown in \cref{fig:bayesian_ml:polynoms:polynomial}.

It is important to note that none of these proposed algorithms and plotted functions is \emph{objectively} the correct solution to the regression problem.
None of them is equal to the function that was used to generate the data and even if it was, there would be no way to tell.
A core property of machine learning problems is that what characterizes the correct solution is an inherently \emph{subjective} question.
This subjectiveness is represented in multiple choices:
\begin{enumerate}
    \item The assumed structure underlying the data.
    \item The space of hypotheses for valid solutions.
    \item The algorithm used to select from these hypotheses.
\end{enumerate}
In the following, statistical learning and Bayesian machine learning are introduced as tools to formalize these choices and establish a mathematical framework.

\section{Statistical learning}
\begin{figure}[t]
    \centering
    \includestandalonewithpath{figures/quantities_of_interest_statistical_learning}
    \caption{
        Statistical Learning
        \label{fig:bayesian_ml:statistical_learning}
    }
\end{figure}
The objective of machine learning is to derive knowledge about a process generating data from a limited set of observations.
Knowledge is represented as a model that both explains existing observations and is able to generalize to new data points.
Statistical learning theory offers a set of tools to reason about generalization and to formulate what it means for a model to be good.
This section presents an interpretation of statistical learning theory similar to the definition of probabilistic numerics in~\parencite{oates_modern_2019,cockayne_bayesian_2019}.

In the following, we denote the space of probability measures over a set $\Zc$ as $\Probs{\Zc}$.
We adopt an overloaded notation common in machine learning where $\Prob*{\mat{z}}$ can both refer to a probability measure and the evaluation of the same probability measure on a specific point $\mat{z} \in \Zc$ depending on the context.
Similarly, $\mat{z}$ can both refer to a random variable with the distribution $\Prob*{\mat{z}}$ and an element $\mat{z} \in \Zc$.
That is, $\mat{k} \sim \Prob*{\mat{z}}$ denotes that the distribution of the random variable $\mat{k}$ is $\Prob*{\mat{z}}$ and $\Moment*{\E}{\mat{z}} = \int \mat{z} \Prob*{\mat{z}} \diff \mat{z}$ denotes the expected value of the random variable $\mat{z}$ under the distribution $\Prob*{\mat{z}}$.
In less ambiguous notation one would assume a random variable $\mat{Z}$ with distribution $\Prob*{\mat{Z}}$ and denote the expected value as $\Moment*{\E}{\mat{Z}} = \int_{\Zc} z \Prob*{\mat{Z} = z} \diff z$.
Here, $\mat{Z}$ and $z$ are identified with each other as $\mat{z}$.

Assume an unknown data-generating distribution $\Prob{\mat{z}} \in \Probs{\Zc}$ over a known space of observations $\Zc$.
The task in a machine learning problem is to infer properties of this distribution given only a set of observations $\Dc \in \Sc$ obtained via some sampling operator $S : \Probs{\Zc} \to \Sc$.
A common choice of $S$ is a draw of $N$ independent points $\Dc = \Set*{\mat{z_1}, \dots, \mat{z_N} \with \mat{z_i} \sim \Prob{\mat{z}}} \subseteq \Zc$ with $\Sc = \Zc^N$.
Other choices include $S$ into the learning problem such as in active learning~\parencite{murphy_machine_2012} or reinforcement learning~\parencite{sutton_reinforcement_2018} settings.
Depending on the machine learning problem, the task might not be to identify $\Prob*{\mat{z}}$ as a whole but instead identify some functional $f \in \Fc$ obtained via the functional operator $F : \Probs{\Zc} \to \Fc$.
This formulation encompasses a large number of different machine learning problems via different choices for the spaces and operators.
We give a few examples here.

\begin{problem}[Common machine learning problems]
\begin{description}
    \item[Parameter estimation]
          In a simple case, the functional $f$ can be chosen to be some constant statistic of $\Prob*{\mat{z}}$ such as the mean
          \begin{align}
              F_{\text{mean}}[\Prob*{\mat{z}}] = \int \mat{z} \Prob*{\mat{z}} \diff \mat{z},
          \end{align}
          with $\Fc = \Zc$.
    \item[Regression]
          Alternatively, assuming that $\Zc = \Xc \times \Yc$ separates into tuples of inputs $\Xc$ and continuous outputs $\Yc$ and choosing
          \begin{align}
              F_{\text{reg}}[\Prob*{\mat{x}, \mat{y}}](\mat{x_\ast}) = \Prob*{\mat{y_\ast} \given \mat{x_\ast}}
          \end{align}
          with $\Fc = \Xc \to \Probs{\Yc}$ gives rise to the regression problem discussed in~\cref{toc:bayesian_ml:ml_problems}.
    \item[Classification]
          Classification is a closely related to regression and assumes the same separation $\Zc = \Xc \times \Yc$.
          In contrast to regression, $\Yc$ is assumed to be discrete, often without a known order.
    \item[Dimensionality Reduction]
          In the dimensionality reduction or manifold learning problem, the assumption is that the support of the data distribution $\Prob*{\mat{z}}$ is a low dimensional manifold in $\Zc$.
          The task is to find a distribution $\Prob*{\mat{m}} \in \Probs{\Mc}$ and a mapping $f : \Mc \to \Zc$ with
          \begin{align}
              F_{\text{dim}}[\Prob*{\mat{z}}] & = (\Prob*{\mat{m}}, f)\text{, such that} \\
              f(\Prob*{\mat{m}})              & = \Prob*{\mat{z}}
          \end{align}
          and $\Fc = \Probs{\Mc} \times (\Mc \to \Zc)$ and a space $\Mc$ of lower dimensionality than $\Zc$.
          If $\Mc = \Zc$ this problem is called density estimation.
\end{description}
\end{problem}

A machine learning Algorithm $A : \Sc \to \Fc$ is a function mapping from the space of samples to the space of functionals, thereby recovering structure from data.
Such an algorithm is successful if it recovers the correct functional $f \in F$ given observations $\Dc$.
\Cref{fig:bayesian_ml:statistical_learning} shows the introduced components in a directed diagram.
An algorithm is successful if this diagram (approximately) commutes such that $A \circ D \simeq F$.
Similarly, a model $f_A \in \Fc$ can be considered good if it is similar to the true functional~$f$.

The similarity of two functionals can be measured via a distance measure in $\Fc$.
In the case of learning about global properties such as the mean of $\Prob*{\mat{z}}$, a possible choice is the standard euclidean norm.
Considering the regression case, where $\Fc$ is a function space mapping inputs to (possibly distributions over) outputs, the effects of choosing a distance measure is more subtle.
While it is reasonable to require pointwise similarity, the choice of which points to evaluate allows us to define what we mean by generalization:
Besides explaining the observations in $\Dc$, a good model should yield correct predictions for the complete data distribution $\Prob*{\mat{z}} = \Prob*{\mat{x}, \mat{y}}$.
The concept of risk minimization is based on this observation.
\begin{definition}[Risk minimization in the regression problem]
    \label{def:risk_minimization}
    Given a loss function $\ell: \Yc \times \Yc \to \Rb$ and a data distribution $\Prob*{\mat{x}, \mat{y}}$ for a regression problem, the \emph{risk} relative to $\ell$ is defined as
    \begin{align}
        \risk_\ell & : \left\{
        \begin{aligned}
            \Fc & \to \Rb                                                                                              \\
            f_A & \mapsto \int \Fun*{\ell}{\mat{y}, f_A(\mat{x})} \Prob{\mat{x}, \mat{y}} \diff \mat{x} \diff \mat{y}.
        \end{aligned}
        \right.
    \end{align}
    \emph{Risk minimization} for a hypothesis space $\Hc \subseteq \Fc$ selects a hypothesis $\hat{f}_A$ with smallest possible risk
    \begin{align}
        \hat{f}_A & \in \argmin_{f_A \in \Hc} \risk_\ell(f_A).
    \end{align}
\end{definition}

A model generalizes with respect to a loss function $\ell$ if it does not only explain the specific dataset $\Dc$ well but all possible choices of $\Dc$ via $S$.
While risk minimization is a theoretical tool to define generalization, it does not immediately yield a learning algorithm.
Since it includes an expectation over the unknown data distribution, the risk-term cannot be evaluated directly.
Instead, calculating a Monte-Carlo estimate over the training data $\Dc$ gives rise to a fundamental machine learning algorithm, empirical risk minimization.

\begin{definition}[Empirical risk minimization in the regression problem]
    \label{def:empirical_risk_minimization}
    Given a loss function $\ell: \Yc \times \Yc \to \Rb$ and a data distribution $\Prob*{\mat{x}, \mat{y}}$ for a regression problem, the \emph{empirical risk} relative to $\ell$ is defined as
    \begin{align}
        \risk^{\text{emp}}_\ell & : \left\{
        \begin{aligned}
            \Fc & \to \Rb                                                             \\
            f_A & \mapsto \frac{1}{N} \sum_{i=1}^N \Fun*{\ell}{\mat{y}, f_A(\mat{z})}
        \end{aligned}
        \right.
    \end{align}
    \emph{Empirical risk minimization} for a hypothesis space $\Hc \subseteq \Fc$ selects a hypothesis $\hat{f}_A$ with smallest possible risk
    \begin{align}
        \hat{f}_A & \in \argmin_{f_A \in \Hc} \risk^{\text{emp}}_\ell(f_A).
    \end{align}
\end{definition}

Intuitively, risk minimization describes global properties of $f_A$ which get approximated by a number of local properties at the observations in empirical risk minimization.
The two questions
\begin{enumerate}
    \item under which conditions $\risk^{\text{emp}}$ converges to $\risk$ for $N \to \infty$ and
    \item if so, what the convergence rates are
\end{enumerate}
underpin (statistical) learning theory~\parencite{vapnik_principles_1992}.
It is safe to assume that for small $N$, $\risk^{\text{emp}}$ can significantly underestimate the true risk as the error on parts of the data distribution is not considered at all.
This problem is called overfitting to the available training data, an example of which can arguably be seen in~\cref{fig:bayesian_ml:polynoms:lagrange}.

In practice, if collection sufficient data is not possible, overfitting has to be avoided via problem-dependent choices for $\ell$ and $\Hc$.
Additionally, the empirical risk minimization algorithm is often extended to regularizing loss functions of the form $\ell^\prime : \Fc \times \Yc \times \Yc \to \Rb$ which depend on the structure of the candidate as well as its predictions.
The Ridge error function shown in~\cref{fig:bayesian_ml:polynoms:linear} is an example of extending the least squares error function with a preference for parameters with small absolute value.
Regularization~\parencite{oates_modern_2019} adds back a global component to empirical risk minimization and has a close relation to the choice of hypothesis space $\Hc$.
The constraint that $f_A \in \Hc \subseteq \Fc$ can be thought of as a binary regularization term which adds infinite loss to the set $\Fc \setminus \Hc$.
Conversely, continuous regularization terms formulate softer and less rigorous preferences within $\Hc$, for example for structurally simpler solutions~\parencite{bishop_christoph_pattern_2007,thorburn_occams_1915}.

For complex loss functions and hypothesis spaces, finding the true minimum $\hat{f}_A$ is often unfeasible.
Another common extension is to modify the optimization scheme to increase the likelihood of selecting a favorable solution.
Examples include the usage of test sets or validations sets~\parencite{bishop_christoph_pattern_2007}, cross validation~\parencite{stone_cross-validatory_1974}, early stopping~\parencite{morgan_generalization_1990} or specific parameter choices~\parencite{daniely_toward_2016}.

It is often possible to reformulate choices in loss functions as changes in the optimization scheme or compensate a more general hypothesis space with stricter regularization.
It is not clear how a machine learning algorithm should be formulated form a formal perspective.
However, problem-dependent adjustments to learning algorithms are generally informed by knowledge provided by domain-experts about the process that generated the data.
An additional dimension is therefore given by the need to communicate assumptions and effects of choices with stakeholders which do not have a deep understanding of the field.
If a learning algorithm should be interpretable and understandable, assumptions should be explicit and optimization schemes should be simple.
The next section introduces Bayesian machine learning as a rigorous formal framework that builds on statistical learning theory and enables the principled formulation of complex hypothesis spaces.

\section{Bayesian machine learning}
The next section introduces Bayesian machine learning as a rigorous formal framework that builds on statistical learning theory and enables the principled formulation of complex hypothesis spaces.
Blargh, \tikz[x=2.5em,baseline=(a.base)]{
    \node[inline random variable] (a) {$\mat{a}$};
    \node[inline random variable] (b) at (1, 0) {$\mat{c}$};
    \node[inline random variable] (c) at (2, 0) {$\mat{e}$};
    \draw[edge, directed] (a) -- (b);
    \draw[edge, directed] (b) -- (c);
}
blubb.
The next section introduces Bayesian machine learning as a rigorous formal framework that builds on statistical learning theory and enables the principled formulation of complex hypothesis spaces.

The next section introduces Bayesian machine learning as a rigorous formal framework that builds on statistical learning theory and enables the principled formulation of complex hypothesis spaces.
\begin{align}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, -0.5);
        \node[random variable] at (0, 0) (X) {$\mat{x}$};
        \node[random variable] at (0, -1) (Y) {$\mat{y}$};
        \draw[edge, directed] (X) -- (Y);
    \end{tikzpicture}
     &   &
    \Prob*{\mat{x}, \mat{y}}
     & =
    \Prob*{\mat{y} \given \mat{x}} \Prob*{\mat{x}}
\end{align}
The next section introduces Bayesian machine learning as a rigorous formal framework that builds on statistical learning theory and enables the principled formulation of complex hypothesis spaces.

The next section introduces Bayesian machine learning as a rigorous formal framework that builds on statistical learning theory and enables the principled formulation of complex hypothesis spaces.
\begin{align}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, -0.5);
        \node[random variable] at (0, 0) (X1) {$\mat{x}_1$};
        \node[random variable] at (0, -1) (Y1) {$\mat{y}_1$};
        \node[random variable] at (1, 0) (X2) {$\mat{x}_2$};
        \node[random variable] at (1, -1) (Y2) {$\mat{y}_2$};
        \node at (1.5, -0.5) (dots) {$\cdots$};
        \draw[edge, directed] (X1) -- (Y1);
        \draw[edge, directed] (X1) -- (Y2);
        \draw[edge, directed] (X2) -- (Y2);
        \draw[edge, directed] (X2) -- (Y1);
        \draw[edge, directed, bend left=20] (Y1) to (Y2);
        \draw[edge, directed, bend left=20] (Y2) to (Y1);
    \end{tikzpicture}
     &   &
    \Prob*{\mat{x}, \mat{y}}
     & =
    \Prob*{\mat{y}_1, \mat{y}_2 \given \mat{x}_1, \mat{x}_2} \Prob*{\mat{x}_1, \mat{x}_2}
\end{align}
The next section introduces Bayesian machine learning as a rigorous formal framework that builds on statistical learning theory and enables the principled formulation of complex hypothesis spaces.

The next section introduces Bayesian machine learning as a rigorous formal framework that builds on statistical learning theory and enables the principled formulation of complex hypothesis spaces.
\begin{align}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, -1);
        \node[random variable] at (0, 0) (X1) {$\mat{x}_1$};
        \node[random variable] at (0, -1) (F1) {$\mat{f}_1$};
        \node[random variable] at (0, -2) (Y1) {$\mat{y}_1$};
        \node[random variable] at (1, 0) (X2) {$\mat{x}_2$};
        \node[random variable] at (1, -1) (F2) {$\mat{f}_2$};
        \node[random variable] at (1, -2) (Y2) {$\mat{y}_2$};
        \node at (1.5, -1) (dots) {$\cdots$};
        \draw[edge, directed] (X1) -- (F1);
        \draw[edge, directed] (F1) -- (Y1);
        \draw[edge, directed, bend left=20] (F1) to (F2);
        \draw[edge, directed] (X2) -- (F2);
        \draw[edge, directed] (F2) -- (Y2);
        \draw[edge, directed, bend left=20] (F2) to (F1);
    \end{tikzpicture}
     &   &
    \Prob*{\mat{x}, \mat{y}}
     & =
    \left\{
    \begin{aligned}
         & \Prob*{\mat{y}_1 \given \mat{f}_1} \Prob*{\mat{y}_2 \given \mat{f}_2}                 \\
         & \cdot \Prob*{\mat{f}_1, \mat{f}_2 \given \mat{x}_1, \mat{x}_2} \Prob*{\mat{x}_1, \mat{x}_2}
    \end{aligned}
    \right.
\end{align}
The next section introduces Bayesian machine learning as a rigorous formal framework that builds on statistical learning theory and enables the principled formulation of complex hypothesis spaces.

\section{Thesis outline}
\todo[inline]{Is it finished yet?}
