\chapter{Introduction}
\label{toc:introduction}
In many machine learning problems, the task is to derive a model for observations of some process in nature that is not fully understood.
While traditional statistics favor the white-box modeling culture that focuses on explaining observational data using causal theoretic models, machine learning is more closely associated with the black-box culture~\parencite{breiman_statistical_2001,shmueli_explain_2010}.
When formulating a black-box model, we accept that we cannot model nature directly and instead focus on the functional dependency between inputs and outputs.
Using general function approximators like neural networks or Gaussian processes, any smooth dependency can be modeled given enough observations.
With growing computational power and the availability of data sets, black-box approaches have led to impressive advancements in domains that are hard to formalize.
Successes have been seen in a wide range of digital tasks such as speech recognition~\parencite{hochreiter_long_1997,chorowski_attention-based_2015,bahdanau_neural_2014,chorowski_attention-based_2015}, computer vision~\parencite{russakovsky_imagenet_2015,lecun_backpropagation_1989}, non-cooperative games~\parencite{berner_dota_2019,silver_mastering_2016}, or machine translation~\parencite{johnson_googles_2017}.
The abundance of data in these domains shifts the focus away from traditional statistical models to approaches that are highly adaptabe and scalable.

However, bridging the gap to applications in critical systems in the physical world has proved challenging.
Problem domains like robotics or industrial control introduce a new set of requirements.
In industrial control, machine learning is used to find more efficient or safer control strategies not obvious to the engineers designing a machine~\parencite{hein_benchmark_2017}.
In such scenarios, relevant data are often scarce.
The most valuable data, such as when a machine will fail or how it will behave in new situations, is never produced.
As applications become more safety-relevant, gathering data through exploration can be problematic due to the adverse consequences of failure.
If they are to operate in safety-critical areas, interact with people, or carry responsibility, machine learning systems must be robust, trustworthy, and assessable.
Besides optimizing for good average-case performance, a responsible system needs to reason about plausible worst-cases to be able to avoid them.
To ensure trust, ML-systems often need to be verified by domain experts before deployment, emphasizing a need for interpretability.
Models are required to incorporate and reproduce expert knowledge and make consistent predictions, while at the same time yielding new insights from data.

The highly adaptable and scalable models that lead to success in digital domains are not well-suited to be deployed in safety-critical environments.
Large neural networks generally suffer from low interpretability~\parencite{rudin_stop_2019}, the need for large amounts of data and their inability to quantify uncertainties~\parencite{goodfellow_deep_2016}.
In this work, we explore how to formulate machine learning models that can satisfy the requirements of physical environments.


\section{Bayesian structured models with Gaussian processes}
% A key technique that allows us to cope with the requirements of critical systems in the physical world is probabilistic modeling.
% Bayesian inference offers a framework for formalizing expert knowledge in prior assumptions and combining them with data.
% To make predictions, all plausible models are taken into account, allowing us to both quantify confidence in predictions and take more unlikely but relevant scenarios into consideration.
% In modern science, probabilistic models are used in large-scale experiments such as in particle physics~\parencite{khachatryan_observation_2015} or astronomy~\parencite{collaboration_first_2019,the_ligo_scientific_collaboration_observation_2016}, where randomness plays a major role.
% These scientific models are generally formulated from a hypothesis testing perspective and follow the white-box philosophy.
% Causal theoretic models are extended with statistical theory to handle uncertain data and lead to conclusions in terms of effect sizes.
% The focus is on explaining the available observations instead of predicting new situations.

% For many applications of machine learning in the physical world, the situation is different.
% Since complete theoretic models are not available, the goal of modeling is to infer some aspects of a system's behavior from data while still yielding physically plausible explanations.
% Take wind power production as an example:
% To optimize the efficiency of a wind-turbine, the speed and pitch have to be controlled according to the local wind conditions.
% In a wind-farm, turbines are typically equipped with sensors for wind speed and direction.
% The goal is to use these sensor data to produce accurate estimates and forecasts of the wind conditions at every turbine in the farm.
% For the ideal case of a homogeneous and very slowly changing wind field, wind conditions can be estimated using the propagation times computed from geometry, wind speed and direction.
% In the real world, however, wind fields are not homogeneous, exhibit global and local turbulence, and interfere with the turbines and the terrain inside and outside the farm.
% This makes it extremely difficult to construct accurate analytical models of wind propagation in a farm.
% At the same time, data-driven black-box approaches struggle due to identifiability issues introduced by the noise of the unpredictable turbulence.
% While we cannot formulate analytical models, domain experts do have an intuitive understanding of the underlying physics of wind propagation.
% A successful model needs to combine the white-box and black-box approaches:
% It needs to reproduce expectations like slowly changing prevailing wind conditions or gusts that travel through the system while still being able to infer new knowledge about specific turbine behavior or the influence of terrain from data.
% By checking if these these expectations are met, domain experts can build trust in the model's predictions for unseen situations.

% In this work, we study how to formulate structured models that fullfill these requirements.
% We use information about the structure of a learning problem to formulate machine learning models that reproduce knowledge, are understandable for domain-experts, make physically plausible predictions in unseen situations and can quantify their own uncertainty.
% First, we explore how to embed general function approximators in Bayesian probabilistic models to enforce structure and learn from data.
% We discuss how to formulate inference schemes based on composite and hierarchical Gaussian process models that yield informative uncertainties.
% Second, we study how to use structured models to reproduce expert knowledge and factorize uncertainties to achieve interpretability.
% We explore how rich internal structure helps models to generalize to unseen situations.
% And third, we discuss how to evaluate structured models.
% As evaluation data in critical parts of the system is often scarce in physical environments, performance measures such as the generalization error are not enough to identify a desirable structured model.
% To formalize the subjectiveness in the description of desirable models, we will explore how downstream tasks can be taken into account for model selection.

Why should I care about structured models and their evaluation?
\begin{enumerate}
    \item Besides predictions, we want to use models for validation and root-cause analysis, so understanding is key. We cannot hide knowledge in the choices of optimizer or the regularization as domain experts will not understand - we need our knowledge to be as explicit as possible.

    \item Large models in deep-learning approaches excel in settings where large amounts of data is available and exploration is cheap and safe.
    When a model behaves incorrectly in a specific setting, more data can be collected and the model can be improved.
    In the physical world however, safety-concerns or high cost make it infeasible to collect data about failure modes or new control strategies.
    We therefore need to formulate models that make do with limited amounts of data that do not cover all situations of interest.
    \item It is nonetheless critical to make reliable predictions about unseen situations to find more efficient or safer control strategies.
    To formulate models that can generalize aggressively, we need to make use of the available expert knowledge.
    In contrast to classical analytical models that fully explain a system, we accept that there are parts of the system we do not understand and want to learn from data.
    Since knowledge uncovered from data is imperfect due to identifiability-issues, we cannot blindly trust predictions.
    Models that can quantify uncertainties allow us to judge how high our trust-level should be.
    \item Experts need to understand and trust our systems to make reliable predictions.
    Fully black-box models are hard to deploy because we cannot explain how they work post-hoc.
    We therefore need to formulate models that inherently fulfill the expectations of domain experts and allow experts to validate this is the case, even in unseen situations.
    Bayesian modelling offers a framework to formulate such models.
    However, traditional Bayesian models and inference schemes are based on causal theoretic models and cannot handle general function approximators.
    How do we combine the properties of both?
    \item The black-box approach generally relies on generalization error measures for model selection.
    When data is abundant, enough generalization data can be collected to comprehensively evaluate a model.
    If evaluation data in critical parts of the system is not available, performance measures are not sufficient to identify desirable models.
    Instead, domain experts inspect a model's internal structure and subjectively select models for deployment.
    Can we make use of this implicit knowledge during model selection?
    \item Our main research questions are:
        \begin{enumerate}
            \item How can we combine the advantages of analytical models and black-box approaches to formulate models that enforce structure and learn from data?
            How can we formulate inference schemes that scale?
            \item How can we use structured models to reproduce expert knowledge and factorize uncertainties to generalize confidently and achieve interpretability?
            \item How do we evaluate structured models beyond classical performance measures?
            How do we formalize the subjectiveness in what makes a model desirable?
        \end{enumerate}
\end{enumerate}


\section{Thesis outline}
The content of this thesis is structured as follows.
In \cref{toc:gp}, we provide a short introduction to Bayesian machine learning and Bayesian nonparametrics with a focus on Gaussian processes (GPs).
We discuss the connection between statistical learning theory and Bayesian inference and their application to hierarchical models.
After introducing GPs, we discuss variational approaches to sparse GPs, their extension to hierarchical GP models, and efficient inference schemes.

In \cref{toc:data_association}, we explore how to formulate structured hierarchical models using the data association problem as an example.
We separate the data association problem into hierarchical components and derive an efficient joint inference scheme, resulting in a fully Bayesian model.
We show that this model is able to factorize multi-modal data into independent processes, providing explicit models for both the processes and assignment probabilities.
Comparisons with previous models show the additional qualitative model capabilities of the structured approach and competitive black-box performance.
However, experiments also show that standard measures are not enough to identify desirable models.

In \cref{toc:alignment}, we formulate a structured Bayesian model that reproduces expert knowledge about wind propagation in a wind-farm.
We interpret the problem of modeling the power production of multiple wind-turbines as a nonlinear alignment problem and derive an efficient inference scheme.
Based on expert knowledge about the underlying latent and turbulent wind field, we can derive a structured model that enforces a physically plausible dependency structure between multiple time-series.
We discuss how these constraints help to solve an otherwise highly ambiguous learning problem and show that the imposed structure leads to a rich internal model-structure that can be interpreted by experts.

In \cref{toc:interpretable_rl}, we consider how to formalize the subjectiveness introduced via expert knowledge and model ambiguities to evalute structured models.
We include the task a model will be used to solve into model selection, which allows us to distinguish between qualitatively different models showing similar performance metrics.
We revisit the data association problem and embed it into a reinforcement learning problem, where identifying the correct underlying dynamics, and therefore a desirable model, is critical to finding a successful policy.
We show that semantic hierarchical structure increases data efficiency and allows domain experts to influence agent behavior through detailed insights into the dynamics of a system.

\Cref{toc:discussion} concludes the work presented in this thesis and interprets the results in a broader context by further exploring the properties and evaluation of structured hierarchical models.
In \cref{toc:discussion:composition}, we present an intuitive argument for why inference schemes based on factorizations between layers cannot represent heterogeneous posteriors.
In \cref{toc:discussion:bo}, we argue why models with suboptimal marginal likelihoods can perform well in hierarchical systems.
In \cref{toc:discussion:mountaincar}, we explore this idea further and consider how tasks can be included in the inference problem directly.
Finally, we discuss possible further directions of research.


\section{Contributions}
This work presents novel Bayesian structured models inspired by industrial applications.
Our main contributions are as follows:
\begin{compactitem}
    \item We formulate fully Bayesian interpretations of the data association problem and nonlinear timeseries alignment.
    \item We formulate efficient and scalable inference schemes for these models by extending the variational inference schemes for deep Gaussian processes to handle multi-output GPs and discrete structure.
    \item We present a Bayesian view on model-based reinforcement learning and formulate an efficient probabilistic policy search algorithm for structured dynamics models.
    \item We discuss the limitations of current variational inference schemes for hierarchical Gaussian process models.
    \item We argue why surrogate models with suboptimal marginal likelihoods can perform well in Bayesian optimization problems.
\end{compactitem}

The main contributions presented in this thesis appeared in a number of peer-reviewed publications and patents we outline below.
In \cref{contrib:data_association,contrib:patent_depeweg,contrib:patent_geipel}, we formulate a Bayesian interpretation of the data association problem and discuss an application of this model to the prediction of the combustion dynamics of a gas-turbine, where the structured model allows us to distinguish between different dynamical regimes.
In~\cref{contrib:windpower,contrib:patent_egedal,contrib:patent_kaiser}, we formulate a hierarchical Bayesian model for a nonlinear time-series alignment problem, show that this model is capable of representing the complex interactions between turbines in a wind farm and discuss how this model can be used to derive more efficient controllers for wind-turbines in.
In \cref{contrib:interpretable_rl,contrib:multi_modal_rl}, we apply our data association model to a reinforcement learning task and show how a semantic decomposition of the dynamics reduces the data requirements and produces interpretable solutions.
In \cref{contrib:surrogates}, we formulate surrogate models for Bayesian optimization problems that focus on the informative structure of the objective functions by sacrificing local accuracy and in \cref{contrib:composition}, we present an argument for why inference schemes based on factorizations between layers cannot represent heterogeneous posteriors.


\subsubsection{Own publications}
\begin{enumerate}
    \item \label[contribution]{contrib:windpower}\fullfullcite{kaiser_bayesian_2018}
    \item \label[contribution]{contrib:data_association}\fullfullcite{kaiser_data_2019}
    \item \label[contribution]{contrib:interpretable_rl}\fullfullcite{kaiser_interpretable_2019}
    \item \label[contribution]{contrib:multi_modal_rl}\fullfullcite{kaiser_bayesian_2020}
    \item \label[contribution]{contrib:surrogates}\fullfullcite{bodin_modulating_2020}
    \item \label[contribution]{contrib:composition}\fullfullcite{ustyuzhaninov_compositional_2020}
\end{enumerate}

\subsubsection{Patents}
\begin{enumerate}
    \setcounter{enumi}{6}
    \item \label[contribution]{contrib:patent_egedal}\fullfullcite{egedal_verfahren_2019}
    \item \label[contribution]{contrib:patent_kaiser}\fullfullcite{kaiser_verfahren_2019}
    \item \label[contribution]{contrib:patent_geipel}\fullfullcite{geipel_transferlernen_2020}
    \item \label[contribution]{contrib:patent_depeweg}\fullfullcite{depeweg_computer_2020}
\end{enumerate}
