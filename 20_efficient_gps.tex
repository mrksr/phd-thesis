\chapter{Computationally efficient hierarchical Gaussian processes}
\label{toc:gp}
Bayesian graphical models as introduced in~\cref{toc:bayesian_ml:bayesian_ml} encode structural assumptions about a machine learning problem.
An important assumption in the regression problem is a latent function $f$ which maps inputs to outputs.
In the Bayesian linear regression example, a posterior over linear functions could be calculated by calculating a posterior over parameters instead, since every parameter explicitly represents a linear function.
A Gaussian process (GP) is a non-parametric model which directly represents a distribution over functions.
Instead of formulating an explicit parameterized formula, a GP prior encodes more general assumptions about $f$ such as differentiability or variability.
A GP posterior can be calculated analytically, making Bayesian inference over GPs and hierarchical models with embedded GPs computationally feasible.

This chapter introduces Gaussian processes formally, describes how to formulate and select a GP prior and how to derive a GP posterior.
Since GPs defined on many observations are computationally expensive, sparse approximations are used in practice.
We will discuss one class of sparse approximations based on inducing observations and introduce approaches based on variational inference.
\todo{Add hierarchical GPs here?}

\section{Gaussian processes}
Gaussian processes are a generalization of the Gaussian distribution to function spaces.
A multivariate Gaussian $\mat{x} \sim \Gaussian{\mat{\mu}, \mat{\Sigma}}$ describes a distribution over the finitely many elements in the vector $\mat{x}$.
Every such element $\rv{x}_i$ is normally distributed according to $\rv{x}_i \sim \Gaussian{\mu_i, \Sigma_{ii}}$ and every linear combination of the $\mat{x}_i$ is also normally distributed~\parencite{astrom_introduction_1971}.
For every pair $(\rv{x}_i, \rv{x}_j)$, their covariance is given by $\Moment{\cov}{\rv{x}_i, \rv{x}_j} = \mat{\Sigma}_{ij}$.
The probability density of $\mat{x}$ is given by
\begin{align}
    \begin{split}
        \label{eq:gaussian_pdf}
        \Prob*{\mat{x}}
        &= \Gaussian{\mat{x} \given \mat{\mu}, \mat{\Sigma}} \\
        &= \frac{1}{\sqrt{\det(2\pi\mat{\Sigma})}}\Fun*{\exp}{-\frac{1}{2}(\mat{x} - \mat{\mu})\tran \mat{\Sigma}\inv (\mat{x} - \mat{\mu})}.
    \end{split}
\end{align}

Multivariate Gaussians have several convenient closure properties~\parencite{astrom_introduction_1971}.
Assume a split $\mat{x} = (\mat{x}_1, \mat{x}_2)$ into two partial vectors and denote
\begin{align}
    \begin{split}
        \begin{pmatrix}
            \mat{x}_1 \\
            \mat{x}_2
        \end{pmatrix}
        \sim \Gaussian*{
            \begin{pmatrix}
                \mat{\mu}_1 \\
                \mat{\mu}_2
            \end{pmatrix},
            \begin{pmatrix}
                \mat{\Sigma}_{11} & \mat{\Sigma}_{12} \\
                \mat{\Sigma}_{21} & \mat{\Sigma}_{22}
            \end{pmatrix}
        }
    \end{split}
\end{align}
the same split of the mean vector $\mat{\mu}$ and covariance matrix $\mat{\Sigma}$.
Then, the marginal distribution of $\mat{x}_1$ is also a Gaussian with
\begin{align}
    \begin{split}
        \label{eq:gaussian_marginal}
        \Prob*{\mat{x}_1}
        &= \int \Prob*{\mat{x}_1, \mat{x}_2} \diff \mat{x}_2 \\
        &= \Gaussian{\mat{\mu}_1, \mat{\Sigma}_1}
    \end{split}
\end{align}
and the conditional of $\mat{x}_1$ given $\mat{x}_2$ is a Gaussian as well with
\begin{align}
    \begin{split}
        \label{eq:gaussian_conditional}
        \Prob*{\mat{x}_1 \given \mat{x}_2}
        &= \frac{\Prob*{\mat{x}_1, \mat{x}_2}}{\Prob*{\mat{x}_2}} \\
        &= \Gaussian{\hat{\mat{\mu}}, \hat{\mat{\Sigma}}}\text{, and} \\[\smallskipamount]
        \hat{\mat{\mu}}
        &= \mat{\mu}_1 + \mat{\Sigma}_{12}\mat{\Sigma}\inv_{22} (\mat{x}_2 - \mat{\mu}_2) \\[\smallskipamount]
        \hat{\mat{\Sigma}}
        &= \mat{\Sigma}_{11} - \mat{\Sigma}_{21} \mat{\Sigma}\inv_{22} \mat{\Sigma}_{12} \\
        &= \mat{\Sigma}_{11} - \mat{\Sigma}\tran_{12} \mat{\Sigma}\inv_{22} \mat{\Sigma}_{12}.
    \end{split}
\end{align}

Modeling functions in general requires an infinite number of random variables, one for every function value.
Such structure of a number of possibly dependent random variables mapping from the same probability space to the same value space is called a stochastic process and is represented via a function.
\begin{definition}[Stochastic Process]
    \label{def:gp:stochastic_process}
    Given a probability space $(\Omega, \mathcal{F}, P)$, an index set $T$ and a measurable space $Y$, a stochastic process $\rv{X}$ is a function
    \begin{align}
        \rv{X} \colon \left\{\begin{aligned}
            T \times \Omega & \to Y                    \\
            (t, \omega)     & \mapsto \rv{X}_t(\omega)
        \end{aligned}\right.
    \end{align}
    mapping indices $t$ to $Y$-valued random-variables.
    For a fixed $\omega \in \Omega$, $\rv{X}(\cdot, \omega)$ is called a trajectory of the process \cite{astrom_introduction_1971}.
\end{definition}

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[x=4em, y=5em]
        \node[random variable, observed] at (0, 0) (X) {$\mat{x}_n$};
        \node[random variable, latent] at (0, -1) (F) {$\mat{f}_n$};
        \node[random variable, hyperparameter] at (-1, -0.75) (mean) {$\mu$};
        \node[random variable, hyperparameter] at (-1, -1.25) (kernel) {$\K$};
        \node[random variable, observed] at (0, -2) (Y) {$\mat{y}_n$};
        \draw[edge, directed] (X) -- (F);
        \draw[edge, directed] (F) -- (Y);
        \draw[edge, directed, loop right] (F) to (F);
        \draw[edge, directed] (mean) -- (F);
        \draw[edge, directed] (kernel) -- (F);
        %
        \begin{scope}[on background layer]
            \node[
                align plate,
                inner xsep=10pt,
                fit=(X)(F)(Y),
                label={[anchor=south east]south east:$N$}
            ] {};
        \end{scope}
    \end{tikzpicture}
    \caption{
        GP graphical model.
        \label{fig:gp:graphical_model}
    }
\end{figure}
The index set of a stochastic process can be an arbitrary set.
It is often interpreted as a time index which can be both discrete and continuous.
A Gaussian process is a particular stochastic process.
\begin{definition}[Gaussian Process]
    \label{def:gp:gaussian_process}
    A stochastic process $\rv{X}$ is called a Gaussian process if for any finite subset $\tau \subseteq T$ of its index set, the random variables $\rv{X}_\tau$ have a joint Gaussian distribution \cite{astrom_introduction_1971}.
\end{definition}
When using a Gaussian process $\rv{X}$ to model a function $f \colon A \to B$, the index set $T$ is assumed to be $A$ and all random variables are $B$-valued.
The random variable $\rv{X}_a$ then models the function value $f(a)$ for all $a \in A$.
Sampling a trajectory from $\rv{X}$ corresponds to sampling one possible function $f^\ast$.

Similar to the finite case, the random variables share a dependency structure.
Instead of a mean vector $\mat{\mu}$ and a covariance matrix $\mat{\Sigma}$, a Gaussian process is completely determined by a mean function $\mu(a) = \Moment{\E}{f(a)}$ and a covariance function
\begin{align}
    \begin{split}
        \K(a, a^\prime) &\coloneqq \Moment{\E}{(f(a) - \mu(a))(f(a^\prime) - \mu_f(a^\prime))} \\
        &= \Moment{\cov}{f(a), f(a^\prime)} \\
        &= \Moment{\cov}{\rv{X}_a, \rv{X}_{a^\prime}}
    \end{split}
\end{align}
with $a, a^\prime \in A$.
The mean function encodes the point-wise mean over all trajectories which could be sampled from $\rv{X}$.
The covariance function is also called a kernel and describes the interaction between different parts of the function.
A function which is distributed according to a Gaussian process is denoted as $f \sim \GP\Cond{\mu, \K}$.

A GP can be used as a distribution over functions in the graphical model for regression problems in~\cref{eq:bayesian_ml:non_parametric}.
Because the random variables $\mat{f}_n$ modelling the function value $f(\mat{x}_n)$ are jointly Gaussian, they are not independent and thus are connected in the graphical model in~\cref{fig:gp:graphical_model}.
The choice of mean function
\tikz[x=2.5em,baseline=(f.base)]{
    \node[inline random variable, hyperparameter] (f) {$\mu$};
}
and kernel
\tikz[x=2.5em,baseline=(f.base)]{
    \node[inline random variable, hyperparameter] (f) {$\K$};
}
describe the GP prior and are often referred to as hyper-parameters.

For convenience it is often assumed that the prior mean function $\mu$ is constant zero.
This assumption is without loss of generality \cite{rasmussen_gaussian_2006} since otherwise, the observations $\left( \mat{X}, \mat{y} \right)$ can be transformed to $\mat{y^\prime} = \mat{y} - \mu(\mat{X})$.
The Gaussian process based on the observations $\left( \mat{X}, \mat{y^\prime} \right)$ then only models the differences to the mean function.
It is the covariance functions which encode the assumptions about the underlying function.

\subsubsection{Kernels}
The covariance for any pair of random variables $(\rv{X}_i, \mat{X}_j)$ in a GP is given by the the kernel $\Moment{\cov}{\rv{X}_i, \rv{X}_j} = \K(i, j)$.
A kernel therefore can not be any arbitrary function but must yield valid covariance matrices $\mat{\Sigma}$.
The matrix obtained by applying a kernel pairwise to finitely many random variables is called the Gram matrix.
Given two sets $\mat{A} = \Set*{\mat{a}_i \with i \in [n]}$ and $\mat{B} = \Set*{\mat{b}_j \with j \in [m]}$ and $[n] = \Set*{1, \dots, n}$.
The Gram matrix with respect to $\mat{A}$ and $\mat{B}$ using kernel $\K$ is given by
\begin{align}
    \K(\mat{A}, \mat{B}) = \mat{K_{\mat{A}\mat{B}}} \coloneqq \bigg( \K(\mat{a}_i, \mat{b}_j) \bigg)_{\substack{i \in [n], \\ j \in [m]}}.
\end{align}

For the Gram matrix to be a valid covariance matrix $\mat{\Sigma}$ of a Gaussian distribution, it must be positive definite.
Kernels are functions which fulfill the property that for every possible subset of random variables, or more generally every set of elements in their domain, their induced Gram matrix is positive definite.
\begin{definition}[Kernel]
    Given a non-empty set $A$, a function
    \begin{align}
        \K \colon A^2 \to \Rb
    \end{align}
    is called a (positive definite) kernel or covariance function, if for any finite subset $X \subseteq A$, the Gram matrix $\K(X, X)$ is positive definite.
\end{definition}
The kernel is crucial in encoding the assumptions about the function a Gaussian process should estimate.
It is a measure of how much different points in the GP's domain inform each other.
A natural assumption to make is that the closer together in the domain two points lie, the more similar their function values will be.
Similarly, to predict a test point, training points close to it are probably more informative than those further away.

But closeness is not the only possible reason two points could be similar.
Assume a function which is a possibly noisy sinusoidal wave with a known frequency.
Then, two points which are a multiple of wavelengths apart should also have similar function values.
Such a kernel which is not only dependent on the distance between two points but also their position in the input space is called non-stationary.
A simple example of such a non-stationary kernel is the linear kernel.
\begin{definition}[Linear Kernel]
    For a finite dimensional euclidean vector space $\Rb^d$, the linear kernel is defined as
    \begin{align}
        \K_{\text{linear}}(\mat{x}, \mat{y}) \coloneqq \mat{x}\tran \mat{y} = \left\langle \mat{x}, \mat{y}\right\rangle.
    \end{align}
\end{definition}
Consider a function $f \colon \Rb \to \Rb$ which is distributed according to a GP with the linear kernel $f \sim \GP\Cond{\mat{0}, \K_{\text{linear}}}$.
According to the definition of GPs, for any two input numbers $x$, $y \in \Rb$ their corresponding random variables $\rv{f}_x$ and $\rv{f}_y$ have a joint Gaussian distribution
\begin{align}
    \begin{pmatrix}
        \rv{f}_x \\ \rv{f}_y
    \end{pmatrix} \sim \Gaussian*{\mat{0}, \begin{bmatrix}
            \K(x, x) & \K(x, y) \\
            \K(y, x) & \K(y, y)
        \end{bmatrix}}.
\end{align}
Assuming that both $x$ and $y$ are not equal to zero, the correlation coefficient $\rho$ of these two variables is given by
\begin{align}
    \begin{split}
        \Moment{\rho}{\rv{f}_x, \rv{f}_y} &= \frac{\Moment{\cov}{\rv{f}_x, \rv{f}_y}}{\sqrt{\Moment{\var}{\rv{f}_x\vphantom{\rv{f}_y}}}\sqrt{\Moment{\var}{\rv{f}_y}}} \\
        &= \frac{\K(x, y)}{\sqrt{\K(x, x)} \sqrt{\K(y, y)}} = \frac{xy}{\sqrt{\vphantom{y^2}x^2}\sqrt{\vphantom{y^2}y^2}} \in \left\{ -1, 1 \right\}.
    \end{split}
\end{align}
A correlation coefficient of plus or minus one implies that the value of one of the random variables is a linear function of the other.
Any function drawn from this Gaussian process, such as the ones shown in \cref{fig:gp:gp_samples:linear}, is therefore a linear function.
This observation generalizes to higher dimensions \cite{rasmussen_gaussian_2006}.
Gaussian process regression with a linear kernel is equivalent to Bayesian linear regression as discussed in~\cref{toc:bayesian_ml:bayesian_ml}.
\begin{figure}[p]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_prior_linear}
        \caption{
            Linear
            \label{fig:gp:gp_samples:linear}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_prior_rbf}
        \caption{
            RBF with $\sigma_f = 1$ and $l=1$
            \label{fig:gp:gp_samples:rbf_normal}
        }
    \end{subfigure}\\[\figureskip]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_prior_rbf_amplitude}
        \caption{
            RBF with $\sigma_f = \sqrt{2}$ and $l=1$
            \label{fig:gp:gp_samples:rbf_noisy}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_prior_rbf_lengthscale}
        \caption{
            RBF with $\sigma_f = 1$ and $l = \sfrac{1}{4}$
            \label{fig:gp:gp_samples:rbf_lengthscale}
        }
    \end{subfigure}\\[\figureskip]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_prior_arccos_1}
        \caption{
            Arc-cos with order 1
            \label{fig:gp:gp_samples:arccos:1}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_prior_arccos_2}
        \caption{
            Arc-cos with order 2
            \label{fig:gp:gp_samples:arccos:2}
        }
    \end{subfigure}
    \caption[Samples from GP priors]{
        Samples from GP priors
        % Since the mean function $\mu_f$ is assumed to be constant zero, the kernel specifies the prior assumptions about the function.
        % A dashed sample function can be drawn by sampling a multivariate Gaussian with the kernel's Gram matrix using a grid of discrete sampling positions.
        % While samples from the linear kernel are always hyperplanes, the RBF kernel describes arbitrary smooth functions.
        % The hyper-parameters $l$ and $\sigma_f$ of the kernel describe the assumed dynamic range in $x$ and $y$ directions respectively.
        \label{fig:gp:gp_samples}
    }
\end{figure}

It is often of interest to also represent non-linear dependencies.
A common approach is to restrict information to local neighborhoods.
A kernel which is a function of $\norm{\mat{x} - \mat{y}}$ is called stationary and is invariant to translations in the input space.
The most important stationary kernel is the squared exponential kernel.
\begin{definition}[Squared Exponential Kernel]
    \label{def:gp:rbf_kernel}
    For a finite dimensional euclidean vector space $\Rb^d$, the squared exponential kernel or RBF kernel is defined as
    \begin{align}
        \K_{\text{SE}}(\mat{x}, \mat{y}) \coloneqq \sigma_f^2 \cdot \exp\!\left( -\frac{1}{2} (\mat{x} - \mat{y})\tran \mat{\Lambda}^{-1} (\mat{x} - \mat{y}) \right).
    \end{align}
    The parameter $\sigma_f^2 \in \Rb_{>0}$ is called the signal variance and $\mat{\Lambda} = \diag(l_1^2, \dots, l_d^2)$ is a diagonal matrix of the squared length scales $l_i \in \Rb_{>0}$.
\end{definition}
The similarity of two data points approaches one when they are close together and for larger distances approaches zero with exponential drop off.
It can be shown that this kernel represents all infinitely differentiable functions \cite{rasmussen_gaussian_2006}.
Gaussian processes with this covariance function are universal function approximators.

The squared exponential kernel is dependent on multiple parameters which influence its behavior.
In contrast to weight parameters in linear regression or constants in physical models, these parameters do not specify the estimated function but rather the prior belief about this function and are therefore hyper-parameters.

The hyper-parameters of the RBF kernel describe the expected dynamic range of the function.
The signal variance $\sigma_f^2$ specifies the average distance of function values from the mean function.
The different length scale parameters $l_i$ roughly specify the distance of data points along their respective axis required for the function values to change considerably.
\Cref{fig:gp:gp_samples} compares sample functions drawn from Gaussian processes with the linear kernel, squared exponential kernels with different hyper-parameters and the arc cosine kernel~\parencite{cho_kernel_2009}.
Arc cosine kernels mimic the behavior of infinitely wide neural networks with a single hidden layer.
The order of an arc cosine kernel specifies the activation function of such an infinitely wide neural network.
The first three orders assume step-functions, rectified linear units or rectified quadratic units respectively.
Arc cosine kernels of low order behave similar to squared exponential kernels in practice but have different generalization-properties.
While GPs with an RBF kernel return to the prior away from data yielding predictions roughly in the area $\mu_f \pm 2\sigma_f$, the arc cosine kernel of order 1 generalizes linearly using the derivative at the closest data points\todo{really?}.

\subsubsection{Predictions and Posterior}
To use Gaussian processes for regression, it is necessary to combine observations with a Gaussian process prior $f \sim \GP\Cond{\mat{0}, \K}$ and obtain a predictive posterior.
We assume Gaussian noise $\epsilon \sim \Gaussian{0, \sigma^2}$ with $\mat{y}_n = f(\mat{x}_n) + \epsilon$ and denote the $N$ observations as $\mat{X} = (\mat{x}_1, \dots, \mat{x}_N)$ and $\mat{y} = (\mat{y}_1, \dots, \mat{y}_N)$.
The likelihood of the observations given the latent function values $\mat{f}$ is given by
\begin{align}
    \begin{split}
        \Prob{\mat{y} \given f, \mat{X}, \sigma}
        = \Prob{\mat{y} \given \mat{f}, \sigma}
        &= \prod_{n = 1}^N \Gaussian{\mat{y}_n \given \mat{f}_n, \sigma^2} \\
        &= \Gaussian{\mat{y} \given \mat{f}, \sigma^2 \Eye}.
    \end{split}
\end{align}
Given a vector of hyper-parameters $\mat{\theta}$, the definition of Gaussian processes yields a joint Gaussian distribution for the latent function values $\mat{f}$ given by
\begin{align}
    \Prob{\mat{f} \given \mat{X}, \mat{\theta}} = \Gaussian*{\mat{f} \given \mat{0}, \mat{K}_{\mat{f}\mat{f}}}
\end{align}
where $\mat{K}_{\mat{f}\mat{f}} = \K(\mat{X}, \mat{X})$ denotes the Gram matrix of the observed data.
Combining the two distributions according to the law of total probability yields the probability distribution of the outputs conditioned on the inputs and is given by
\begin{align}
    \begin{split}
        \label{eq:gp:gp_marginal_likelihood}
        \Prob{\mat{y} \given \mat{X}, \mat{\theta}} &= \int \Prob{\mat{y} \given \mat{f}} \Prob{\mat{f} \given \mat{X}, \mat{\theta}} \diff \mat{f} \\
        &= \int \Gaussian{\mat{y} \given \mat{f}, \sigma^2 \Eye} \Gaussian*{\mat{f} \given \mat{0}, \mat{K}_{\mat{f}\mat{f}}} \diff \mat{f} \\
        &= \Gaussian{\mat{y} \given \mat{0}, \mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye}.
    \end{split}
\end{align}
Note that this distribution is obtained by integrating over all possible latent function values $\mat{f}$ and thereby taking all possible function realizations into account.
The closed form solution of the integral is obtained using well-known results about Gaussian distributions which are for example detailed in \cite{petersen_matrix_2008}.

\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_posterior_prior}
        \caption{GP Prior}
        \label{fig:gp:gp_posterior:prior}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_posterior_posterior}
        \caption{GP Posterior}
        \label{fig:gp:gp_posterior:posterior}
    \end{subfigure}
    \caption[GP posterior]{
        \Cref{fig:gp:gp_posterior:prior} shows a GP prior with an RBF kernel.
        After observing the black data points, the mean function of the posterior GP in \cref{fig:gp:gp_posterior:posterior} is no longer constant zero.
        The dashed function samples of the posterior GP interpolate the data but can be different in-between.
        The shaded area represents the point wise mean plus and minus two times the standard deviation.
        \label{fig:gp:gp_posterior}
    }
\end{figure}
Now consider a set of new points $\mat{X}_\ast$ for which the predictive posterior should be obtained.
By definition of GPs, the latent function values $\mat{f}$ of the data set and the latent function values at the new points $\mat{f}_\ast = f(\mat{X}_\ast)$ have the joint Gaussian distribution
\begin{align}
    \Prob*{\begin{pmatrix}
            \mat{f} \\
            \mat{f}_\ast
        \end{pmatrix} \given \mat{X}, \mat{X}_\ast, \mat{\theta}} & = \Gaussian*{\begin{pmatrix}
            \mat{f} \\
            \mat{f}_\ast
        \end{pmatrix} \given \mat{0}, \begin{bmatrix}
            \mat{K}_{\mat{f}\mat{f}} & \mat{K}_{\mat{f}\ast} \\
            \mat{K}_{\ast\mat{f}}    & \mat{K}_{\ast\ast}
        \end{bmatrix}}.
\end{align}
Adding the noise model to this distribution leads to the joint Gaussian of training outputs $\mat{y}$ and test outputs $\mat{f}_\ast$ which is given by
\begin{align}
    \label{eq:gp:predictive_joint}
    \Prob*{\begin{pmatrix}
            \mat{y} \\
            \mat{f}_\ast
        \end{pmatrix} \given \mat{X}, \mat{X}_\ast, \mat{\theta}} & = \Gaussian*{\begin{pmatrix}
            \mat{y} \\
            \mat{f}_\ast
        \end{pmatrix} \given \mat{0}, \begin{bmatrix}
            \mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye & \mat{K}_{\mat{f}\ast} \\
            \mat{K}_{\ast\mat{f}}                    & \mat{K}_{\ast\ast}
        \end{bmatrix}}.
\end{align}
In this distribution, the training outputs $\mat{y}$ are known.
The predictive posterior for the test outputs $\mat{f}_\ast$ can therefore be directly obtained by applying~\cref{eq:gaussian_conditional} to~\cref{eq:gp:predictive_joint} and is also a Gaussian.
\begin{lemma}[GP predictive posterior]
    \label{lem:gp:gp_posterior}
    Given a latent function with a Gaussian process distribution $f \sim \GP(\mat{0}, \K)$ and $N$ training points $\mat{X}$ with noisy observations of the form $\mat{y} = f(\mat{X}) + \Gaussian{\mat{0}, \sigma^2 \Eye}$.
    The predictive posterior $\mat{f}_\ast$ of the test points $\mat{X}_\ast$ is then given by
    \begin{align}
        \begin{split}
            \Prob{\mat{f}_\ast \given \mat{X}, \mat{y}, \mat{X}_\ast}
            &= \Gaussian*{\mat{f}_\ast \given \mat{\mu}_\ast, \mat{\Sigma}_\ast} \text{, where} \\
            \mat{\mu}_\ast
            &= \mat{K}_{\ast \mat{f}} \left( \mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye \right)\inv \mat{y} \\
            \mat{\Sigma}_\ast
            &= \mat{K}_{\ast\ast} - \mat{K}_{\ast \mat{f}} \left( \mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye \right)\inv \mat{K}_{\mat{f}\ast}.
        \end{split}
    \end{align}
\end{lemma}

This predictive posterior makes it possible to evaluate the function approximation based on the input at arbitrary points in the input space.
Since any set of these points always has a joint Gaussian distribution, the predictive posterior defines a new Gaussian process, which is the posterior Gaussian process given the observations.
This posterior process $\GP(\mu_\text{post}, \K_\text{post})$ has new mean and covariance functions given by
\begin{align}
    \begin{split}
        \mu_\text{post}(\mat{a}) &= \K(\mat{a}, \mat{X}) \left(\mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye \right)\inv \mat{y} \\
        \K_\text{post}(\mat{a}, \mat{b}) &= \K(\mat{a}, \mat{b}) - \K(\mat{a}, \mat{X}) \left( \mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye \right)\inv \K(\mat{X}, \mat{b}).
    \end{split}
\end{align}
Note that the posterior mean function is not necessarily the constant zero function.
\Cref{fig:gp:gp_posterior} shows samples from a pair of prior and posterior Gaussian processes with an RBF kernel.

Computing the inverse $\left(\mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye \right)\inv$ takes $\Oh(N^3)$ time but can be done as a preprocessing step since it is independent of the test points.
Predicting the mean function value of a single test point is a weighted sum of $N$ basis functions $\mu_\ast = \mat{K_{\ast \mat{f}}} \mat{\beta}$ where $\mat{\beta} = \left(\mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye \right)\inv \mat{y}$ which can be precomputed.
After this pre-computation, predicting the mean of a single test point takes $\Oh(N)$ time.
To predict the variance, it is still necessary to perform a vector-matrix multiplication which takes $\Oh(N^2)$ time for a single prediction.
Since all of these operations are dependent on the number of training points, evaluating Gaussian processes on large data sets can be computationally expensive.
Before introducing sparse approximations with better asymptotic complexity, we first consider how to choose good values for the hyper-parameters $\mat{\theta}$.

\subsubsection{Choosing hyper-parameters}
In the previous section we derived the posterior GP given constant hyper-parameters $\mat{\theta}$.
In this case, Gaussian processes models do not have to be trained or optimized at all as the posterior GP can be computed analytically.
Usually however, the correct choice of hyper-parameters is not clear a priori.
In a fully Bayesian setup we place a prior on the hyper-parameters $\Prob{\mat{\theta}}$ and marginalize it to derive the dependent distributions
\begin{align}
    \begin{split}
        \label{eq:gp:theta_posterior_integration}
        \Prob{f}
        & = \int \Prob{f \given \mat{\theta}} \Prob{\mat{\theta}} \diff \mat{\theta}                                                           \\
        \Prob{\mat{y} \given \mat{X}}
        & = \int \Prob{\mat{y} \given \mat{f}} \Prob*{\mat{f} \given \mat{X}, \mat{\theta}} \Prob{\mat{\theta}} \diff \mat{f} \diff \mat{\theta}.
    \end{split}
\end{align}
Updating the belief about the distribution of the hyper-parameters then becomes part of calculating the posterior using Bayes' theorem.
However, the integration required in \cref{eq:gp:theta_posterior_integration} is expensive as no closed form solution exists.
While true posteriors can be obtained for GPs with few observations such as in Bayesian optimization or probabilistic numerics contexts~\parencite{shahriari_taking_2016,oates_modern_2019}, the required calculations are often not tractable for larger problems.

A common approximation is to use maximum-a-posteriori point-estimates instead.
These estimates are obtained by maximizing $\Prob{\mat{\theta} \given \mat{X}, \mat{y}}$.
This posterior is proportional to the numerator of Bayes' theorem and given by
\begin{align}
    \begin{split}
        \Prob{\mat{\theta} \given \mat{X}, \mat{y}}
        &\propto \Prob*{\mat{\theta}} \Prob{\mat{y} \given \mat{X}, \mat{\theta}} \\
        &= \int \Prob*{\mat{\theta}}\Prob{\mat{y} \given \mat{f}, \mat{\theta}} \Prob{\mat{f} \given \mat{X}, \mat{\theta}} \diff \mat{f}.
    \end{split}
\end{align}
If $\Prob*{\mat{\theta}}$ is set to a flat distribution, the prior term vanishes and only the likelihood term remains.
Choosing hyper-parameters by maximizing the likelihood term is called a type II maximum likelihood estimate.

The marginal likelihood is the integral of the product of Gaussians in~\cref{eq:gp:gp_marginal_likelihood} given by
\begin{align}
    \begin{split}
        \Prob{\mat{y} \given \mat{X}, \mat{\theta}}
        &= \Gaussian{\mat{y} \given \mat{0}, \mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye}.
    \end{split}
\end{align}
Instead of maximizing the marginal likelihood directly, it is numerically convenient to minimize the negative logarithm of the likelihood
\begin{align}
    \begin{split}
        \Lc(\mat{\theta}) &= -\log\Prob{\mat{y} \given \mat{X}, \mat{\theta}} \\
        &=
        \frac{1}{2} \mat{y}\tran \left( \mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye \right)\inv \mat{y} +
        \frac{1}{2} \log \abs*{\mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye} +
        \frac{N}{2} \log(2\pi).
    \end{split}
\end{align}
Since the logarithm is a monotonous function it does not change the position of optima.
The maximum likelihood estimate is the solution of the optimization problem
\begin{align}
    \mat{\theta}^\ast & \in \argmin_{\mat{\theta}} \Lc(\mat{\theta})
\end{align}
and is calculated using standard approaches to non-convex optimization.
The computational complexity of evaluating the likelihood term and its derivatives is dominated by the inversion of $\mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye$ with a time complexity of $\Oh(N^3)$.

\section{Sparse Gaussian processes using inducing observations}
A major drawback of Gaussian processes in real-world applications is their high computational cost for large data sets.
Assume a data set $(\mat{X}, \mat{y})$ with $N$ training samples, then the operations on a posterior Gaussian process are usually dominated by the inversion of the kernel matrix $\mat{K}_{\mat{f}\mat{f}}$ which takes $\Oh(N^3)$ time.
While this inversion can be pre-computed, the cost of predicting the mean and variance of one test point remains $\Oh(N)$ and $\Oh(N^2)$ respectively.
Additionally, these operations have a space requirement of $\Oh(N^2)$.
The goal of sparse approximations of Gaussian processes is to find model representations which avoid the cubic complexities or at least restrict them to the training phase of finding hyper-parameters.
This section introduces one type of approximation based on representing the complete data set through a smaller set of points~\parencite{snelson_flexible_2007}.
The next section will place this approximation in a principled variational context~\parencite{titsias_variational_2009,hensman_gaussian_2013}.

The most direct approach to reduce the computational time required to invert $\mat{K}_{\mat{f}\mat{f}}$ is to only use a small subset of size $M \ll N$ of the original data.
Calculating the posterior GP relative to these $M$ observations only has a time complexity of $\Oh(M^3)$.
This approach can work for data sets with a very high level of redundancy but does impose the problem of choosing an appropriate subset.
While choosing a random subset can be effective \cite{snelson_flexible_2007}, the optimal choice is dependent on the hyper-parameters and both should therefore be chosen in a joint optimization scheme.
The selection of an appropriate subset defines a combinatorial optimization problem and is very hard to solve.

To overcome this problem, inducing observation approximations lift the restriction of choosing points from the data set and instead allow arbitrary positions in the input space.
The original data set is replaced by an inducing data set $(\mat{Z}, \mat{u})$ of inducing inputs $\mat{Z}$ and inducing variables $\mat{u} = f(\mat{Z})$ which are equal to the true latent values of the function function $f \sim \GP(\mat{0}, \K)$.
Since they are not true observations, they are assumed to be noise-free.
Given an inducing data set and hyper-parameters $\mat{\theta}$, the predictive posterior of this approximation is a standard GP posterior
\begin{align}
    \Prob{\mat{f}_\ast \given \mat{X}_\ast, \mat{Z}, \mat{u}, \mat{\theta}}
     & = \Gaussian{
    \mat{K}_{\ast \mat{u}}\mat{K}_{\mat{u}\mat{u}}\inv \mat{u},
    \mat{K}_{\ast\ast} - \mat{K}_{\ast \mat{u}} \mat{K}_{\mat{u}\mat{u}}\inv \mat{K}_{\mat{u}\ast}
    }
\end{align}
with $\mat{K}_{\mat{u}\mat{u}} = \K(\mat{Z}, \mat{Z})$ denoting the Gram matrix of the inducing inputs.

The true data set is independent given the latent function $f$ and can be assumed independent given the inducing data set if it represents $f$ well.
The likelihood of the original data under the Gaussian process trained on the inducing data set is given by
\begin{align}
    \begin{split}
        \Prob{\mat{y} \given \mat{X}, \mat{Z}, \mat{u}, \mat{\theta}}
        &= \prod_{n=1}^N \Prob{\mat{y}_n \given \mat{x}_n, \mat{Z}, \mat{u}, \mat{\theta}} \\
        &= \prod_{n=1}^N \Gaussian*{
        \mat{y}_n \given \mat{K_{\mat{f}_n\mat{u}}}\mat{K}_{\mat{u}\mat{u}}\inv \mat{u},
        \mat{K}_{\mat{f}_n \mat{f}_n} - \mat{K}_{\mat{f}_n \mat{u}} \mat{K}_{\mat{u}\mat{u}}\inv\mat{K}_{\mat{u} \mat{f}_n} + \sigma^2
        } \\
        &= \Gaussian*{
        \mat{y} \given \mat{K_{\mat{f}\mat{u}}}\mat{K}_{\mat{u}\mat{u}}\inv \mat{u},
        \Fun{\diag}{\mat{K}_{\mat{f}\mat{f}} - \mat{K}_{\mat{f} \mat{u}} \mat{K}_{\mat{u}\mat{u}}\inv\mat{K}_{\mat{u} \mat{f}}} + \sigma^2\Eye
        } \\
        &= \Gaussian*{
        \mat{y} \given \mat{K_{\mat{f}\mat{u}}}\mat{K}_{\mat{u}\mat{u}}\inv \mat{u},
        \Fun{\diag}{\mat{K}_{\mat{f}\mat{f}} - \mat{Q}_{\mat{f}\mat{f}}} + \sigma^2\Eye
        }
    \end{split}
\end{align}
with $\mat{Q}_{\mat{f}\mat{f}} \coloneqq \mat{K}_{\mat{f} \mat{u}} \mat{K}_{\mat{u}\mat{u}}\inv\mat{K}_{\mat{u} \mat{f}}$.

Since the inducing variables $\mat{u}$ are latent function values, the original GP prior for $f$ is a reasonable prior for their values given by
\begin{align}
    \Prob*{\mat{u} \given \mat{Z}} = \Gaussian{\mat{u} \given \mat{0}, \mat{K}_{\mat{u}\mat{u}}}.
\end{align}
Using this prior, the inducing variables can be marginalized through an integral of a product of Gaussians in
\begin{align}
    \begin{split}
        \Prob*{\mat{y} \given \mat{X}, \mat{Z}}
        &= \int \Prob*{\mat{y} \given \mat{X}, \mat{Z}, \mat{u}} \Prob*{\mat{u} \given \mat{Z}} \diff \mat{u} \\
        &= \int \Prob*{\mat{y} \given \mat{X}, \mat{Z}, \mat{u}} \Gaussian{\mat{u} \given \mat{0}, \mat{K}_{\mat{u}\mat{u}}} \diff \mat{u} \\
        &= \Gaussian{\mat{y} \given \mat{0}, \mat{Q}_{\mat{f}\mat{f}} + \Fun{\diag}{\mat{K}_{\mat{f}\mat{f}} - \mat{Q}_{\mat{f}\mat{f}}}},
    \end{split}
\end{align}
dropping the conditioning on $\mat{\theta}$ for notational simplicity.
Due to the conditional independence assumption of the data given the inducing variables, this formulation is called the fully independent training conditional (FITC) and was originally introduced by \citeauthor{snelson_sparse_2005}~\parencite{snelson_sparse_2005,snelson_flexible_2007}.

\begin{figure}[tp]
    \begin{subfigure}{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/spgp_full}
        \caption{Full GP
            \label{fig:sparse_gp:spgp_example:gp}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/spgp_sparse}
        \caption{
            FITC approximation
            \label{fig:sparse_gp:spgp_example:spgp}
        }
    \end{subfigure}
    \caption[FITC example]{
        The black crosses signify data sampled from a noisy sine function.
        \Cref{fig:sparse_gp:spgp_example:gp} shows a full GP trained on the complete data.
        \Cref{fig:sparse_gp:spgp_example:spgp} shows FITC with inducing inputs located at the dart positions.
        Since the inducing variables are marginalized, only their $x$-coordinate is meaningful.
        Three inducing inputs are enough to approximate the full GP with reasonable accuracy.
        \label{fig:sparse_gp:spgp_example}
    }
\end{figure}
This approximate marginal likelihood can be interpreted as the marginal likelihood of a specific GP defined on the original data set $(\mat{X}, \mat{y})$.
In this GP, the original kernel $\K$ is replaced by the kernel $\K_{\text{FITC}}$.
With $\Ind$ denoting the indicator function, it is defined as
\begin{align}
    \begin{split}
        \Qc(\mat{a}, \mat{b}) &\coloneqq \mat{K}_{\mat{a}\mat{u}} \mat{K}_{\mat{u}\mat{u}}\inv\mat{K}_{\mat{u}\mat{b}} \\
        \K_{\text{FITC}}(\mat{a}, \mat{b}) &\coloneqq \Qc(\mat{a}, \mat{b}) + \Indicator{\mat{a} = \mat{b}} \left( \K(\mat{a}, \mat{b}) - \Qc(\mat{a}, \mat{b}) \right).
    \end{split}
\end{align}
This kernel is equal to $\K$ when both arguments are identical and equal to $\Qc$ everywhere else.
For well-chosen inducing inputs, $\mat{Q}_{\mat{f}\mat{f}}$ is a low-rank approximation of $\mat{K}_{\mat{f}\mat{f}}$~\parencite{snelson_flexible_2007}.
The inducing inputs $\mat{Z}$ are hidden in the kernel matrix $\mat{K}_{\mat{u}\mat{u}}$ and are additional hyper-parameters to this kernel.
The predictive posterior $\mat{f}_\ast$ of the test points $\mat{X}_\ast$ is then given by
\begin{align}
    \begin{split}
        \Prob{\mat{f}_\ast \given \mat{X}_\ast, \mat{X}, \mat{y}, \mat{Z}}
        &= \Gaussian*{\mat{f}_\ast \given \mat{\mu}_\ast, \mat{\Sigma}_\ast} \text{, where} \\
        \mat{\mu}_\ast
        &= \mat{Q}_{\ast \mat{f}} \left( \mat{Q}_{\mat{f}\mat{f}} + \diag(\mat{K}_{\mat{f}\mat{f}} - \mat{Q}_{\mat{f}\mat{f}}) + \sigma^2 \Eye \right)\inv \mat{y} \\
        \mat{\Sigma}_\ast
        &= \mat{K}_{\ast\ast} - \mat{Q}_{\ast \mat{f}} \left( \mat{Q}_{\mat{f}\mat{f}} + \diag(\mat{K}_{\mat{f}\mat{f}} - \mat{Q}_{\mat{f}\mat{f}}) + \sigma^2 \Eye \right)\inv \mat{Q}_{\mat{f} \ast}.
    \end{split}
\end{align}
and $\mat{Q}_{\mat{f}\mat{f}} \coloneqq \mat{K}_{\mat{f} \mat{u}} \mat{K}_{\mat{u}\mat{u}}\inv\mat{K}_{\mat{u} \mat{f}}$ obtained by inserting the kernel definition into~\cref{lem:gp:gp_posterior}.
This formulation of the predictive posterior for the FITC approximation still requires the inversion of matrices of size $N \times N$ and therefore does not offer computational improvements.
Using the matrix inversion lemma \cite{petersen_matrix_2008}, they can be rewritten in the form
\begin{align}
    \begin{split}
        \mat{\mu}_\ast
        &= \mat{K}_{\ast \mat{u}}\mat{B}\inv \mat{K}_{\mat{u}\mat{f}}\left(\Fun{\diag}{\mat{K}_{\mat{f}\mat{f}} - \mat{Q}_{\mat{f}\mat{f}}} + \sigma^2\Eye\right)\inv \mat{y} \\
        \mat{\Sigma}_\ast
        &= \mat{K}_{\ast\ast} - \mat{K}_{\ast \mat{u}} \left(\mat{K}_{\mat{u}\mat{u}}\inv - \mat{B}\inv \right) \mat{K}_{\mat{u}\ast} \\
        \mat{B}
        &= \mat{K}_{\mat{u}\mat{u}} + \mat{K}_{\mat{u}\mat{f}} \left(\Fun{\diag}{\mat{K}_{\mat{f}\mat{f}} - \mat{Q}_{\mat{f}\mat{f}}} + \sigma^2\Eye\right)\inv \mat{K}_{\mat{f}\mat{u}},
    \end{split}
\end{align}
which only involves the inversion of $M \times M$ matrices and one diagonal $N \times N$ matrix.
Implemented this way, the calculation of all terms independent of the test points has a complexity of $\Oh(NM^2)$ and predicting individual means and variances takes $\Oh(M)$ and $\Oh(M^2)$ time respectively.
The space requirement also drops to $\Oh(M^2)$.
Since the positions of the inducing inputs $\mat{Z}$ are additional hyper-parameters in $\K_{\text{FITC}}$, they can be chosen together with the hyper-parameters of the original kernel $\mat{\theta}$ using maximum likelihood.
This optimization chooses the positions in such a way that together with appropriate other hyper-parameters, the original data is represented as good as possible.
\Cref{fig:sparse_gp:spgp_example} shows that a surprisingly small number of inducing inputs can be enough to represent the dynamics of a function.

However, with a large number of inducing inputs, the number of hyper-parameters can grow large as well.
This implies the danger of overfitting since the altered Gaussian process has no direct connection to the original Gaussian process over the complete training set.
It is also not clear what properties a set of training inputs $\mat{Z}$ must fulfill such that it recovers an original GP well.
To address these issues, a variational formulation of sparse GPs using inducing observations formulated originally in~\parencite{titsias_variational_2009,hensman_gaussian_2013} is discussed next.

\section{Variational sparse Gaussian process approximations}
\label{toc:gp:sparse_gps}
\Textcite{titsias_variational_2009} introduced a variational interpretation of sparse GPs with inducing observations $(\mat{Z}, \mat{u})$.
Similar to the FITC approximation discussed above, inducing observations are assumed to be generated from the latent function $\mat{u} = f(\mat{Z})$.
Due to the consistency of GPs, the true data and inducing data are jointly Gaussian.
\begin{align}
    \label{eq:gp:augmented_model}
    \Prob{\mat{f}, \mat{u}} & = \Gaussian*{\begin{pmatrix} \mat{f} \\ \mat{u}\end{pmatrix} \given \mat{0}, \begin{pmatrix} \mat{K_{ff}} & \mat{K_{fu}} \\ \mat{K_{uf}} & \mat{K_{uu}} \end{pmatrix}}
\end{align}
Instead of defining a new GP on the inducing data, we want to choose the $M$ inducing locations such that the original GP defined on the $N$ true data points is approximated as closely as possible.

More formally, we consider the predictive posterior of the augmented GP containing both the true and inducing data given by
\begin{align}
    \Prob*{\mat{f^\ast} \given \mat{y}}
     & = \int \Prob*{\mat{f^\ast} \given \mat{f}, \mat{u}} \Prob*{\mat{f}, \mat{u} \given \mat{y}} \diff \mat{f} \diff \mat{u},
\end{align}
where we drop the conditioning on $\mat{X}$ and $\mat{Z}$ for notational simplicity.
We adopt this convention for the rest of this chapter.
The inducing observations are optimal if $\mat{f^\ast}$ and $\mat{f}$ are independent given $\mat{u}$ in
\begin{align}
    \begin{split}
        \Prob*{\mat{f^\ast} \given \mat{f}, \mat{u}}
        & = \Prob*{\mat{f^\ast} \given \mat{u}}                           \\
        \Prob*{\mat{f}, \mat{u} \given \mat{y}}
        & = \Prob*{\mat{f} \given \mat{u}} \Prob*{\mat{u} \given \mat{y}}.
    \end{split}
\end{align}
In this case, $\mat{u}$ is said to be a sufficient statistic for $\mat{f}$, capturing all information contained in the latter.
In practice, finding $(\mat{Z}, \mat{u})$ that are indeed a sufficient statistic for $\mat{f}$ is hard.
We will approximate this situation with a variational distribution $\Variat*{\mat{f}, \mat{u}}$, thereby formulating a variational approximation to the original GP.
Due to the joint Gaussian distribution in~\cref{eq:gp:augmented_model}, it is convenient to consider the factorization
\begin{align}
    \Variat*{\mat{f}, \mat{u}}
     & = \Prob*{\mat{f} \given \mat{u}} \Variat*{\mat{u}},
\end{align}
where $\Prob*{\mat{f} \given \mat{u}}$ is a standard Gaussian conditional.
Assuming $\mat{u}$ is indeed a sufficient statistic for $\mat{f}$, the variational predictive posterior reduces to
\begin{align}
    \begin{split}
        \label{eq:gp:sgpr_predictive_posterior}
        \Variat*{\mat{f^\ast}}
        & = \int \Prob*{\mat{f^\ast} \given \mat{f}, \mat{u}} \Variat*{\mat{f}, \mat{u}} \diff \mat{f} \diff \mat{u}              \\
        & = \int \Prob*{\mat{f^\ast} \given \mat{u}} \Prob*{\mat{f} \given \mat{u}} \Variat*{\mat{u}} \diff \mat{f} \diff \mat{u} \\
        & = \int \Prob*{\mat{f^\ast} \given \mat{u}} \Variat*{\mat{u}} \diff \mat{u}.
    \end{split}
\end{align}
To formulate a variational lower bound on the original marginal likelihood $\Lc^{\text{GP}}$ in~\cref{eq:gp:gp_marginal_likelihood}, we have to decide how to choose $\mat{Z}$ and $\Variat*{\mat{u}}$ such that as much information of $\mat{f}$ is captured as possible.
We will discuss two approaches.
First, we derive the optimal choice of $\Variat*{\mat{u}}$ given a set of inducing inputs $\mat{Z}$.
Because calculating this optimal choice is computationally expensive, we will then show how to improve performance through further approximation.

\subsubsection{Optimal inducing outputs}
We assume a free-form variational distribution $\Variat*{\mat{u}}$ to derive the variational lower bound to the likelihood $\Lc^{\text{SGPR}}$ of the augmented model.
\begin{align}
    \begin{split}
        \Lc^{\text{SGPR}}(\mat{\theta}, \mat{Z}, \Variat*{\mat{u}})
        & = -\log \Prob*{\mat{y} \given \mat{\theta}, \mat{Z}, \Variat*{\mat{u}}}                                                                                                                                                       \\
        & = -\log \int \Prob*{\mat{y} \given \mat{f}} \Prob*{\mat{f} \given \mat{u}} \Prob*{\mat{u}} \diff \mat{f} \diff \mat{u}                                                                                                        \\
        & = -\log \int \Variat*{\mat{f}, \mat{u}} \frac{\Prob*{\mat{y} \given \mat{f}} \Prob*{\mat{f} \given \mat{u}} \Prob*{\mat{u}}}{\Variat*{\mat{f}, \mat{u}}} \diff \mat{f} \diff \mat{u}.
    \end{split}
\end{align}
We bound the likelihood using Jensen's inequality~\parencite{bishop_christoph_pattern_2007} which states that for convex functions $f$ and integrable functions $g$ it holds that
\begin{align}
    \begin{split}
        \label{eq:jensens_inequality}
        \Fun*{f\,}{\int g(x) \diff x} \leq \int f(g(x)) \diff x.
    \end{split}
\end{align}
Since the natural logarithm is concave we have
\begin{align}
    \begin{split}
        \label{eq:gp:sgpr_bound_1}
        \Lc^{\text{SGPR}}(\mat{\theta}, \mat{Z}, \Variat*{\mat{u}})
        & \geq -\int \Variat*{\mat{f}, \mat{u}} \log\frac{\Prob*{\mat{y} \given \mat{f}} \Prob*{\mat{f} \given \mat{u}} \Prob*{\mat{u}}}{\Variat*{\mat{f}, \mat{u}}} \diff \mat{f} \diff \mat{u}                                        \\
        & = -\int \Prob*{\mat{f} \given \mat{u}}\Variat*{\mat{u}} \log\frac{\Prob*{\mat{y} \given \mat{f}} \Prob*{\mat{f} \given \mat{u}} \Prob*{\mat{u}}}{\Prob*{\mat{f} \given \mat{u}}\Variat*{\mat{u}}} \diff \mat{f} \diff \mat{u} \\
        & = -\int \Prob*{\mat{f} \given \mat{u}}\Variat*{\mat{u}} \log\frac{\Prob*{\mat{y} \given \mat{f}} \Prob*{\mat{u}}}{\Variat*{\mat{u}}} \diff \mat{f} \diff \mat{u}                                                              \\
        & = -\int \Variat*{\mat{u}} \left( \int\Prob*{\mat{f} \given \mat{u}} \log\Prob*{\mat{y} \given \mat{f}} \diff \mat{f} + \log\frac{\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \right) \diff \mat{u} \\
        & = -\int \Variat*{\mat{u}} \left( \Moment*{\E_{\Prob*{\mat{f} \given \mat{u}}}}{\log \Prob*{\mat{y} \given \mat{f}}} + \log\frac{\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \right) \diff \mat{u},
    \end{split}
\end{align}
dropping the conditioning on $\mat{\theta}$ and $\mat{Z}$.
We assume a Gaussian likelihood $\Prob*{\mat{y} \given \mat{f}} = \Gaussian{\mat{y} \given \mat{f}, \sigma_n^2\Eye}$.
The expectation of the log-likelihood can be evaluated analytically~\parencite{petersen_matrix_2008} and is given by
\begin{align}
    \begin{split}
        \label{eq:gp:expected_log_likelihood}
        \Moment*{\E_{\Prob*{\mat{f} \given \mat{u}}}}{\log \Prob*{\mat{y} \given \mat{f}}}
        &= \int \Prob*{\mat{f} \given \mat{u}} \log\Prob*{\mat{y} \given \mat{f}} \diff \mat{f} \\
        &= \int \log \Gaussian*{\mat{y} \given \mat{f}, \sigma_n^2 \Eye} \Gaussian*{\mat{f} \given \mat{K_{fu}} \mat{K_{uu}}\inv \mat{u}, \mat{K_{ff}} - \mat{Q_{ff}}} \diff \mat{f} \\
        &= \log \Gaussian*{\mat{y} \given \mat{K_{fu}} \mat{K_{uu}}\inv \mat{u}, \sigma_n^2\Eye} - \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &= \log G(\mat{u}) - \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}},
    \end{split}
\end{align}
where we set $G(\mat{u}) = \Gaussian*{\mat{y} \given \mat{K_{fu}} \mat{K_{uu}}\inv \mat{u}, \sigma_n^2\Eye}$, remember $\mat{Q_{ff}} = \mat{K_{fu}} \mat{K_{uu}}\inv \mat{K_{uf}}$ and $\tr(\mat{M})$ denotes the trace of the matrix $\mat{M}$.
Inserting~\cref{eq:gp:expected_log_likelihood} into~\cref{eq:gp:sgpr_bound_1} yields
\begin{align}
    \begin{split}
        \label{eq:gp:sgpr_bound_2}
        \Lc^{\text{SGPR}}(\mat{\theta}, \mat{Z}, \Variat*{\mat{u}})
        &\geq -\int \Variat*{\mat{u}} \left( \Moment*{\E_{\Prob*{\mat{f} \given \mat{u}}}}{\log \Prob*{\mat{y} \given \mat{f}}} + \log\frac{\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \right) \diff \mat{u} \\
        &= -\int \Variat*{\mat{u}} \left( \log G(\mat{u}) - \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} + \log\frac{\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \right) \diff \mat{u} \\
        &= -\int \Variat*{\mat{u}} \log\frac{G(\mat{u})\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \diff \mat{u} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}}.
    \end{split}
\end{align}

The distribution $\Variat*{\mat{u}}$ is chosen optimally if it maximizes the variational bound $\Lc^{\text{SGPR}}$ in
\begin{align}
    \begin{split}
        \MoveEqLeft\argmax_{\Variat*{\mat{u}}}\Lc^{\text{SGPR}}(\mat{\theta}, \mat{Z}, \Variat*{\mat{u}})                                                                                             \\
        & = \argmax_{\Variat*{\mat{u}}} -\int \Variat*{\mat{u}} \log\frac{G(\mat{u})\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \diff \mat{u} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        & = \argmax_{\Variat*{\mat{u}}} -\int \Variat*{\mat{u}} \log\frac{G(\mat{u})\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \diff \mat{u}                                                                \\
        & = \argmin_{\Variat*{\mat{u}}} \KL*{\Variat*{\mat{u}}}{G(\mat{u})\Prob*{\mat{u}}},
    \end{split}
\end{align}
where $\KL{\q)}{\p}$ denotes the Kullback-Leibler divergence.
This divergence is minimized if $\q$ and $\p$ are proportional and $G(\mat{u})\Prob*{\mat{u}}$ is a product of Gaussians
\begin{align}
    \q^\ast(\mat{u}) \propto G(\mat{u}) \Prob*{\mat{u}}
     & = \Gaussian*{\mat{y} \given \mat{K_{fu}} \mat{K_{uu}}\inv \mat{u}, \sigma_n^2\Eye}\Gaussian*{\mat{u} \given \mat{0}, \mat{K_{uu}}},
\end{align}
where $\Prob*{\mat{u}}$ is due to the GP prior in~\cref{eq:gp:augmented_model}.
Since the product of two Gaussian densities is an un-normalized Gaussian density, the optimal choice $\q^\ast(\mat{u})$ is a Gaussian given by
\begin{align}
    \begin{split}
        \label{eq:gp:sgpr_optimal_q}
        \q^\ast(\mat{u})
        & = \frac{G(\mat{u}) \Prob*{\mat{u}}}{\int G(\mat{u}) \Prob*{\mat{u}} \diff \mat{u}}
        = \Gaussian*{\mat{u} \given \mat{\mu}_u, \mat{\Sigma}_u}\text{, with}                 \\[\smallskipamount]
        \mat{\mu}_u
        & = \sigma_n^{-2} \mat{K_{uu}}\mat{B}\inv \mat{K_{uf}}\mat{y}                        \\
        \mat{\Sigma}_u
        & = \mat{K_{uu}}\mat{B}\inv\mat{K_{uu}}                                              \\[\smallskipamount]
        \mat{B}
        & = \mat{K_{uu}} + \sigma_n^{-2} \mat{K_{uf}}\mat{K_{fu}}.
    \end{split}
\end{align}
Inserting $\q^\ast$ into~\cref{eq:gp:sgpr_bound_2} yields the final bound
\begin{align}
    \begin{split}
        \label{eq:gp:sgpr_bound}
        \Lc^{\text{SGPR}}(\mat{\theta}, \mat{Z})
        &\geq \Lc^{\text{SGPR}}(\mat{\theta}, \mat{Z}, \q^\ast(\mat{u})) \\
        &= -\int \q^\ast(\mat{u}) \log\frac{G(\mat{u})\Prob*{\mat{u}}}{\q^\ast(\mat{u})} \diff \mat{u} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &= -\int \frac{G(\mat{u}) \Prob*{\mat{u}}}{\int G(\mat{u}) \Prob*{\mat{u}} \diff \mat{u}} \log\frac{G(\mat{u})\Prob*{\mat{u}}}{\frac{G(\mat{u}) \Prob*{\mat{u}}}{\int G(\mat{u}) \Prob*{\mat{u}} \diff \mat{u}}} \diff \mat{u} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &= -\log \int G(\mat{u}) \Prob*{\mat{u}} \diff \mat{u} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &= -\log \Gaussian*{\mat{y} \given \mat{0}, \mat{Q_{ff}} + \sigma_n^2\Eye} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}},
    \end{split}
\end{align}
which can be maximized to jointly find hyper-parameters $\mat{\theta}$ and the variational parameters $\mat{Z}$.
Evaluating this bound takes $\Oh(NM^2)$ time.
In contrast to the FITC approximation where inducing inputs are additional hyper-parameters to the model which can lead to overfitting, adding additional inducing inputs $\mat{Z}$ to the variational model only improves the approximation to the full GP.

\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \begin{tikzpicture}[x=4em, y=5em]
            \node[random variable, observed] at (0, 0) (X) {$\mat{x}_n$};
            \node[random variable, latent] at (0, -1) (F) {$\mat{f}_n$};
            \node[random variable, hyperparameter] at (-1, -1) (theta) {$\mat{\theta}$};
            \node[random variable, variational] at (1, -0.75) (Z) {$\mat{Z}$};
            \node[random variable, variational] at (1, -1.25) (u) {$\mat{u}^\ast$};
            \node[random variable, observed] at (0, -2) (Y) {$\mat{y}_n$};
            \draw[edge, directed] (X) -- (F);
            \draw[edge, directed] (F) -- (Y);
            \draw[edge, directed] (theta) -- (F);
            \draw[edge, directed] (Z) -- (F);
            \draw[edge, directed] (u) to (F);
            \draw[edge, directed] (Y) to (u);
            %
            \begin{scope}[on background layer]
                \node[
                    align plate,
                    inner xsep=10pt,
                    fit=(X)(F)(Y),
                    label={[anchor=south east]south east:$N$}
                ] {};
            \end{scope}
        \end{tikzpicture}
        \caption{
            SGPR
            \label{fig:gp:sparse_graphical_model:sgpr}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \begin{tikzpicture}[x=4em, y=5em]
            \node[random variable, observed] at (0, 0) (X) {$\mat{x}_n$};
            \node[random variable, latent] at (0, -1) (F) {$\mat{f}_n$};
            \node[random variable, hyperparameter] at (-1, -1) (theta) {$\mat{\theta}$};
            \node[random variable, variational] at (1, -0.75) (Z) {$\mat{Z}$};
            \node[random variable, variational] at (1, -1.25) (u) {$\mat{u}$};
            \node[random variable, observed] at (0, -2) (Y) {$\mat{y}_n$};
            \draw[edge, directed] (X) -- (F);
            \draw[edge, directed] (F) -- (Y);
            \draw[edge, directed] (theta) -- (F);
            \draw[edge, directed] (Z) -- (F);
            \draw[edge, directed] (u) -- (F);
            %
            \begin{scope}[on background layer]
                \node[
                    align plate,
                    inner xsep=10pt,
                    fit=(X)(F)(Y),
                    label={[anchor=south east]south east:$N$}
                ] {};
            \end{scope}
        \end{tikzpicture}
        \caption{
            SVGP
            \label{fig:gp:sparse_graphical_model:svgp}
        }
    \end{subfigure}
    \caption{
        Sparse GP graphical model.
        \label{fig:gp:sparse_graphical_model}
    }
\end{figure}
\Cref{fig:gp:sparse_graphical_model:sgpr} shows the graphical model for the variational GP approximation using the optimal $\q^\ast(\mat{u})$ distribution.
The inducing inputs
\tikz[x=2.5em,baseline=(f.base)]{
    \node[inline random variable, variational] (f) {$\mat{Z}$};
}
are referred to as variational parameters.
While the SGPR model is more computationally efficient, the interdependencies between different observations are not completely removed.
The optimal choice for $\q^\ast(\mat{u})$ depends on the observations in~\cref{eq:gp:sgpr_optimal_q}.
Because of this, the bound does not factorize along the data, preventing stochastic variational inference techniques~\parencite{hensman_gaussian_2013} and requiring the bound to be evaluated jointly for all data points.
Additionally, evaluating the predictive posterior~\cref{eq:gp:sgpr_predictive_posterior} takes $\Oh(M^3 + MN)$ time.
For large $N$, linear growth with the amount of observations can be prohibitive.
We will now consider the SVGP model introduced by~\textcite{hensman_gaussian_2013} which avoids this growth.

\subsubsection{Approximate inducing outputs}
\begin{figure}[p]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/variational_gp_spgp}
        \caption{
            spgp
            \label{fig:gp:variational:spgp}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/variational_gp_spgp_huge}
        \caption{
            spgp huge
            \label{fig:gp:variational:spgp_huge}
        }
    \end{subfigure}\\[\figureskip]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/variational_gp_sgpr}
        \caption{
            SGPR
            \label{fig:gp:variational:sgpr}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/variational_gp_sgpr_huge}
        \caption{
            SGPR huge
            \label{fig:gp:variational:sgpr_huge}
        }
    \end{subfigure}\\[\figureskip]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/variational_gp_svgp}
        \caption{
            SVGP
            \label{fig:gp:variational:svgp}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/variational_gp_svgp_huge}
        \caption{
            SVGP huge
            \label{fig:gp:variational:svgp_huge}
        }
    \end{subfigure}
    \caption[Variational GP posteriors]{
        Variational GP posteriors
        \label{fig:gp:variational}
    }
\end{figure}
In the SGPR approximation, a linear dependency on the number of points in the true data set is introduced through $\q^\ast(\mat{u})$.
We have shown in~\cref{eq:gp:sgpr_optimal_q} that this optimal choice is a Gaussian.
Instead of using $\q^\ast(\mat{u})$, the SVGP model shown in~\cref{fig:gp:sparse_graphical_model:svgp} uses a free-form Gaussian $\Variat*{\mat{u}} = \Gaussian*{\mat{u} \given \mat{m}, \mat{S}}$.
The $\Oh(M^2)$ variational parameters in $\mat{m}$ and $\mat{S}$ are optimized jointly with $\mat{Z}$ in a variational bound.
Poor choices of $\mat{m}$ and $\mat{S}$ can only worsen the variational approximation compared to the optimal choice in $\Lc^{\text{SGPR}}$ but does not alter the model.

To derive the variational bound $\Lc^{\text{SVGP}}$, we start with the bound in~\cref{eq:gp:sgpr_bound_2}.
Instead of inserting the optimal $\q^\ast(\mat{u})$, we reformulate the bound to recover an expectation and KL-divergence in
\begin{align}
    \begin{split}
        \label{eq:gp:svgp_bound_1}
        \MoveEqLeft\Lc^{\text{SVGP}}(\mat{\theta}, \mat{Z}, \mat{m}, \mat{S}) \\
        &\geq -\int \Variat*{\mat{u}} \log\frac{G(\mat{u})\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \diff \mat{u} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &= -\int \Variat*{\mat{u}} \log\frac{G(\mat{u})\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \diff \mat{u} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &= -\int \log G(\mat{u}) \Variat*{\mat{u}} \diff \mat{u} + \KL{\Variat*{\mat{u}}}{\Prob*{\mat{u}}} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &= -\Moment*{\E_{\Variat*{\mat{u}}}}{\log G(\mat{u})} + \KL{\Variat*{\mat{u}}}{\Prob*{\mat{u}}} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}}.
    \end{split}
\end{align}
Similar to~\cref{eq:gp:expected_log_likelihood}, this expectation can be evaluated analytically and is given by
\begin{align}
    \begin{split}
        \label{eq:gp:expected_log_likelihood_svgp}
        \Moment*{\E_{\Variat*{\mat{u}}}}{\log G(\mat{u})}
        &= \int \log G(\mat{u}) \Variat*{\mat{u}} \diff \mat{u} \\
        &= \int \log \Gaussian*{\mat{y} \given \mat{K_{fu}} \mat{K_{uu}}\inv \mat{u}, \sigma_n^2\Eye} \Gaussian*{\mat{u} \given \mat{m}, \mat{S}} \diff \mat{u} \\
        &= \log \Gaussian*{\mat{y} \given \mat{K_{fu}}\mat{K_{uu}}\inv \mat{m}, \sigma_n^2 \Eye} - \frac{1}{2\sigma_n^2} \Fun*{\tr}{\mat{K_{fu}}\mat{K_{uu}}\inv \mat{S}\mat{K_{uu}}\inv \mat{K_{uf}}}.
    \end{split}
\end{align}
Inserting~\cref{eq:gp:expected_log_likelihood_svgp} into~\cref{eq:gp:svgp_bound_1} yields the final bound for the SVGP model given by
\begin{align}
    \begin{split}
        \label{eq:gp:svgp_bound_2}
        \MoveEqLeft\Lc^{\text{SVGP}}(\mat{\theta}, \mat{Z}, \mat{m}, \mat{S}) \\
        &\geq -\Moment*{\E_{\Variat*{\mat{u}}}}{\log G(\mat{u})} + \KL{\Variat*{\mat{u}}}{\Prob*{\mat{u}}} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &=
        -\log \Gaussian*{\mat{y} \given \mat{K_{fu}}\mat{K_{uu}}\inv \mat{m}, \sigma_n^2 \Eye}
        + \KL{\Variat*{\mat{u}}}{\Prob*{\mat{u}}} \\
        &\quad + \frac{1}{2\sigma_n^2} \Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}}
        + \frac{1}{2\sigma_n^2} \Fun*{\tr}{\mat{K_{fu}}\mat{K_{uu}}\inv \mat{S}\mat{K_{uu}}\inv \mat{K_{uf}}}.
    \end{split}
\end{align}
The KL-divergence is a divergence of two Gaussians which can be evaluated analytically.
While evaluating the complete bound still takes $\Oh(NM^2)$, the data-likelihood term is given by a diagonal Gaussian which factorizes along the data.
This enables stochastic optimization techniques like mini-batching, greatly increasing the scalability of the variational GP approximation to large data sets~\parencite{hensman_gaussian_2013}.
Similarly, evaluating the predictive posterior~\cref{eq:gp:sgpr_predictive_posterior} now takes $\Oh(M^3)$ time, removing the dependency on the training data completely.
\cref{fig:gp:variational} compares predictive posteriors for the FITC, SGPR and SVGP approximations.
While the approximations show similar results with small amounts of inducing inputs, both SGPR and SVGP converge to the original GP for large $M$ while the FITC approximation does not.
Because the SGPR approximation does not explicitly represent inducing outputs, only the input locations are meaningful, similar to the FITC approximation.
SVGP directly infers a posterior about latent function values, which are shown in the figure.

The $\Oh(M^3)$ computational cost due to the inversion of $\mat{K_{uu}}$ can still be prohibitive for models with a large number of inducing points.
To reduce the number of required points, recent work explored how Bayesian inference can be added to the search for good inducing point locations~\parencite{hensman_mcmc_2015,rossi_rethinking_2020}.
There have also been multiple extensions to the variational inducing point approach to further reduce computational cost.
One approach is to impose grid structure on the inducing inputs $\mat{Z}$~\parencite{wilson_kernel_2015}.
Instead of optimizing their location, the position of a large number of inputs is fixed to perform fast computations exploiting the structure.
While this approach suffers from the curse of dimensionality, it can increase performance for low input dimensionalities.
Another approach is to orthogonally decouple the computations for predictive means and variances~\parencite{shi_sparse_2020,salimbeni_orthogonally_2018,cheng_variational_2017}.
This decoupling allows the calculation of the predictive mean in linear time, allowing for a larger number of inducing points for the mean.
\todo{Let's move to deep GPs.}


\section{Hierarchical Gaussian Processes}
\label{toc:dgp}
\begin{figure}[t]
    \centering
    \begin{tikzpicture}[x=4em, y=5em]
        \node[random variable, observed] at (0, 0) (X) {$\mat{x}_n$};
        %
        \node[random variable, latent] at (0, -1) (F1) {$\mat{f}_{n,1}$};
        \node[random variable, variational] at (1, -1) (u1) {$\mat{u}_1$};
        %
        \node[random variable, latent] at (0, -2) (Fl) {$\mat{f}_{n,l}$};
        \node[random variable, variational] at (1, -2) (ul) {$\mat{u}_l$};
        %
        \node[random variable, hyperparameter] at (-1, -1.5) (theta) {$\mat{\theta}$};
        \node[random variable, observed] at (0, -3) (Y) {$\mat{y}_n$};
        \draw[edge, directed] (X) -- (F1);
        %
        \draw[edge, directed] (theta) |- (F1);
        \draw[edge, directed] (u1) -- (F1);
        %
        \draw[edge, directed, densely dashed] (F1) -- (Fl);
        %
        \draw[edge, directed] (theta) |- (Fl);
        \draw[edge, directed] (ul) -- (Fl);
        %
        \draw[edge, directed] (Fl) -- (Y);
        %
        \begin{scope}[on background layer]
            \node[
                align plate,
                inner xsep=10pt,
                fit=(X)(Y),
                label={[anchor=south east]south east:$N$}
            ] {};
        \end{scope}
    \end{tikzpicture}
    \caption{
        Deep GP
        \label{fig:dgp:graphical_model}
    }
\end{figure}
\begin{align}
    \begin{split}
        \mat{y} = f_L(f_{L-1}(\dots(f_1(\mat{X})))) + \epsilon
    \end{split}
\end{align}
The joint probability distribution of the data can then be written as
\begin{align}
    \begin{split}
        \label{eq:dgp:full_model}
        \Prob{\mat{y}, \mat{f}_1, \dots, \mat{f}_L \given \mat{X}}
        &= \Prob*{\mat{y} \given \mat{f}_L} \prod_{l=1}^L \Prob*{\mat{f}_l \given \mat{f}_{l-1}}\text{, with} \\
        \mat{y} \mid \mat{f}_L &\sim \Gaussian{\mat{f}_L, \sigma^2_n \Eye} \\
        \mat{f}_l \mid \mat{f}_{l-1} &\sim \Gaussian{\mat{0}, \mat{K}_{\mat{f}_l\mat{f}_l} + \sigma^2_l \Eye} \\
        \mat{f}_0 &= \mat{X}.
    \end{split}
\end{align}
We will denote $\mat{K}_{ll} = \mat{K}_{\mat{f}_l\mat{f}_l}$.

\subsubsection{Nested Variational Compression}
To derive the desired variational lower bound for the log marginal likelihood of the complete model, multiple steps are necessary.
First, we will consider the innermost GPs $a_d$ describing the alignment functions.
We derive the Scalable Variational GP (SVGP), a lower bound for this model part which can be calculated efficiently and can be used for stochastic optimization, first introduced by \textcite{hensman_gaussian_2013}.
In order to apply this bound recursively, we will both show how to propagate the uncertainty through the subsequent layers $f_d$ and $g_d$ and how to avoid the inter-layer cross-dependencies using another variational approximation as presented by \textcite{hensman_nested_2014}.
While \citeauthor{hensman_nested_2014} considered standard deep GP models, we will show how to apply their results to CPs.

Our next goal is to derive a bound on the outputs of the second layer
\begin{align}
    \begin{split}
        \log \Prob{\mat{f}_2 \given \mat{u}_2}
        &= \log \int \Prob{\mat{f}_2, \mat{f}_1, \mat{u}_1 \given \mat{u}_2} \diff \mat{f}_1 \diff \mat{u}_1 \\
        &= \log \int \Prob{\mat{f}_2, \given \mat{u}_2, \mat{f}_1, \mat{u}_1} \Prob*{\mat{f}_1, \mat{u}_1} \diff \mat{f}_1 \diff \mat{u}_1,
    \end{split}
\end{align}
that is, an expression in which the uncertainty about the different $\mat{a_d}$  and the cross-layer dependencies on the $\mat{u_{a, d}}$ are both marginalized.
While on the first layer, the different $\mat{a_d}$ are conditionally independent, the second layer explicitly models the cross-covariances between the different outputs via convolutions over the shared latent processes $w_r$.
We will therefore need to handle all of the different $\mat{f_d}$, together denoted as $\mat{f}$, at the same time.

We start by considering the relevant terms from \cref{eq:dgp:full_model} and apply \cref{eq:gp:expected_log_likelihood_svgp} to marginalize $\mat{f}_1$ in
\begin{align}
    \begin{split}
        \log\Prob{\mat{f}_2 \given \mat{u}_2, \mat{u}_1}
        &= \log\int\Prob{\mat{f}_2, \mat{f}_1 \given \mat{u}_2, \mat{u}_1}\diff\mat{f}_1 \\
        &\geq \log\int \aProb{\mat{f}_2 \given \mat{u}_2, \mat{f}_1} \aProb{\mat{f}_1 \given \mat{u}_1} \\
        &\qquad\qquad \cdot \Fun*{\exp}{-\frac{1}{2\sigma_1^2} \Fun*{\tr}{\mat{K}_{11} - \mat{Q}_{11}} - \frac{1}{2\sigma_2^2} \Fun*{\tr}{\mat{K}_{22} - \mat{Q}_{22}}} \diff \mat{f}_1 \\
        &\geq \Moment{\E_{\aProb{\mat{f}_1 \given \mat{u}_1}}}{\log \aProb{\mat{f}_2 \given \mat{u}_2, \mat{f}_1}} \\
        &\qquad\qquad - \Moment*{\E_{\aProb{\mat{f}_1 \given \mat{u}_1}}}{\frac{1}{2\sigma_2^2} \Fun*{\tr}{\mat{K}_{22} - \mat{Q}_{22}}}
        - \frac{1}{2\sigma_1^2} \Fun*{\tr}{\mat{K}_{11} - \mat{Q}_{11}},
    \end{split}
\end{align}
where we write $\aProb{\mat{f}_1 \given \mat{u}_1} = \Gaussian*{\mat{f}_1 \given \mat{\mu}_1, \sigma_1^2 \Eye}$\todo{Fix $\ptilde$} to incorporate the Gaussian noise in the latent space.
Due to our assumption that $\mat{u}_1$ is a sufficient statistic for $\mat{f}_1$ we choose
\begin{align}
    \label{eq:var_compression:variational_assumption}
    \begin{split}
        \Variat{\mat{f}_1 \given \mat{u}_1}
        &= \aProb{\mat{f}_1 \given \mat{u}_1}\text{, and}\\
        \Variat{\mat{f}_1} &= \int \aProb{\mat{f}_1 \given \mat{u}_1} \Variat{\mat{u}_1} \diff \mat{u}_1,
    \end{split}
\end{align}
and use another variational approximation to marginalize $\mat{u}_1$.
This yields
\begin{align}
    \begin{split}
        \label{eq:var_compression:f_marginal_likelihood}
        \log \Prob{\mat{f}_2 \given \mat{u}_2}
        &= \log \int \Prob{\mat{f}_2, \mat{u}_1 \given \mat{u}_2} \diff \mat{u}_1 \\
        &= \log \int \Prob{\mat{f}_2 \given \mat{u}_2, \mat{u}_1} \Prob{\mat{u}_1} \diff \mat{u}_1 \\
        &\geq \int \Variat{\mat{u}_1} \log\frac{\Prob{\mat{f}_2 \given \mat{u}_2, \mat{u}_1} \Prob{\mat{u}_1}}{\Variat{\mat{u}_1}} \diff \mat{u}_1 \\
        &= \Moment*{\E_{\Variat{\mat{u}_1}}}{\log \Prob{\mat{f}_2 \given \mat{u}_1, \mat{u}_2}}
        - \KL{\Variat{\mat{u}_1}}{\Prob{\mat{u}_1}} \\
        &\geq \Moment*{\E_{\Variat{\mat{u}_1}}}{\Moment*{\E_{\aProb{\mat{f}_1 \given \mat{u}_1}}}{\log \aProb{\mat{f}_2 \given \mat{u}_2, \mat{f}_1}}}
        - \KL{\Variat{\mat{u}_1}}{\Prob{\mat{u}_1}} \\
        &\quad {} - \frac{1}{2\sigma_1^2} \Fun*{\tr}{\mat{K}_{11} - \mat{Q}_{11}}
        - \Moment*{\E_{\Variat{\mat{u}_1}}}{\Moment*{\E_{\aProb{\mat{f}_1 \given \mat{u}_1}}}{\frac{1}{2\sigma_2^2} \Fun*{\tr}{\mat{K}_{22} - \mat{Q}_{22}}}} \\
        &\geq \Moment*{\E_{\Variat{\mat{f}_1}}}{\log \aProb{\mat{f}_2 \given \mat{u}_2, \mat{f}_1}},
        - \KL{\Variat{\mat{u}_1}}{\Prob{\mat{u}_1}} \\
        &\quad {} - \frac{1}{2\sigma_1^2} \Fun*{\tr}{\mat{K}_{11} - \mat{Q}_{11}}
        - \frac{1}{2\sigma_2^2} \Moment*{\E_{\Variat{\mat{f}_1}}}{\Fun*{\tr}{\mat{K}_{22} - \mat{Q}_{22}}},
    \end{split}
\end{align}
where we apply Fubini's theorem to exchange the order of integration in the expected values.
The expectations with respect to $\Variat{\mat{f}_1}$ involve expectations of kernel matrices, also called $\Psi$-statistics, in the same way as in \parencites{damianou_deep_2013} and are given by
\begin{align}
    \begin{split}
        \label{eq:var_compression:psi_statistics}
        \psi_2 &= \Moment*{\E_{\Variat{\mat{f}_1}}}{\Fun*{\tr}{\mat{K}_{\mat{f}_2 \mat{f}_2}}}, \\
        \mat{\Psi_2} &= \Moment*{\E_{\Variat{\mat{f}_1}}}{\mat{K}_{\mat{f}_2 \mat{u}_2}}, \\
        \mat{\Phi_2} &= \Moment*{\E_{\Variat{\mat{f}_1}}}{\mat{K}_{\mat{u}_2 \mat{f}_2}\mat{K}_{\mat{f}_2 \mat{u}_2}}. \\
    \end{split}
\end{align}
These $\Psi$-statistics can be computed analytically for multiple kernels, including the squared exponential kernel.
In \cref{toc:var_compression:kernel_expectations} we show closed-form solutions for these $\Psi$-statistics for the implicit kernel defined in the CP layer.
To obtain the final formulation of the desired bound for $\log \Prob{\mat{f}_2 \given \mat{u}_2}$ we substitute \cref{eq:var_compression:psi_statistics} into \cref{eq:var_compression:f_marginal_likelihood} and get the analytically tractable bound
\begin{align}
    \begin{split}
        \MoveEqLeft \log \Prob{\mat{f}_2 \given \mat{u}_2} \geq \\
        &\log\Gaussian*{\mat{f}_2 \given \mat{\Psi}_2\mat{K}_{\mat{u}_2\mat{u}_2}\inv \mat{m}_2, \sigma_2^2\Eye}
        - \KL{\Variat{\mat{u}_1}}{\Prob{\mat{u}_1}} \\
        &- \frac{1}{2\sigma_1^2} \Fun*{\tr}{\mat{K}_{11} - \mat{Q}_{11}}
        - \frac{1}{2\sigma_2^2} \left( \psi_2 - \Fun*{\tr}{\mat{\Psi_2}\mat{K}_{\mat{u}_2\mat{u}_2}\inv} \right) \\
        &- \frac{1}{2\sigma_2^2} \tr\left(\left(\mat{\Phi}_2 - \mat{\Psi}_2\tran\mat{\Psi}_2\right) \mat{K}_{\mat{u}_2\mat{u}_2}\inv \left(\mat{m}_2\mat{m}_2\tran + \mat{S}_2\right)\mat{K}_{\mat{u}_2\mat{u}_2}\inv\right)
    \end{split}
\end{align}
To derive a bound for $\log \Prob{\mat{y} \given \mat{X}}$ we apply the same steps as described above, resulting in the final bound, which factorizes along the data, allowing for stochastic optimization methods:
Recursive bound.
\begin{align}
    \begin{split}
        \label{eq:var_compression:full_bound}
        \MoveEqLeft \Lc_{\text{NVC}} = \log \Prob{\mat{y} \given \mat{X}} \geq \\
        &\log\Gaussian*{\mat{y} \given \mat{\Psi}_L\mat{K}_{\mat{u}_L\mat{u}_L}\inv \mat{m}_L, \sigma_n^2\Eye}
        - \sum_{l=1}^L \KL{\Variat{\mat{u}_l}}{\Prob{\mat{u}_l}} \\
        &- \frac{1}{2\sigma_1^2} \Fun*{\tr}{\mat{K}_{11} - \mat{Q}_{11}}
        - \sum_{l=2}^L \frac{1}{2\sigma_l^2} \left( \psi_l - \Fun*{\tr}{\mat{\Psi_l}\mat{K}_{\mat{u}_l\mat{u}_l}\inv} \right) \\
        &- \sum_{l=2}^L \frac{1}{2\sigma_l^2} \tr\left(\left(\mat{\Phi}_l - \mat{\Psi}_l\tran\mat{\Psi}_l\right) \mat{K}_{\mat{u}_l\mat{u}_l}\inv \left(\mat{m}_l\mat{m}_l\tran + \mat{S}_l\right)\mat{K}_{\mat{u}_l\mat{u}_l}\inv\right)
    \end{split}
\end{align}

For approximate predictions we need to pass our Gaussian messages through the model recursively.
Since every message besides the initial $\mat{X_{\ast}}$ is itself a Gaussian, it needs to be marginalized:
\begin{align}
    \begin{split}
        \Variat{\mat{f}_{\ast} \given \mat{X}_{\ast}} =
        &= \int \Variat{\mat{f_{d, \ast}}, \mat{a_{d, \ast}} \given \mat{X_{d, \ast}}} \diff \mat{a_{d, \ast}} \\
        &= \int \Variat{\mat{f_{d, \ast}} \given \mat{a_{d, \ast}}} \Variat{\mat{a_{d, \ast}} \given \mat{X_{d, \ast}}} \diff \mat{a_{d, \ast}} \\
        &= \Moment*{\E_{\Variat{\mat{a_{d, \ast}} \given \mat{X_{d, \ast}}}}}{\Variat{\mat{f_{d, \ast}} \given \mat{a_{d, \ast}}}} \\
        &= \Gaussian*{\mat{f_{d, \ast}} \given \mat{\bar{\mu}_{\ast}}, \mat{\bar{\Sigma}_{\ast}}}
    \end{split}
\end{align}
with \todo{I am super uncertain about $\mat{\bar{\Sigma}}$.}
\begin{align*}
    \mat{\bar{\mu}_{\ast}}    & = \mat{\Psi_{f\ast}} \mat{K_{mm}}\inv \mat{m}                                        \\
    \mat{\bar{\Sigma}_{\ast}} & = \mat{\Psi_{f\ast}}\mat{K_{mm}}\inv\mat{S}\mat{K_{mm}}\inv\mat{\Psi_{f\ast}}\tran.
\end{align*}
For the first layer, the expectation collapses to usual SVGP predictions as derived in \cite{hensman_scalable_2014}.
Given the approximation of $\mat{f_{d, \ast}}$, the approximation of $\mat{g_{d, \ast}}$ can be derived via the same procedure for the next layer.
This yields an approximate prediction for the final function values.

\subsubsection{Doubly Stochastic Variational Inference}
\label{toc:dsvi}
A central assumption of this approximation is that given enough well-placed inducing variables $\mat{u}_l$, they are a sufficient statistic for the latent function values $\mat{f}_l$.
This implies conditional independence of the $\mat{f}_{n,l}$ given $\mat{u}_l$ and $\mat{X}$.
The variational posterior of a single GP can then be written as,
\begin{align}
    \begin{split}
        \Variat*{\mat{f}_l \given \mat{X}}
        &=
        \int \Variat*{\mat{u}_l}
        \Prob*{\mat{f}_l \given \mat{u}_l, \mat{X}}
        \diff \mat{u}_l
        \\
        &=
        \int \Variat*{\mat{u}_l}
        \prod_{n=1}^N \Prob*{\mat{f}_{n,l} \given \mat{u}_l, \mat{x}_n}
        \diff \mat{u}_l,
    \end{split}
\end{align}
which can be evaluated analytically, since it is a convolution of Gaussians.
This formulation simplifies inference within single GPs.
Next, we discuss how to handle the correlations between the different functions and the assignment processes.

Following the ideas of doubly stochastic variational inference (DSVI) presented by~\textcite{salimbeni_doubly_2017} in the context of deep GPs, we maintain these correlations between different parts of the model while assuming factorization of the variational distribution.
That is, our variational posterior takes the factorized form,
\begin{align}
    \begin{split}
        \label{eq:dsvi:variational_distribution}
        \Variat*{\mat{f}_1, \mat{u}_1, \dots, \mat{f}_L, \mat{u}_L}
        &= \prod_{l=1}^L \Prob*{\mat{f}_l \given \mat{u}_l, \mat{f}_{l-1}} \Variat*{\mat{u}_l}.
    \end{split}
\end{align}

After marginalizing the inducing variables:
\begin{align}
    \begin{split}
        \label{eq:dsvi:variational_layers}
        \Variat*{\mat{f}_1, \mat{u}_1}
        &= \prod_{l=1}^L \Variat*{\mat{f}_l \given \mat{u}_l, \mat{f}_{l-1}} \\
        &= \prod_{l=1}^L \Gaussian{\mat{f}_l \given \mat{\mu}_l, \mat{\Sigma}_l},
    \end{split}
\end{align}
with $\mat{\mu}_l$ and $\mat{\Sigma}_l$ from~\cref{eq:gp:sgpr_predictive_posterior}.

It turns out the this holds for the marginals in a forwards-pass:
\begin{align}
    \begin{split}
        \label{eq:dsvi:variational_layers}
        \Variat*{\mat{f}_{n,L}}
        &= \int \prod_{l=1}^{L-1} \Variat*{\mat{f}_{n, l} \given \mat{u}_l, \mat{f}_{n, l-1}} \diff \mat{f}_{n, l}.
    \end{split}
\end{align}
That's convenient because we can sample it directly, giving a bound:
\begin{align}
    \begin{split}
        \label{eq:dsvi:full_bound}
        \MoveEqLeft \Lc_{\text{DSVI}} = \log \Prob{\mat{y} \given \mat{X}} \geq \\
        \sum_{n=1}^N \Moment*{\E_{\Variat*{\mat{f}_{n, L}}}}{\log \Prob*{\mat{y}_n} \given \mat{f}_{n, L}}
        - \sum_{l=1}^L \KL*{\Variat*{\mat{u}_l}}{\Prob*{\mat{u}_l}}
    \end{split}
\end{align}
Evaluating this has $\Oh(NM^2D)$ complexity.
Approximate predictions work the same way:
\begin{align}
    \begin{split}
        \label{eq:dsvi:approximate_predictions}
        \Variat*{\mat{f}_{\ast,L}}
        &\approx \frac{1}{S} \sum_{s=1}^S \Variat*{\mat{q}_{\ast, L} \given \mat{u}_L, \mat{f}_{\ast, L-1}}.
    \end{split}
\end{align}
