\chapter{Introduction}
\label{toc:introduction}
In many machine learning problems, the task is to derive a model for observational data that has been generated by some process in nature that is not fully understood.
Starting with an input-vector~$\mat{x}$ describing its initial state, nature produces an output-vector~$\mat{y}$.
Given a set of observational pairs $(\mat{x}, \mat{y})$, we want to learn about the underlying, possibly stochastic, functional dependency
\begin{align}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, 1);
        \node[random variable, observed] (x) at (0, 2) {$\mat{x}$};
        \node[random variable, observed] (y) at (0, 0) {$\mat{y}$};
        % ---
        \node[
            white box component,
            minimum height=42pt,
            text width=100pt,
            align=center,
            shape=rectangle,
        ] (nature) at (0, 1) {Nature};
        % ---
        \draw[edge, directed] (x) -- (nature);
        \draw[edge, directed] (nature) -- (y);
    \end{tikzpicture}
     &   &
    \mat{y}
     & =
    f(\mat{x}).
\end{align}
The goal of learning is often to either gain new knowledge about nature by describing the data or to be able to predict outputs $\mat{y}_\ast$ for previously unseen inputs $\mat{x}_\ast$.
While traditional statistics favor the white-box modeling culture that focuses on explaining observational data using causal theoretic models, machine learning and specifically deep learning is more closely associated with the black-box culture, which focuses on prediction instead~\parencite{breiman_statistical_2001,shmueli_explain_2010}.
The impressive successes of both cultures have been achieved in different application domains, however, which is due to their respective pros and cons.


\section{Black-box models in deep learning}
When formulating a black-box model, we accept that we cannot model nature directly and focus on the functional dependency $f$ instead.
The approach is to find an algorithm $f_{\mat{w}}$ with parameters $\mat{w}$ that approximates the true dependency $f$ in
\begin{align}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, 1);
        \node[random variable, observed] (x) at (0, 2) {$\mat{x}$};
        \node[random variable, observed] (y) at (0, 0) {$\mat{y}$};
        % ---
        \node[
            black box component,
            minimum height=42pt,
            text width=100pt,
            align=center,
            shape=rectangle,
        ] (nature) at (0, 1) {Neural network};
        % ---
        \draw[edge, directed] (x) -- (nature);
        \draw[edge, directed] (nature) -- (y);
    \end{tikzpicture}
     &   &
    \mat{y}
     & =
    f_{\mat{w}}(\mat{x}).
\end{align}
Using general function approximators like neural networks with a typically large set of weights $\mat{w}$, any smooth dependency can be modeled given enough observations.
With growing computational power and the availability of data sets, black-box approaches have led to impressive advancements in domains that are hard to formalize.
Successes have been seen in a wide range of digital tasks such as speech recognition~\parencite{hochreiter_long_1997,chorowski_attention-based_2015,bahdanau_neural_2014,chorowski_attention-based_2015}, computer vision~\parencite{russakovsky_imagenet_2015,lecun_backpropagation_1989}, non-cooperative games~\parencite{berner_dota_2019,silver_mastering_2016}, or machine translation~\parencite{johnson_googles_2017}.
The abundance of data in these domains shifts the focus away from traditional statistical models to highly adaptable and scalable approaches.
Black-box models do not claim to uncover or represent knowledge about the true process in nature but capture correlations between inputs and outputs instead.
Their advantages and disadvantages can be summarized as follows.

\begin{Pros}[Deep learning]
    \begin{compactdesc}
        \item[Universal approximation]
        Neural networks can potentially find useful structure in any data set, even if the problem domain is not well-understood or cannot be formalized by experts.
        \item[Simple portability]
        Black-box model formulations can be applied to many different applications due to their assumed independence of the concrete problem.
        \item[Strong scalability]
        The internal structure of black-box models can often be chosen to leverage computational power optimally.
        Models in deep learning can cope with billions of parameters and observations.
    \end{compactdesc}
\end{Pros}
\begin{Cons}[Deep learning]
    \begin{compactdesc}
        % \item[Complex parametrization]
        % Many models in deep learning depend on specific architecture-choices and carefully tuned hyper-parameters to produce desired results, drawing their robustness into question.
        \item[Data bias]
        Black-box approaches can only uncover the structure that is present in the training data.
        They, therefore, typically require a large amount of training data in all parts of the system.
        \item[Weak interpretability]
        Black-box models are not designed to offer causal explanations of observations.
        As a result, the complex internal structure of models in deep learning is generally not understandable to human experts~\parencite{rudin_stop_2019}.
        \item[Unclear generalization]
        A black-box model can only be evaluated in terms of its ability to predict unseen data.
        In parts of a system where no data is available, it is hard to make statements about when or how a model generalizes.
    \end{compactdesc}
\end{Cons}

The highly adaptable and scalable models from deep learning excel at finding structure in large and complex data sets.
However, the black-box approach struggles in environments where the model's internal structure or the generalization behavior come into focus.


\section{White-box models in Bayesian statistics}
White-box models are based on strong and human-interpretable internal structure.
The focus is on describing and understanding observational data and thus the true generative process in nature.
In modern science, white-box models are used in large-scale experiments such as in particle physics~\parencite{khachatryan_observation_2015} or astronomy~\parencite{collaboration_first_2019,the_ligo_scientific_collaboration_observation_2016}.
In such experiments, noisy and limited data play a major role.
To cope with imperfect knowledge, causal theoretic models are extended with statistical theory to formulate probabilistic models.
Probabilistic models consider the likelihood $\Prob*{\mat{y} \given \mat{x}}$ based on knowledge about the model components $\whiteboxblob{latent}, \whiteboxblob{hyperparameter}, \dots$ in
\begin{align}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, 1);
        \node[random variable, observed] (x) at (0, 2) {$\mat{x}$};
        \node[random variable, observed] (y) at (0, 0) {$\mat{y}$};
        % ---
        \node[white box component, latent] (a) at (-0.75, 1.25) {};
        \node[white box component, hyperparameter] (b) at (0.75, 1.25) {};
        \node[white box component, variational] (c) at (0, 0.75) {};
        \node[white box component, fill=white] (d) at (-0.75, 0.75) {};
        % ---
        \draw[edge, directed] (x) -- (a);
        \draw[edge, directed] (x) -- (b);
        \draw[edge, directed] (a) -- (c);
        \draw[edge, directed] (b) -- (c);
        \draw[edge, directed] (d) -- (c);
        \draw[edge, directed] (c) -- (y);
        \begin{scope}[on background layer]
            \node[
                white box,
                fit=(a)(b)(c)(d),
            ] {};
        \end{scope}
    \end{tikzpicture}
     &   &
    \Prob*{\mat{y} \given \mat{x}}
     & =
    \begin{aligned}
         & \int
        \Prob*{
            \mat{y},
            \whiteboxblob{latent},
            \whiteboxblob{hyperparameter},
            \whiteboxblob{variational},
            \whiteboxblob{}
            \given \mat{x}
        }
        \diff\whiteboxblob{latent}
        \diff\whiteboxblob{hyperparameter}
        \diff\whiteboxblob{variational}
        \diff\whiteboxblob{}.
    \end{aligned}
\end{align}

Bayesian inference offers a framework for formalizing this knowledge in prior assumptions and combining them with data.
Using Bayes' rule, a distribution of plausible models is reweighed by their ability to generate data to yield a posterior distribution
$
    \Prob*{
        \whiteboxblob{latent},
        \whiteboxblob{hyperparameter},
        \whiteboxblob{variational},
        \whiteboxblob{}
        \given \mat{x}, \mat{y}
    }
$
over plausible explanations.
To make predictions about novel observations $\Prob*{\mat{y}_\ast \given \mat{x}_\ast}$, all plausible models are taken into account, allowing us to both quantify confidence in predictions and take more unlikely but relevant scenarios into consideration.
Scientific white-box models are driven by strong theory that provides causal explanations.
Observational data is used to answer specific questions about a small number of hypotheses, leading to the following pros and cons.

\begin{Pros}[Bayesian statistics]
    \begin{compactdesc}
        \item[Strong interpretability]
        Since a white-box model is constructed from human-interpretable components and causal theory, model behavior and predictions are understandable to experts.
        \item[Trustworthy predictions]
        Bayesian probabilistic models quantify their confidence about predictions in a principled manner based on explicit assumptions.
        \item[Safe generalization]
        The generalization of white-box models is driven by human-interpretable causal theory that can be evaluated and verified by experts.
    \end{compactdesc}
\end{Pros}
\begin{Cons}[Bayesian statistics]
    \begin{compactdesc}
        \item[Model bias]
        All insights from white-box models are with respect to strong causal theoretic modeling assumptions.
        Wrong or insufficient assumptions cannot be compensated through data, limiting the applicability of white-box models in problem domains that are hard to formalize.
        \item[Subjective model-selection]
        White-box models aim to model causal relationships in nature.
        It is a hard problem to select the correct model from a set of plausible explanations~\parencite{thorburn_occams_1915}.
        \item[Weak scalability]
        The theory behind white-box models is often problem-specific and hard to transfer to other applications.
        Similarly, complex theory often requires specialized learning algorithms that do not scale to large amounts of data.
    \end{compactdesc}
\end{Cons}

White-box models have been the driving force behind many advances in economics, medicine, and the natural sciences and allowed researchers to find and understand the structure in complex and heterogenous data~\parencite{efron_modern_2005,giulio_d._agostini_bayesian_2003}.
However, as processes get more complex, and the amount of available data grows, formulating white-box models becomes a very difficult task~\parencite{sutton_bitter_2019}.


\section{Structured models with Gaussian processes}
White-box and black-box modeling approaches serve different purposes in data analysis.
White-box models are used in applications where hypotheses should be tested when strong a theoretical background is available.
Black-box models are successful when correlations and associations in large amounts of data should be used to make predictions about systems where the consequences of mistakes are mild.
However, for many safety-critical applications of machine learning in the physical world, the situation is different.
Complete theoretic models are not available to solve a learning task, and new insights need to be inferred from potentially large amounts of data.
Simultaneously, models need to make physically plausible predictions and are required to be interpretable for domain experts.

Take wind power production as an example:
To optimize the efficiency of a wind-turbine, the speed and pitch have to be controlled according to the local wind conditions.
In a wind-farm, turbines are typically equipped with sensors for wind speed and direction.
The goal is to use these sensor data to produce accurate estimates and forecasts of the wind conditions at every turbine in the farm.
For the ideal case of a homogeneous and very slowly changing wind field, wind conditions can be estimated using the propagation times computed from geometry, wind speed and direction.
In the real world, however, wind fields are not homogeneous, exhibit global and local turbulence, and interfere with the turbines and the terrain inside and outside the farm.
This makes it extremely difficult to construct accurate white-box models of wind propagation in a farm.
At the same time, data-driven black-box approaches struggle due to identifiability issues introduced by the noise of the unpredictable turbulence.
While we cannot formulate analytical models, domain experts do have an intuitive understanding of the underlying physics of wind propagation.
A successful model needs to combine the white-box and black-box approaches:
It needs to reproduce expectations like slowly changing prevailing wind conditions or gusts that travel through the system while still being able to infer new knowledge about specific turbine behavior or the influence of terrain from data.
By checking if these expectations are met, domain experts can build trust in the model's predictions for novel situations.

\begin{table}[t]
    \caption[Research questions]{
        \label{tab:introduction:research_questions}
        The model properties considered as research questions in this thesis.
    }
    \centering
    \newcolumntype{Y}{>{\centering\arraybackslash}X}
    \newcommand{\yes}{✔}
    \newcommand{\no}{\textcolor{sRedLight}{✘}}
    \newcommand\tabnode[2]{%
        \tikz[remember picture, baseline] \node[inner sep=0pt, anchor=base] (#1) {#2};
    }
    \begin{tabularx}{.9\textwidth}{lYY}
        \toprule
                                 & Deep learning    & Bayesian statistics \\
        \midrule
        Data-driven insights     & \yes             & \tabnode{a}{\no}    \\
        Strong scalability       & \yes             & \tabnode{b}{\no}    \\
        \addlinespace
        Interpretable results    & \tabnode{c}{\no} & \yes                \\
        Trustworthy predictions  & \tabnode{d}{\no} & \yes                \\
        \addlinespace
        Semantic model-selection & \tabnode{f}{\no} & \tabnode{e}{\no}    \\
        \bottomrule
    \end{tabularx}
    \begin{tikzpicture}[remember picture, overlay]
        \tikzstyle{question} = [
        draw=sGreenDark, fill=sGreenDark,
        fill opacity=.2, very thick,
        inner xsep=6pt, inner ysep=3pt,
        ]
        \node[
            question,
            fit=(a)(b),
            label={[sGreenDark, anchor=west]east:RQ1}
        ] {};
        \node[
            question,
            fit=(c)(d),
            label={[sGreenDark, anchor=east]west:RQ2}
        ] {};
        \node[
            question,
            fit=(e)(f),
            label={[sGreenDark, anchor=west]east:RQ3}
        ] {};
    \end{tikzpicture}
\end{table}
In this work, we study how to formulate structured Bayesian models that reproduce knowledge, are understandable for domain experts, and can still gain new insights from data.
\Cref{tab:introduction:research_questions} shows that structured models need to combine the advantages of the general function approximators from deep learning and the strong and interpretable structure from Bayesian statistics.
To that end, we embed Gaussian processes (GPs) in hierarchical probabilistic models.
GPs are non-parametric distributions over functions that allow us to encode Bayesian assumptions explicitly to achieve interpretable results.
The research questions we consider in this thesis are as follows.

\paragraph{RQ1: How can we reduce the model-bias of white-box models and efficiently learn from data?}
Starting from a white-box model, we explore how to relax a strong theoretical structure by embedding general function approximators in a hierarchy.
While conserving interpretability and informative uncertainty quantification, we use GPs to learn about problem-components we cannot formalize.
We discuss how to formulate inference schemes that scale to large data sets.

\paragraph{RQ2: How can we reduce the data-bias of black-box models and reliably reproduce expert expectations?}
We study how to add structure to black-box deep GP models to reproduce expert knowledge and factorize uncertainties.
The additional structure allows us to formulate problem-driven constraints for desirable solutions that yield physically plausible results.
We explore how rich internal structure helps models to generalize to novel situations in an interpretable manner.

\paragraph{RQ3: How can we formalize model selection in situations where internal model structure and generalization behavior come into focus?}
As evaluation data in critical parts of the system is often scarce in physical environments, performance measures such as the generalization error are not enough to identify desirable structured models.
Simultaneously, the subjective model-selection of white-box approaches is problematic in domains that are not well-understood.
To formalize the description of desirable structured models, we explore how downstream tasks can be taken into account for model selection.

\section{Contributions}
This work presents novel Bayesian structured models inspired by industrial applications.
The main contributions to the research questions in~\cref{tab:introduction:research_questions} are as follows.
They appeared in a number of peer-reviewed publications and patents we outline below.

\begin{compactdesc}
    \item[RQ1]
    In \cref{contrib:data_association,contrib:patent_depeweg,contrib:patent_geipel}, we formulate a Bayesian interpretation of the data association problem.
    We discuss an application of this model to the prediction of the combustion dynamics of a gas turbine, where the structured model allows us to distinguish between different dynamical regimes.
    \item[RQ2]
    In~\cref{contrib:windpower,contrib:patent_egedal,contrib:patent_kaiser}, we formulate a hierarchical Bayesian model for a nonlinear time-series alignment problem.
    We show that this model is capable of representing the complex interactions between turbines in a wind farm, and discuss how it can be used to derive more efficient controllers for wind-turbines.
    \item[RQ3]
    In \cref{contrib:interpretable_rl,contrib:multi_modal_rl}, we apply our data association model to a reinforcement learning task and show how a semantic decomposition of the dynamics reduces the data requirements, and produces interpretable solutions.
    In \cref{contrib:surrogates}, we formulate surrogate models for Bayesian optimization problems that focus on the informative structure of the objective functions by sacrificing local accuracy and in \cref{contrib:composition}, we present an argument for why inference schemes based on factorizations between layers cannot represent heterogeneous posteriors.
\end{compactdesc}


\subsubsection{Own publications}
\begin{enumerate}
    \item \label[contribution]{contrib:windpower}\fullfullcite{kaiser_bayesian_2018}
    \item \label[contribution]{contrib:data_association}\fullfullcite{kaiser_data_2019}
    \item \label[contribution]{contrib:interpretable_rl}\fullfullcite{kaiser_interpretable_2019}
    \item \label[contribution]{contrib:multi_modal_rl}\fullfullcite{kaiser_bayesian_2020}
    \item \label[contribution]{contrib:surrogates}\fullfullcite{bodin_modulating_2020}
    \item \label[contribution]{contrib:composition}\fullfullcite{ustyuzhaninov_compositional_2020}
\end{enumerate}

\subsubsection{Patents}
\begin{enumerate}
    \setcounter{enumi}{6}
    \item \label[contribution]{contrib:patent_egedal}\fullfullcite{egedal_verfahren_2019}
    \item \label[contribution]{contrib:patent_kaiser}\fullfullcite{kaiser_verfahren_2019}
    \item \label[contribution]{contrib:patent_geipel}\fullfullcite{geipel_transferlernen_2020}
    \item \label[contribution]{contrib:patent_depeweg}\fullfullcite{depeweg_computer_2020}
\end{enumerate}


\section{Thesis outline}
The content of this thesis is structured as follows.
In \cref{toc:gp}, we provide a short introduction to Bayesian machine learning and Bayesian nonparametrics with a focus on Gaussian processes (GPs).
We discuss the connection between statistical learning theory and Bayesian inference and their application to hierarchical models.
After introducing GPs, we discuss variational approaches to sparse GPs, their extension to hierarchical GP models, and efficient inference schemes.

In \cref{toc:data_association}, we consider the first research question and explore how to formulate structured hierarchical models using the data association problem as an example.
We separate the data association problem into hierarchical components and derive an efficient joint inference scheme, resulting in a fully Bayesian model.
We show that this model can factorize multi-modal data into independent processes, providing explicit models for both the processes and assignment probabilities.
Comparisons with previous models show the additional qualitative model capabilities of the structured approach and competitive black-box performance.
However, experiments also show that standard measures are not enough to identify desirable models.

In \cref{toc:alignment}, we consider the second research question and formulate a structured Bayesian model that reproduces expert knowledge about wind propagation in a wind-farm.
We interpret the problem of modeling the power production of multiple wind-turbines as a nonlinear alignment problem and derive an efficient inference scheme.
Based on expert knowledge about the underlying latent and turbulent wind field, we can derive a structured model that enforces a physically plausible dependency structure between multiple time-series.
We discuss how these constraints help solve an otherwise highly ambiguous learning problem and show that the imposed structure leads to a rich internal model-structure that experts can interpret.

In \cref{toc:interpretable_rl}, we consider the third research question and discuss how to formalize the subjectiveness introduced via expert knowledge and model ambiguities to evaluate structured models.
We include the task a model will be used to solve into model selection, which allows us to distinguish between qualitatively different models showing similar performance metrics.
We revisit the data association problem and embed it into a reinforcement learning problem, where identifying the correct underlying dynamics, and therefore a desirable model, is critical to finding a successful policy.
We show that semantic hierarchical structure increases data efficiency and allows domain experts to influence agent behavior through detailed insights into the dynamics of a system.

\Cref{toc:discussion} concludes the work presented in this thesis and interprets the results in a broader context by further exploring the properties and evaluation of structured hierarchical models.
In \cref{toc:discussion:composition}, we present an intuitive argument for why inference schemes based on factorizations between layers cannot represent heterogeneous posteriors.
In \cref{toc:discussion:bo}, we argue why models with suboptimal marginal likelihoods can perform well in hierarchical systems.
In \cref{toc:discussion:mountaincar}, we explore this idea further and consider how tasks can be included in the inference problem directly.
Finally, we discuss possible further directions for research.
