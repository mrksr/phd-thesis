\chapter{Introduction}
\label{toc:introduction}

% Machine learning methods have seen great success recently in a wide range of digital domains such as speech recognition, computer vision, or translation.
% However, bridging the gap to applications in the physical world has proved challenging.
% Problem domains like robotics, industrial control, decision support systems, or the natural sciences introduce a new set of requirements.
% ML-systems that operate in safety-critical areas, interact with people, or carry responsibility must be robust, trustworthy, and assessable.
% Besides optimizing for good average-case performance, a responsible system needs to reason about plausible worst-cases and reliably avoid or inform about them.

% In this workshop, we will identify a list of properties required in real-world scenarios and discuss how they can be evaluated.
% We aim to address challenges arising from the need for interaction of machine learning researchers and domain experts in successful applications.
% How do we design models which facilitate communication with experts and how do we evaluate interpretability?
% How can we benchmark the trustworthiness and robustness of a model and ensure that it can cope with unobserved situations?
% What do uncertainties mean in practice and how do we make use of them to reason about worst-case performance?

% We want to bring together computer scientists, mathematicians, and statisticians with domain experts to discuss approaches that facilitate interdisciplinary dialogue.
% We welcome contributions on explainable and interpretable models, human-in-the-loop learning, uncertainty quantification, and learning guarantees.
% We hope to introduce open problems to a broader community and discuss benchmarks and success criteria for requirements driven by machine learning problems in the physical world.

%%%

Machine learning methods have seen great success recently in a wide range of digital domains such as speech recognition, computer vision, or translation.
In such domains, data is abundant, and the consequences of mistakes tend to be mild.
However, bridging the gap to applications in the physical world has proved challenging.
Problem domains like robotics, industrial control, decision support systems, or the natural sciences introduce a new set of requirements.
ML-systems which operate in safety-critical areas, interact with people or carry responsibility must be robust, trustworthy, and assessable.
As applications become more safety-relevant, gathering data through exploration can be problematic due to the adverse consequences of failure.

Besides optimizing for good average-case performance, a responsible system needs to reason about plausible worst-cases and reliably avoid or inform about them.
ML-systems need to be verified by domain experts before deployment and are used to test hypotheses or make impactful decisions, emphasizing a need for interpretability.
Models are required to incorporate and reproduce expert knowledge and make consistent predictions.
A key technique that allows us to cope with these requirements is principled probabilistic models that allow us to represent and propagate uncertainties explicitly.
This allows us to both quantify confidence in predictions and take more unlikely but relevant scenarios into consideration.

% At Siemens Research, I have worked on a number of pioneering applications of machine learning to industrial systems in safety-critical applications.
% In industrial control problems, machine learning is often used to find more efficient or safer control strategies not obvious to the engineers designing a machine.
% Relevant data for finding new strategies is scarce since the most valuable data such as when a machine will fail or how it will behave in new situations is never produced.
% Finding exploration strategies is a collaborative task combining the knowledge of machine learning experts and domain experts.
% Above all, interdisciplinary work requires a common language and understanding.
% One of my research goals is to explore how to effectively formulate robust models together with domain experts to combine the available data with their knowledge.
% This knowledge is often based on an intuitive understanding of the underlying physics leading to coarse expectations about system-behavior on different layers of abstraction.
% In recent work\footnote{Bayesian Alignments of Warped Multi-Output Gaussian Processes, \url{https://arxiv.org/abs/1710.02766}}, we formulated a model that is capable of representing the complex interactions between turbines in a wind-farm.
% We combined strong hierarchical prior knowledge about wind propagation and turbine behavior with the flexibility of general function approximations to separate stochastic turbulence from adverse wake effects.
% During my fellowship, I will build on this work and explore how abstract expert knowledge can be embedded in hierarchical models efficiently.

\todoi{And so on, and so forth...}


\section{Structured Machine Learning Models}
\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{intro_model_nature}
        \caption{
            \label{fig:introduction:models:nature}
            Nature
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{intro_model_black_box}
        \caption{
            \label{fig:introduction:models:black_box}
            Black-Box
        }
    \end{subfigure}
    \caption{
        \label{fig:introduction:models:one}
        Nature and black box models.
    }
\end{figure}
\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{intro_model_white_box}
        \caption{
            \label{fig:introduction:models:white_box}
            White-Box
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{intro_model_structured}
        \caption{
            \label{fig:introduction:models:structured}
            Structured
        }
    \end{subfigure}
    \caption{
        \label{fig:introduction:models:two}
        Nature and black box models.
    }
\end{figure}
There are two cultures.
We explore a third.
This fall.

\begin{itemize}
    \item Introduce regression as a functional dependency in nature we want to model.
    \item Following \textcite{breiman_statistical_2001}, we can tackle the problem using algorithmic (black-box) and data (white-box) models.
    \item Traditional statistics favor white-box models and care about describing and understanding observational data.
          In this view of the world, we need to come up with a model of nature that completely explains all aspects of the data and learning comes down to fitting a limited number of highly interpretable parameters.
          The smaller the number of parameters the better, as we want to develop a simple and specific theory that explains the data well (Occam's razor).
          We do not necessarily care about predictions.
    \item The other end of the spectrum are black-box or algorithmic models which are more closely associated with machine learning.
          Here, we accept that we cannot model nature and instead recognize that there is some functional dependency in the data a general function approximator can, given enough data, approximate arbitrarily well.
          Since a successfully powerful approximator can explain any observation set, we need to measure performance in terms of predictions and that is actually what we care about.
          We never claim that we are able to uncover the correct model or the true generative process.
    \item In this thesis, we want to study what we call structured models.
          Structured models live between these extremes because we design models based on our understanding of nature (the underlying problem).
          At the same time, we accept that there are things we do not (or cannot) understand and insert general function approximators.
          Our goal is to be more data efficient compared to black-box models because there is less we need to infer from data thanks to our prior knowledge.
          At the same time, we want to gain insights from the data thanks to the general function approximators, so our model is not purely descriptive either.
    \item Many successes in machine learning are based on highly structured models which are often motivated differently.
          Examples include convolutional networks for vision, LSTM for language processing or Monte-Carl tree search for two-player perfect information games.
          Since these models are generally understood as being black-box, they do not care about statistical measures such as uncertainty quantification or actual insights about the generating process.
          They are being used as engineering-tools instead.
          In this thesis we care about well-founded probabilistic models.
\end{itemize}


\section{Thesis outline}
\todoi{Is it finished yet?}
We will explore two things in the thesis.
\begin{itemize}
    \item We show two approaches to adding structure to a learning problem.
          First, we use what might be called formal structure motivated by the mathematical formulation of an underlying problem, the data association problem.
          This is closer to black-box models.
          And second, we use structure motivated by expert understanding of the underlying physical process that generated the data, which is closer to white-box models.
    \item As it turns out, since the models live in-between the two extremes, their respective tools for judging whether a model is good are not sufficient.
          Only looking at data-fits (both in training and test) is not enough since things like symmetries or ambiguities in the general function approximators can lead to multiple plausible models that show similar performance.
          Marignal likelihoods do not capture the inner shape of a model and we there is no straightforward way to formalize success.
          Because of this, we discuss measuring success based on a downstream task which requires a model to actually represent the true underlying generative process.
\end{itemize}


\section{Publications}
Publications
\begin{itemize}
    \item \fullfullcite{kaiser_bayesian_2018}

          Here, I did great things.
    \item \fullfullcite{kaiser_data_2019}
    \item \fullfullcite{kaiser_interpretable_2019}
    \item \fullfullcite{kaiser_bayesian_2020}
    \item \fullfullcite{bodin_modulating_2020}
    \item \fullfullcite{ustyuzhaninov_compositional_2020}
\end{itemize}

Patents
\begin{itemize}
    \item \fullfullcite{egedal_verfahren_2019}
    \item \fullfullcite{kaiser_verfahren_2019}
    \item \fullfullcite{geipel_transferlernen_2020}
    \item \fullfullcite{depeweg_computer_2020}
\end{itemize}
