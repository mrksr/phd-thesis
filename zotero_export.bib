
@inproceedings{alvarez_efficient_2010,
  title = {Efficient {{Multioutput Gaussian Processes}} through {{Variational Inducing Kernels}}.},
  booktitle = {{{AISTATS}}},
  author = {Alvarez, Mauricio A. and Luengo, David and Titsias, Michalis K. and Lawrence, Neil D.},
  date = {2010},
  volume = {9},
  pages = {25--32},
  url = {http://www.jmlr.org/proceedings/papers/v9/alvarez10a/alvarez10a.pdf},
  urldate = {2017-03-02},
  file = {C\:\\Users\\pub\\Zotero\\storage\\6Q4I9FRF\\Alvarez et al. - 2010 - Efficient Multioutput Gaussian Processes through V.pdf}
}

@article{alvarez_kernels_2011,
  title = {Kernels for {{Vector}}-{{Valued Functions}}: A {{Review}}},
  shorttitle = {Kernels for {{Vector}}-{{Valued Functions}}},
  author = {Alvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
  date = {2011-06-30},
  url = {http://arxiv.org/abs/1106.6251},
  urldate = {2017-02-06},
  abstract = {Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.},
  archivePrefix = {arXiv},
  eprint = {1106.6251},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\R6PZ939E\\Alvarez et al. - 2011 - Kernels for Vector-Valued Functions a Review.pdf;C\:\\Users\\pub\\Zotero\\storage\\IFE9Z28Q\\1106.html},
  keywords = {Computer Science - Artificial Intelligence,Mathematics - Statistics Theory,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@inproceedings{alvarez_sparse_2009,
  title = {Sparse Convolved {{Gaussian}} Processes for Multi-Output Regression},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Alvarez, Mauricio and Lawrence, Neil D.},
  date = {2009},
  pages = {57--64},
  url = {http://papers.nips.cc/paper/3553-sparse-convolved-gaussian-processes-for-multi-output-regression},
  urldate = {2017-07-14},
  file = {C\:\\Users\\pub\\Zotero\\storage\\SIZMYY5F\\Alvarez and Lawrence - 2009 - Sparse convolved Gaussian processes for multi-outp.pdf;C\:\\Users\\pub\\Zotero\\storage\\A2QU9XT7\\3553-sparse-convolved-gaussian-processes-for-multi-output-regression.html}
}

@book{astrom_introduction_1971,
  title = {Introduction to {{Stochastic Control Theory}}},
  author = {Åström, Karl J.},
  date = {1971-02-27},
  publisher = {{Elsevier}},
  abstract = {In this book, we study theoretical and practical aspects of computing methods for mathematical modelling of nonlinear systems. A number of computing techniques are considered, such as methods of operator approximation with any given accuracy; operator interpolation techniques including a non-Lagrange interpolation; methods of system representation subject to constraints associated with concepts of causality, memory and stationarity; methods of system representation with an accuracy that is the best within a given class of models; methods of covariance matrix estimation;methods for low-rank matrix approximations; hybrid methods based on a combination of iterative procedures and best operator approximation; andmethods for information compression and filtering under condition that a filter model should satisfy restrictions associated with causality and different types of memory.As a result, the book represents a blend of new methods in general computational analysis,and specific, but also generic, techniques for study of systems theory ant its particularbranches, such as optimal filtering and information compression.- Best operator approximation,- Non-Lagrange interpolation,- Generic Karhunen-Loeve transform- Generalised low-rank matrix approximation- Optimal data compression- Optimal nonlinear filtering},
  isbn = {978-0-08-095579-7},
  keywords = {Mathematics / Algebra / Linear,Mathematics / Calculus,Mathematics / Mathematical Analysis,Mathematics / Numerical Analysis,Technology & Engineering / Mechanical},
  langid = {english},
  pagetotal = {318}
}

@article{barshalom_tracking_1990,
  title = {Tracking and {{Data Association}}},
  author = {Bar‐Shalom, Yaakov and Fortmann, Thomas E. and Cable, Peter G.},
  date = {1990-02-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {87},
  pages = {918--919},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.398863},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.398863},
  urldate = {2020-03-04},
  file = {C\:\\Users\\pub\\Zotero\\storage\\GIAF2DIW\\1.html},
  number = {2}
}

@article{barto_neuronlike_1983,
  title = {Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems},
  author = {Barto, A. G. and Sutton, R. S. and Anderson, C. W.},
  date = {1983-09},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {SMC-13},
  pages = {834--846},
  issn = {0018-9472},
  doi = {10.1109/TSMC.1983.6313077},
  abstract = {It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\899XKXEN\\Barto et al. - 1983 - Neuronlike adaptive elements that can solve diffic.pdf;C\:\\Users\\pub\\Zotero\\storage\\CL99BSYP\\6313077.html;C\:\\Users\\pub\\Zotero\\storage\\MKQ85H6W\\6313077.html},
  keywords = {adaptive control,adaptive critic element,Adaptive systems,animal learning studies,associative search element,Biological neural networks,learning control problem,learning systems,movable cart,neural nets,neuronlike adaptive elements,Neurons,Pattern recognition,Problem-solving,Supervised learning,Training},
  number = {5}
}

@book{bertsekas_stochastic_1978,
  title = {Stochastic {{Optimal Control}}: {{The Discrete}}-{{Time Case}}},
  author = {Bertsekas, Dimitir P. and Shreve, Steven},
  date = {1978},
  publisher = {{Academic press}},
  abstract = {The book is a comprehensive and theoretically sound treatment of the mathematical foundations of stochastic optimal control of discrete-time systems, including the treatment of the intricate measure-theoretic issues.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\5JHEMU42\\book.pdf},
  isbn = {1-886529-03-5}
}

@report{bishop_mixture_1994,
  title = {Mixture Density Networks},
  author = {Bishop, Christopher M.},
  date = {1994},
  file = {C\:\\Users\\pub\\Zotero\\storage\\HLZBIATM\\Bishop - 1994 - Mixture density networks.pdf}
}

@inproceedings{bitar_coordinated_2013,
  title = {Coordinated Control of a Wind Turbine Array for Power Maximization},
  booktitle = {2013 {{American Control Conference}}},
  author = {Bitar, Eilyan and Seiler, Pete},
  date = {2013-06},
  pages = {2898--2904},
  issn = {2378-5861},
  doi = {10.1109/ACC.2013.6580274},
  abstract = {Wind turbines are currently operated at their peak power extraction efficiency without consideration of the aerodynamic coupling between neighboring turbines. This mode of operation leads to inefficient, sub-optimal power capture at the wind farm level. By explicitly accounting for the aerodynamic wake interactions between neighboring wind turbines within a farm, we aim to characterize optimal control policies that maximize the power captured by a collection of wind turbines operating in quasi-steady wind flow conditions. In this paper, we consider two wake interaction models, termed near-field and far-field, describing wake propagation under densely and sparsely spaced turbine arrays, respectively. Under the near-field model, we derive a closed form expression for the optimal control policy maximizing power capture for a one-dimensional array of wind turbines. Moreover, we show that the optimal control policy is both static and independent of the free stream wind velocity, being thus amenable to a decentralized implementation. We also formulate and solve numerically the problem of jointly optimizing over the control policy and placement of turbines in a one dimensional 3-turbine array under the far-field model.},
  eventtitle = {2013 {{American Control Conference}}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\SEJY5JEY\\6580274.html},
  keywords = {aerodynamic wake interactions,aerodynamics,Aerodynamics,Arrays,closed form expression,coordinated wind turbine array control,Couplings,decentralised control,decentralized implementation,densely spaced turbine array,dynamic programming,Equations,far-field wake interaction model,free stream wind velocity-independent optimal control policy,inefficient-suboptimal power capturing,Mathematical model,near-field wake interaction model,numerical analysis,one-dimensional 3-turbine array,one-dimensional wind turbine array,optimal control,Optimal Control,peak power extraction efficiency,power maximization,quasisteady wind flow conditions,sparsely spaced turbine array,static optimal control policy,turbine placement,wake propagation,wakes,Wind Energy,wind farm level,wind turbines,Wind turbines}
}

@article{bodin_latent_2017,
  title = {Latent {{Gaussian Process Regression}}},
  author = {Bodin, Erik and Campbell, Neill D. F. and Ek, Carl Henrik},
  date = {2017-07-18},
  url = {http://arxiv.org/abs/1707.05534},
  urldate = {2017-08-29},
  abstract = {We introduce Latent Gaussian Process Regression which is a latent variable extension allowing modelling of non-stationary processes using stationary GP priors. The approach is built on extending the input space of a regression problem with a latent variable that is used to modulate the covariance function over the input space. We show how our approach can be used to model non-stationary processes but also how multi-modal or non-functional processes can be described where the input signal cannot fully disambiguate the output. We exemplify the approach on a set of synthetic data and provide results on real data from geostatistics.},
  archivePrefix = {arXiv},
  eprint = {1707.05534},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\IWU4IK4P\\Bodin et al. - 2017 - Latent Gaussian Process Regression.pdf;C\:\\Users\\pub\\Zotero\\storage\\AZSUGMDS\\1707.html},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{boyle_dependent_2004,
  title = {Dependent {{Gaussian Processes}}.},
  booktitle = {{{NIPS}}},
  author = {Boyle, Phillip and Frean, Marcus R.},
  date = {2004},
  volume = {17},
  pages = {217--224},
  url = {https://papers.nips.cc/paper/2561-dependent-gaussian-processes.pdf},
  urldate = {2017-01-27},
  file = {C\:\\Users\\pub\\Zotero\\storage\\HJT7BPIT\\Boyle and Frean - 2004 - Dependent Gaussian Processes..pdf}
}

@report{boyle_multiple_2005,
  title = {Multiple Output Gaussian Process Regression},
  author = {Boyle, Phillip and Frean, Marcus and Boyle, Phillip and Frean, Marcus},
  date = {2005},
  abstract = {Gaussian processes are usually parameterised in terms of their covariance functions. However, this makes it difficult to deal with multiple outputs, because ensuring that the covariance matrix is positive definite is problematic. An alternative formulation is to treat Gaussian processes as white noise sources convolved with smoothing kernels, and to parameterise the kernel instead. Using this, we extend Gaussian processes to handle multiple, coupled outputs. 1},
  file = {C\:\\Users\\pub\\Zotero\\storage\\STU7NV59\\Boyle et al. - 2005 - Multiple output gaussian process regression.pdf;C\:\\Users\\pub\\Zotero\\storage\\ZWMMCM3F\\summary.html}
}

@article{briol_probabilistic_2017,
  title = {Probabilistic {{Integration}}: {{A Role}} in {{Statistical Computation}}?},
  shorttitle = {Probabilistic {{Integration}}},
  author = {Briol, François-Xavier and Oates, Chris J. and Girolami, Mark and Osborne, Michael A. and Sejdinovic, Dino},
  date = {2017-10-18},
  url = {http://arxiv.org/abs/1512.00933},
  urldate = {2020-02-20},
  abstract = {A research frontier has emerged in scientific computation, wherein numerical error is regarded as a source of epistemic uncertainty that can be modelled. This raises several statistical challenges, including the design of statistical methods that enable the coherent propagation of probabilities through a (possibly deterministic) computational work-flow. This paper examines the case for probabilistic numerical methods in routine statistical computation. Our focus is on numerical integration, where a probabilistic integrator is equipped with a full distribution over its output that reflects the presence of an unknown numerical error. Our main technical contribution is to establish, for the first time, rates of posterior contraction for these methods. These show that probabilistic integrators can in principle enjoy the "best of both worlds", leveraging the sampling efficiency of Monte Carlo methods whilst providing a principled route to assess the impact of numerical error on scientific conclusions. Several substantial applications are provided for illustration and critical evaluation, including examples from statistical modelling, computer graphics and a computer model for an oil reservoir.},
  archivePrefix = {arXiv},
  eprint = {1512.00933},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\439DVI6X\\Briol et al. - 2017 - Probabilistic Integration A Role in Statistical C.pdf;C\:\\Users\\pub\\Zotero\\storage\\PIGM8TG3\\1512.html},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{brockman_openai_2016,
  title = {{{OpenAI Gym}}},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  date = {2016-06-05},
  url = {http://arxiv.org/abs/1606.01540},
  urldate = {2018-09-24},
  abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
  archivePrefix = {arXiv},
  eprint = {1606.01540},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\Y7R3KYIX\\Brockman et al. - 2016 - OpenAI Gym.pdf;C\:\\Users\\pub\\Zotero\\storage\\IEKJED9E\\1606.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{choi_choicenet_2018,
  title = {{{ChoiceNet}}: {{Robust Learning}} by {{Revealing Output Correlations}}},
  shorttitle = {{{ChoiceNet}}},
  author = {Choi, Sungjoon and Hong, Sanghoon and Lim, Sungbin},
  date = {2018-05-16},
  url = {http://arxiv.org/abs/1805.06431},
  urldate = {2018-08-23},
  abstract = {In this paper, we focus on the supervised learning problem with corrupted training data. We assume that the training dataset is generated from a mixture of a target distribution and other unknown distributions. We estimate the quality of each data by revealing the correlation between the generated distribution and the target distribution. To this end, we present a novel framework referred to here as ChoiceNet that can robustly infer the target distribution in the presence of inconsistent data. We demonstrate that the proposed framework is applicable to both classification and regression tasks. ChoiceNet is evaluated in comprehensive experiments, where we show that it constantly outperforms existing baseline methods in the handling of noisy data. Particularly, ChoiceNet is successfully applied to autonomous driving tasks where it learns a safe driving policy from a dataset with mixed qualities. In the classification task, we apply the proposed method to the MNIST and CIFAR-10 datasets and it shows superior performances in terms of robustness to noisy labels.},
  archivePrefix = {arXiv},
  eprint = {1805.06431},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\F886D4JS\\Choi et al. - 2018 - ChoiceNet Robust Learning by Revealing Output Cor.pdf;C\:\\Users\\pub\\Zotero\\storage\\M6QSGCFL\\1805.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{choi_robust_2016,
  title = {Robust Learning from Demonstration Using Leveraged {{Gaussian}} Processes and Sparse-Constrained Optimization},
  booktitle = {Robotics and {{Automation}} ({{ICRA}}), 2016 {{IEEE International Conference}} On},
  author = {Choi, Sungjoon and Lee, Kyungjae and Oh, Songhwai},
  date = {2016},
  pages = {470--475},
  publisher = {{IEEE}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\LNLKLPLD\\Choi et al. - 2016 - Robust learning from demonstration using leveraged.pdf;C\:\\Users\\pub\\Zotero\\storage\\AGU553L3\\7487168.html}
}

@book{coburn_geostatistics_2000,
  title = {Geostatistics for Natural Resources Evaluation},
  author = {Coburn, Timothy C.},
  date = {2000},
  publisher = {{Taylor \& Francis Group}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\QKNJSFHQ\\auD.html}
}

@article{cockayne_bayesian_2019,
  title = {Bayesian {{Probabilistic Numerical Methods}}},
  author = {Cockayne, Jon and Oates, Chris and Sullivan, Tim and Girolami, Mark},
  date = {2019-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {61},
  pages = {756--789},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/17M1139357},
  url = {http://arxiv.org/abs/1702.03673},
  urldate = {2020-02-14},
  abstract = {The emergent field of probabilistic numerics has thus far lacked clear statistical principals. This paper establishes Bayesian probabilistic numerical methods as those which can be cast as solutions to certain inverse problems within the Bayesian framework. This allows us to establish general conditions under which Bayesian probabilistic numerical methods are well-defined, encompassing both non-linear and non-Gaussian models. For general computation, a numerical approximation scheme is proposed and its asymptotic convergence established. The theoretical development is then extended to pipelines of computation, wherein probabilistic numerical methods are composed to solve more challenging numerical tasks. The contribution highlights an important research frontier at the interface of numerical analysis and uncertainty quantification, with a challenging industrial application presented.},
  archivePrefix = {arXiv},
  eprint = {1702.03673},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\IYE9Y4P8\\Cockayne et al. - 2019 - Bayesian Probabilistic Numerical Methods.pdf;C\:\\Users\\pub\\Zotero\\storage\\X8T74HX4\\1702.html},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Methodology},
  number = {3}
}

@article{cockayne_probabilistic_2017,
  title = {Probabilistic {{Numerical Methods}} for {{Partial Differential Equations}} and {{Bayesian Inverse Problems}}},
  author = {Cockayne, Jon and Oates, Chris and Sullivan, Tim and Girolami, Mark},
  date = {2017-07-11},
  url = {http://arxiv.org/abs/1605.07811},
  urldate = {2020-02-18},
  abstract = {This paper develops a probabilistic numerical method for solution of partial differential equations (PDEs) and studies application of that method to PDE-constrained inverse problems. This approach enables the solution of challenging inverse problems whilst accounting, in a statistically principled way, for the impact of discretisation error due to numerical solution of the PDE. In particular, the approach confers robustness to failure of the numerical PDE solver, with statistical inferences driven to be more conservative in the presence of substantial discretisation error. Going further, the problem of choosing a PDE solver is cast as a problem in the Bayesian design of experiments, where the aim is to minimise the impact of solver error on statistical inferences; here the challenge of non-linear PDEs is also considered. The method is applied to parameter inference problems in which discretisation error in non-negligible and must be accounted for in order to reach conclusions that are statistically valid.},
  archivePrefix = {arXiv},
  eprint = {1605.07811},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\PDK3PR2J\\Cockayne et al. - 2017 - Probabilistic Numerical Methods for Partial Differ.pdf;C\:\\Users\\pub\\Zotero\\storage\\MIWRP8CG\\1605.html},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Methodology},
  primaryClass = {cs, math, stat},
  version = {3}
}

@article{cox_review_1993,
  title = {A Review of Statistical Data Association Techniques for Motion Correspondence},
  author = {Cox, Ingemar J.},
  date = {1993-02-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vision},
  volume = {10},
  pages = {53--66},
  issn = {1573-1405},
  doi = {10.1007/BF01440847},
  url = {https://doi.org/10.1007/BF01440847},
  urldate = {2020-03-04},
  abstract = {Motion correspondence is a fundamental problem in computer vision and many other disciplines. This article describes statistical data association techniques originally developed in the context of target tracking and surveillance and now beginning to be used in dynamic motion analysis by the computer vision community. The Mahalanobis distance measure is first introduced before discussing the limitations of nearest neighbor algorithms. Then, the track-splitting, joint likelihood, multiple hypothesis algorithms are described, each method solving an increasingly more complicated optimization. Real-time constraints may prohibit the application of these optimal methods. The suboptimal joint probabilistic data association algorithm is therefore described. The advantages, limitations, and relationships between the approaches are discussed.},
  langid = {english},
  number = {1}
}

@inproceedings{damianou_deep_2013,
  title = {Deep {{Gaussian Processes}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Damianou, Andreas and Lawrence, Neil},
  date = {2013-04-29},
  pages = {207--215},
  url = {http://proceedings.mlr.press/v31/damianou13a.html},
  urldate = {2018-10-02},
  abstract = {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inpu...},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\BUXWE2UV\\Damianou and Lawrence - 2012 - Deep Gaussian Processes.pdf;C\:\\Users\\pub\\Zotero\\storage\\WKSUAVEY\\Damianou und Lawrence - 2013 - Deep Gaussian Processes.pdf;C\:\\Users\\pub\\Zotero\\storage\\K6M9IPNY\\damianou13a.html;C\:\\Users\\pub\\Zotero\\storage\\S2KB72DK\\1211.html},
  keywords = {60G15; 58E30,Computer Science - Learning,G.1.2,G.3,I.2.6,Mathematics - Probability,Statistics - Machine Learning},
  langid = {english}
}

@thesis{damianou_deep_2015,
  title = {Deep {{Gaussian}} Processes and Variational Propagation of Uncertainty},
  author = {Damianou, Andreas},
  date = {2015},
  institution = {{University of Sheffield}},
  url = {http://etheses.whiterose.ac.uk/id/eprint/9968},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\JEWANA87\\Damianou - 2015 - Deep Gaussian processes and variational propagatio.pdf;C\:\\Users\\pub\\Zotero\\storage\\DKUPAECG\\9968.html}
}

@thesis{deisenroth_efficient_2010,
  title = {Efficient {{Reinforcement Learning}} Using {{Gaussian Processes}}},
  author = {Deisenroth, Marc Peter},
  date = {2010},
  institution = {{KIT Scientific Publishing}},
  url = {http://www.cs.washington.edu/research/projects/aiweb/media/papers/tmppqidj5},
  urldate = {2016-04-19},
  file = {C\:\\Users\\pub\\Zotero\\storage\\K5TE3E38\\deisenroth.pdf;C\:\\Users\\pub\\Zotero\\storage\\63UCNMSK\\books.html}
}

@inproceedings{deisenroth_pilco_2011,
  title = {{{PILCO}}: {{A}} Model-Based and Data-Efficient Approach to Policy Search},
  shorttitle = {{{PILCO}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on Machine Learning ({{ICML}}-11)},
  author = {Deisenroth, Marc and Rasmussen, Carl E.},
  date = {2011},
  pages = {465--472},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Deisenroth_323.pdf},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\YEDBHXGB\\Deisenroth and Rasmussen - 2011 - PILCO A model-based and data-efficient approach t.pdf}
}

@inproceedings{depeweg_decomposition_2018,
  title = {Decomposition of {{Uncertainty}} in {{Bayesian Deep Learning}} for {{Efficient}} and {{Risk}}-Sensitive {{Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Depeweg, Stefan and Hernandez-Lobato, Jose-Miguel and Doshi-Velez, Finale and Udluft, Steffen},
  date = {2018},
  pages = {1192--1201},
  file = {C\:\\Users\\pub\\Zotero\\storage\\IQV8Z9UK\\Depeweg et al. - Decomposition of Uncertainty in Bayesian Deep Lear.pdf;C\:\\Users\\pub\\Zotero\\storage\\TK4ZWLEU\\Depeweg et al. - Decomposition of Uncertainty in Bayesian Deep Lear.pdf;C\:\\Users\\pub\\Zotero\\storage\\FR49N2TD\\auD.html}
}

@article{depeweg_learning_2016,
  title = {Learning and {{Policy Search}} in {{Stochastic Dynamical Systems}} with {{Bayesian Neural Networks}}},
  author = {Depeweg, Stefan and Hernández-Lobato, José Miguel and Doshi-Velez, Finale and Udluft, Steffen},
  date = {2016-05-23},
  url = {http://arxiv.org/abs/1605.07127},
  urldate = {2019-02-19},
  abstract = {We present an algorithm for policy search in stochastic dynamical systems using model-based reinforcement learning. The system dynamics are described with Bayesian neural networks (BNNs) that include stochastic input variables. These input variables allow us to capture complex statistical patterns in the transition dynamics (e.g. multi-modality and heteroskedasticity), which are usually missed by alternative modeling approaches. After learning the dynamics, our BNNs are then fed into an algorithm that performs random roll-outs and uses stochastic optimization for policy learning. We train our BNNs by minimizing α-divergences with α = 0.5, which usually produces better results than other techniques such as variational Bayes. We illustrate the performance of our method by solving a challenging problem where model-based approaches usually fail and by obtaining promising results in real-world scenarios including the control of a gas turbine and an industrial benchmark.},
  archivePrefix = {arXiv},
  eprint = {1605.07127},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\PDDB5NVV\\Depeweg et al. - 2016 - Learning and Policy Search in Stochastic Dynamical.pdf;C\:\\Users\\pub\\Zotero\\storage\\X5C6XVZT\\Depeweg et al. - 2016 - Learning and Policy Search in Stochastic Dynamical.pdf;C\:\\Users\\pub\\Zotero\\storage\\MWFZ45PK\\1605.html},
  keywords = {Computer Science - Learning,Computer Science - Machine Learning,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@book{gauss_theoria_1809,
  title = {Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientium},
  author = {Gauss, Carl Friedrich},
  date = {1809},
  publisher = {{sumtibus Frid. Perthes et IH Besser}},
  url = {https://books.google.de/books?id=VKhu8yPcat8C},
  urldate = {2016-06-06},
  file = {C\:\\Users\\pub\\Zotero\\storage\\NEPIE6G8\\books.html}
}

@inproceedings{hans_efficient_2009,
  title = {Efficient Uncertainty Propagation for Reinforcement Learning with Limited Data},
  booktitle = {International {{Conference}} on {{Artificial Neural Networks}}},
  author = {Hans, Alexander and Udluft, Steffen},
  date = {2009},
  pages = {70--79},
  publisher = {{Springer}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\28GBNUPD\\Hans und Udluft - 2009 - Efficient uncertainty propagation for reinforcemen.pdf;C\:\\Users\\pub\\Zotero\\storage\\WGKEVCHC\\978-3-642-04274-4_8.html}
}

@inproceedings{hein_benchmark_2017,
  title = {A Benchmark Environment Motivated by Industrial Control Problems},
  booktitle = {2017 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  author = {Hein, Daniel and Depeweg, Stefan and Tokic, Michel and Udluft, Steffen and Hentschel, Alexander and Runkler, Thomas A. and Sterzing, Volkmar},
  date = {2017-11},
  pages = {1--8},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/SSCI.2017.8280935},
  url = {http://ieeexplore.ieee.org/document/8280935/},
  urldate = {2019-02-19},
  abstract = {In the research area of reinforcement learning (RL), frequently novel and promising methods are developed and introduced to the RL community. However, although many researchers are keen to apply their methods on real-world problems, implementing such methods in real industry environments often is a frustrating and tedious process. Generally, academic research groups have only limited access to real industrial data and applications. For this reason, new methods are usually developed, evaluated and compared by using artificial software benchmarks. On one hand, these benchmarks are designed to provide interpretable RL training scenarios and detailed insight into the learning process of the method on hand. On the other hand, they usually do not share much similarity with industrial real-world applications. For this reason we used our industry experience to design a benchmark which bridges the gap between freely available, documented, and motivated artificial benchmarks and properties of real industrial problems. The resulting industrial benchmark (IB) has been made publicly available to the RL community by publishing its Java and Python code, including an OpenAI Gym wrapper, on Github. In this paper we motivate and describe in detail the IB’s dynamics and identify prototypic experimental settings that capture common situations in real-world industry control problems.},
  eventtitle = {2017 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  file = {C\:\\Users\\pub\\Zotero\\storage\\P9Z3ASIR\\Hein et al. - 2017 - A benchmark environment motivated by industrial co.pdf;C\:\\Users\\pub\\Zotero\\storage\\QTA4H4W9\\Hein et al. - 2017 - A benchmark environment motivated by industrial co.pdf;C\:\\Users\\pub\\Zotero\\storage\\XMHQWZRL\\8280935.html},
  isbn = {978-1-5386-2726-6},
  keywords = {academic research groups,artificial software benchmarks,Automobiles,benchmark environment,Benchmark testing,frustrating process,Games,Helicopters,industrial control,Industrial control,industrial data,Industries,industry environments,industry experience,interpretable RL training scenarios,learning (artificial intelligence),learning process,motivated artificial benchmarks,public domain software,real-world applications,real-world industry control problems,reinforcement learning,research area,RL community,tedious process,Wind turbines},
  langid = {english}
}

@inproceedings{hensman_gaussian_2013,
  title = {Gaussian {{Processes}} for {{Big Data}}},
  booktitle = {Uncertainty in {{Artificial Intelligence}}},
  author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D.},
  date = {2013},
  pages = {282},
  publisher = {{Citeseer}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\EU8WZFR4\\Hensman et al. - 2013 - Gaussian Processes for Big Data.pdf;C\:\\Users\\pub\\Zotero\\storage\\XV3VH9PJ\\Hensman et al. - 2013 - Gaussian Processes for Big Data.pdf;C\:\\Users\\pub\\Zotero\\storage\\2JAR4BNM\\1309.html;C\:\\Users\\pub\\Zotero\\storage\\9GMK8QF5\\auD.html;C\:\\Users\\pub\\Zotero\\storage\\ISZ4Z86Q\\1309.html},
  keywords = {Computer Science - Learning,Statistics - Machine Learning}
}

@article{hensman_nested_2014,
  title = {Nested {{Variational Compression}} in {{Deep Gaussian Processes}}},
  author = {Hensman, James and Lawrence, Neil D.},
  date = {2014-12-03},
  url = {http://arxiv.org/abs/1412.1370},
  urldate = {2017-07-19},
  abstract = {Deep Gaussian processes provide a flexible approach to probabilistic modelling of data using either supervised or unsupervised learning. For tractable inference approximations to the marginal likelihood of the model must be made. The original approach to approximate inference in these models used variational compression to allow for approximate variational marginalization of the hidden variables leading to a lower bound on the marginal likelihood of the model [Damianou and Lawrence, 2013]. In this paper we extend this idea with a nested variational compression. The resulting lower bound on the likelihood can be easily parallelized or adapted for stochastic variational inference.},
  archivePrefix = {arXiv},
  eprint = {1412.1370},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\ZKNA6NYN\\Hensman and Lawrence - 2014 - Nested Variational Compression in Deep Gaussian Pr.pdf;C\:\\Users\\pub\\Zotero\\storage\\UMQ96R94\\1412.html},
  keywords = {Statistics - Machine Learning},
  primaryClass = {stat}
}

@article{hensman_scalable_2015,
  title = {Scalable Variational {{Gaussian}} Process Classification},
  author = {Hensman, James and Matthews, Alexander G. de G. and Ghahramani, Zoubin},
  date = {2015},
  journaltitle = {Journal of Machine Learning Research},
  volume = {38},
  pages = {351--360},
  abstract = {Gaussian process classification is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, outperforming the state of the art on benchmark datasets. Importantly, the variational formulation can be exploited to allow classification in problems with millions of data points, as we demonstrate in experiments. Copyright 2015 by the authors.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\T4WFAQPK\\Hensman et al. - 2014 - Scalable Variational Gaussian Process Classificati.pdf;C\:\\Users\\pub\\Zotero\\storage\\5GEKF8R7\\1411.html;C\:\\Users\\pub\\Zotero\\storage\\FX4I5R8Q\\display.html},
  keywords = {Statistics - Machine Learning}
}

@article{hodge_survey_2004,
  title = {A Survey of Outlier Detection Methodologies},
  author = {Hodge, Victoria and Austin, Jim},
  date = {2004},
  journaltitle = {Artificial intelligence review},
  volume = {22},
  pages = {85--126},
  file = {C\:\\Users\\pub\\Zotero\\storage\\2KWBFKJ4\\Hodge und Austin - 2004 - A survey of outlier detection methodologies.pdf;C\:\\Users\\pub\\Zotero\\storage\\M35HYIRV\\BAIRE.0000045502.10941.html},
  number = {2}
}

@article{jacobs_adaptive_1991,
  title = {Adaptive Mixtures of Local Experts},
  author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  date = {1991},
  journaltitle = {Neural computation},
  volume = {3},
  pages = {79--87},
  file = {C\:\\Users\\pub\\Zotero\\storage\\T4BQHV93\\Jacobs et al. - 1991 - Adaptive mixtures of local experts.pdf;C\:\\Users\\pub\\Zotero\\storage\\APX3N2YA\\neco.1991.3.1.html;C\:\\Users\\pub\\Zotero\\storage\\WZAY9YTW\\auD.html},
  number = {1}
}

@book{journel_mining_1978,
  title = {Mining Geostatistics},
  author = {Journel, Andre G. and Huijbregts, Ch J.},
  date = {1978},
  publisher = {{Academic press}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\T8DUJGCJ\\27687.pdf}
}

@incollection{kaiser_bayesian_2018,
  title = {Bayesian {{Alignments}} of {{Warped Multi}}-{{Output Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Kaiser, Markus and Otte, Clemens and Runkler, Thomas and Ek, Carl Henrik},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {6995--7004},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/7931-bayesian-alignments-of-warped-multi-output-gaussian-processes.pdf},
  urldate = {2019-01-23},
  file = {C\:\\Users\\pub\\Zotero\\storage\\MJQDUDFP\\Kaiser et al. - 2017 - Bayesian Alignments of Warped Multi-Output Gaussia.pdf;C\:\\Users\\pub\\Zotero\\storage\\N8FXLQBZ\\Kaiser et al. - 2018 - Bayesian Alignments of Warped Multi-Output Gaussia.pdf;C\:\\Users\\pub\\Zotero\\storage\\P98SK2MP\\Kaiser et al. - 2017 - Bayesian Alignments of Warped Multi-Output Gaussia.pdf;C\:\\Users\\pub\\Zotero\\storage\\5KMTSPFI\\7931-bayesian-alignments-of-warped-multi-output-gaussian-processes.html;C\:\\Users\\pub\\Zotero\\storage\\AHX4Q83I\\1710.html;C\:\\Users\\pub\\Zotero\\storage\\UKGW6CEX\\1710.html},
  keywords = {Computer Science - Learning,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{kaiser_data_2018,
  title = {Data {{Association}} with {{Gaussian Processes}}},
  author = {Kaiser, Markus and Otte, Clemens and Runkler, Thomas and Ek, Carl Henrik},
  date = {2018-10-16},
  url = {http://arxiv.org/abs/1810.07158},
  urldate = {2019-02-14},
  abstract = {The data association problem is concerned with separating data coming from different generating processes, for example when data come from different data sources, contain significant noise, or exhibit multimodality. We present a fully Bayesian approach to this problem. Our model is capable of simultaneously solving the data association problem and the induced supervised learning problems. Underpinning our approach is the use of Gaussian process priors to encode the structure of both the data and the data associations. We present an efficient learning scheme based on doubly stochastic variational inference and discuss how it can be applied to deep Gaussian process priors.},
  archivePrefix = {arXiv},
  eprint = {1810.07158},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\EC5P3VNZ\\Kaiser et al. - Data Association with Gaussian Processes.pdf;C\:\\Users\\pub\\Zotero\\storage\\ZB65CUNK\\Kaiser et al. - 2018 - Data Association with Gaussian Processes.pdf;C\:\\Users\\pub\\Zotero\\storage\\S7VSJTCS\\1810.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{kingma_variational_2015,
  title = {Variational {{Dropout}} and the {{Local Reparameterization Trick}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Kingma, Diederik P and Salimans, Tim and Welling, Max},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  date = {2015},
  pages = {2575--2583},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick.pdf},
  urldate = {2018-09-12},
  file = {C\:\\Users\\pub\\Zotero\\storage\\89SIZL5F\\Kingma et al. - 2015 - Variational Dropout and the Local Reparameterizati.pdf;C\:\\Users\\pub\\Zotero\\storage\\VE5EGL5C\\5666-variational-dropout-and-the-local-reparameterization-trick.html}
}

@incollection{lange_batch_2012,
  title = {Batch Reinforcement Learning},
  booktitle = {Reinforcement Learning},
  author = {Lange, Sascha and Gabel, Thomas and Riedmiller, Martin},
  date = {2012},
  pages = {45--73},
  publisher = {{Springer}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\9AQ568ZU\\978-3-642-27645-3_2.html}
}

@inproceedings{lazaro-gredilla_bayesian_2012,
  title = {Bayesian Warped {{Gaussian}} Processes},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lázaro-Gredilla, Miguel},
  date = {2012},
  pages = {1619--1627},
  url = {http://papers.nips.cc/paper/4494-bayesian-warped-gaussian-processes},
  urldate = {2016-12-06},
  file = {C\:\\Users\\pub\\Zotero\\storage\\NTS9SDBA\\Lázaro-Gredilla - 2012 - Bayesian warped Gaussian processes.pdf;C\:\\Users\\pub\\Zotero\\storage\\HFAKAI4X\\4494-bayesian-warped-gaussian-processes.html}
}

@article{lazaro-gredilla_overlapping_2012,
  title = {Overlapping Mixtures of {{Gaussian}} Processes for the Data Association Problem},
  author = {Lázaro-Gredilla, Miguel and Van Vaerenbergh, Steven and Lawrence, Neil D.},
  date = {2012},
  journaltitle = {Pattern Recognition},
  volume = {45},
  pages = {1386--1395},
  file = {C\:\\Users\\pub\\Zotero\\storage\\9PWMT93W\\Lázaro-Gredilla et al. - 2011 - Overlapping Mixtures of Gaussian Processes for the.pdf;C\:\\Users\\pub\\Zotero\\storage\\GSDXF73X\\Lázaro-Gredilla et al. - 2012 - Overlapping mixtures of Gaussian processes for the.pdf;C\:\\Users\\pub\\Zotero\\storage\\HZYC3SEH\\S0031320311004109.html;C\:\\Users\\pub\\Zotero\\storage\\WXY7PLDG\\1108.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  number = {4}
}

@article{levine_reinforcement_2018,
  title = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}: {{Tutorial}} and {{Review}}},
  shorttitle = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}},
  author = {Levine, Sergey},
  date = {2018-05-20},
  url = {http://arxiv.org/abs/1805.00909},
  urldate = {2020-02-18},
  abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
  archivePrefix = {arXiv},
  eprint = {1805.00909},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\N8D2BMI9\\Levine - 2018 - Reinforcement Learning and Control as Probabilisti.pdf;C\:\\Users\\pub\\Zotero\\storage\\FN6PF6J5\\1805.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{maddison_concrete_2016,
  title = {The {{Concrete Distribution}}: {{A Continuous Relaxation}} of {{Discrete Random Variables}}},
  shorttitle = {The {{Concrete Distribution}}},
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  date = {2016-11-02},
  url = {http://arxiv.org/abs/1611.00712},
  urldate = {2018-09-12},
  abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
  archivePrefix = {arXiv},
  eprint = {1611.00712},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\4JCGSNBV\\Maddison et al. - 2016 - The Concrete Distribution A Continuous Relaxation.pdf;C\:\\Users\\pub\\Zotero\\storage\\M4WZ8CC4\\1611.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{matthews_gpflow_2017,
  title = {{{GPflow}}: {{A Gaussian}} Process Library Using {{TensorFlow}}},
  shorttitle = {{{GPflow}}},
  author = {Matthews, Alexander G. de G. and van der Wilk, Mark and Nickson, Tom and Fujii, Keisuke and Boukouvalas, Alexis and León-Villagrá, Pablo and Ghahramani, Zoubin and Hensman, James},
  date = {2017},
  journaltitle = {Journal of Machine Learning Research},
  volume = {18},
  pages = {1--6},
  url = {http://www.jmlr.org/papers/volume18/16-537/16-537.pdf},
  urldate = {2017-09-27},
  file = {C\:\\Users\\pub\\Zotero\\storage\\X6QGAFR8\\Matthews et al. - 2017 - GPflow A Gaussian process library using TensorFlo.pdf},
  number = {40},
  options = {useprefix=true}
}

@inproceedings{minka_expectation_2001,
  title = {Expectation {{Propagation}} for Approximate {{Bayesian}} Inference},
  booktitle = {Proceedings of the {{Seventeenth}} Conference on {{Uncertainty}} in Artificial Intelligence},
  author = {Minka, Thomas P.},
  date = {2001},
  pages = {362--369},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  url = {http://arxiv.org/abs/1301.2294},
  urldate = {2016-09-06},
  abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, "Expectation Propagation", unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. All three algorithms try to recover an approximate distribution which is close in KL divergence to the true distribution. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining certain expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Expectation Propagation also extends belief propagation in the opposite direction - it can propagate richer belief states that incorporate correlations between nodes. Experiments with Gaussian mixture models show Expectation Propagation to be convincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.},
  archivePrefix = {arXiv},
  eprint = {1301.2294},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\KNSHS9MI\\Minka - 2013 - Expectation Propagation for approximate Bayesian i.pdf;C\:\\Users\\pub\\Zotero\\storage\\3UKPDVIC\\1301.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning}
}

@book{murphy_machine_2012,
  title = {Machine {{Learning}}: {{A Probabilistic Perspective}}},
  shorttitle = {Machine {{Learning}}},
  author = {Murphy, Kevin P.},
  date = {2012-08-24},
  publisher = {{MIT Press}},
  abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package -- PMTK (probabilistic modeling toolkit) -- that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\FERQXL3E\\17.pdf},
  isbn = {978-0-262-01802-9},
  keywords = {Computers / Machine Theory,Textbook},
  langid = {english},
  pagetotal = {1098}
}

@article{oates_modern_2019,
  title = {A {{Modern Retrospective}} on {{Probabilistic Numerics}}},
  author = {Oates, C. J. and Sullivan, T. J.},
  date = {2019-01-14},
  url = {http://arxiv.org/abs/1901.04457},
  urldate = {2019-10-02},
  abstract = {This article attempts to place the emergence of probabilistic numerics as a mathematical-statistical research field within its historical context and to explore how its gradual development can be related both to applications and to a modern formal treatment. We highlight in particular the parallel contributions of Sul'din and Larkin in the 1960s and how their pioneering early ideas have reached a degree of maturity in the intervening period, mediated by paradigms such as average-case analysis and information-based complexity. We provide a subjective assessment of the state of research in probabilistic numerics and highlight some difficulties to be addressed by future works.},
  archivePrefix = {arXiv},
  eprint = {1901.04457},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\D67KFS6E\\Oates and Sullivan - 2019 - A Modern Retrospective on Probabilistic Numerics.pdf;C\:\\Users\\pub\\Zotero\\storage\\HUTII4YQ\\1901.html},
  keywords = {62-03; 65-03; 01A60; 01A65; 01A67,Mathematics - History and Overview,Mathematics - Numerical Analysis,Mathematics - Probability,Statistics - Machine Learning},
  primaryClass = {math, stat}
}

@article{petersen_matrix_2008,
  title = {The Matrix Cookbook},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind and others},
  date = {2008},
  journaltitle = {Technical University of Denmark},
  volume = {7},
  pages = {15},
  url = {http://www.cim.mcgill.ca/~dudek/417/Papers/matrixOperations.pdf},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\N4W86FFD\\imm3274.pdf}
}

@book{rasmussen_gaussian_2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward},
  date = {2006},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.86.3414},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\TFXHMXDE\\summary.html}
}

@inproceedings{rasmussen_infinite_2002,
  title = {Infinite {{Mixtures}} of {{Gaussian Process Experts}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 14},
  author = {Rasmussen, Carl E. and Ghahramani, Zoubin},
  editor = {Dietterich, T. G. and Becker, S. and Ghahramani, Z.},
  date = {2002},
  pages = {881--888},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/2055-infinite-mixtures-of-gaussian-process-experts.pdf},
  urldate = {2018-08-23},
  file = {C\:\\Users\\pub\\Zotero\\storage\\VHKBGGWV\\Rasmussen und Ghahramani - 2002 - Infinite Mixtures of Gaussian Process Experts.pdf;C\:\\Users\\pub\\Zotero\\storage\\3YP3XGSE\\2055-infinite-mixtures-of-gaussian-process-experts.html}
}

@article{rezende_stochastic_2014,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  date = {2014-01-16},
  url = {https://arxiv.org/abs/1401.4082},
  urldate = {2018-09-12},
  file = {C\:\\Users\\pub\\Zotero\\storage\\N244K3J2\\Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf;C\:\\Users\\pub\\Zotero\\storage\\LEJMJNS8\\1401.html},
  langid = {english}
}

@inproceedings{riedmiller_neural_2005,
  title = {Neural Fitted {{Q}} Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method},
  booktitle = {European {{Conference}} on {{Machine Learning}}},
  author = {Riedmiller, Martin},
  date = {2005},
  pages = {317--328},
  publisher = {{Springer}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\3JSSKLJS\\Riedmiller - 2005 - Neural fitted Q iteration–first experiences with a.pdf;C\:\\Users\\pub\\Zotero\\storage\\SCF964Y7\\11564096_32.html}
}

@book{rousseeuw_robust_2005,
  title = {Robust Regression and Outlier Detection},
  author = {Rousseeuw, Peter J. and Leroy, Annick M.},
  date = {2005},
  volume = {589},
  publisher = {{John wiley \& sons}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\X4BEMJQI\\books.html}
}

@inproceedings{salimbeni_doubly_2017,
  title = {Doubly {{Stochastic Variational Inference}} for {{Deep Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Salimbeni, Hugh and Deisenroth, Marc},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  pages = {4588--4599},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.pdf},
  urldate = {2018-10-02},
  file = {C\:\\Users\\pub\\Zotero\\storage\\FTCRG5BC\\Salimbeni und Deisenroth - 2017 - Doubly Stochastic Variational Inference for Deep G.pdf;C\:\\Users\\pub\\Zotero\\storage\\TX47KKYE\\Salimbeni und Deisenroth - 2017 - Doubly Stochastic Variational Inference for Deep G.pdf;C\:\\Users\\pub\\Zotero\\storage\\AP6UXDGD\\1705.html;C\:\\Users\\pub\\Zotero\\storage\\FNPVJ7AA\\7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.html;C\:\\Users\\pub\\Zotero\\storage\\GVUNMT8K\\display.html},
  keywords = {Statistics - Machine Learning}
}

@article{schepers_improved_2007,
  title = {Improved Modelling of Wake Aerodynamics and Assessment of New Farm Control Strategies},
  author = {Schepers, J G and van der Pijl, S P},
  date = {2007-07-01},
  journaltitle = {Journal of Physics: Conference Series},
  shortjournal = {J. Phys.: Conf. Ser.},
  volume = {75},
  pages = {012039},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/75/1/012039},
  url = {http://stacks.iop.org/1742-6596/75/i=1/a=012039?key=crossref.77f9d84ce31d5b802648696d4d6f1f03},
  urldate = {2020-03-05}
}

@book{scholkopf_learning_2002,
  title = {Learning with {{Kernels}}: {{Support Vector Machines}}, {{Regularization}}, {{Optimization}}, and {{Beyond}}},
  shorttitle = {Learning with {{Kernels}}},
  author = {Schölkopf, Bernhard and Smola, Alexander J.},
  date = {2002-01},
  publisher = {{MIT Press}},
  abstract = {In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -- -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.},
  isbn = {978-0-262-19475-4},
  keywords = {Computers / Intelligence (AI) & Semantics,Computers / Programming / General},
  langid = {english},
  pagetotal = {658}
}

@book{shalev-shwartz_understanding_2014,
  title = {Understanding {{Machine Learning}}},
  author = {Shalev-Shwartz, Shai and Ben-David, Shai},
  date = {2014},
  file = {C\:\\Users\\pub\\Zotero\\storage\\6NSW4FQU\\Shalev-Shwartz and Ben-David - Understanding Machine Learning.pdf},
  langid = {english}
}

@article{silver_mastering_2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  date = {2017-10},
  journaltitle = {Nature},
  volume = {550},
  pages = {354--359},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  url = {https://www.nature.com/articles/nature24270},
  urldate = {2019-04-04},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\9VNUJ3GJ\\Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf;C\:\\Users\\pub\\Zotero\\storage\\DLDP8I3H\\nature24270.html;C\:\\Users\\pub\\Zotero\\storage\\RE8QUYV6\\nature24270.html;C\:\\Users\\pub\\Zotero\\storage\\SBYNLYP2\\display.html;C\:\\Users\\pub\\Zotero\\storage\\ZW2KG8K4\\nature24270.html},
  langid = {english},
  number = {7676},
  options = {useprefix=true}
}

@thesis{snelson_flexible_2007,
  title = {Flexible and Efficient {{Gaussian}} Process Models for Machine Learning},
  author = {Snelson, Edward Lloyd},
  date = {2007},
  institution = {{Citeseer}},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.4041},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\T34X6F3F\\Snelson - 2007 - Flexible and efficient Gaussian process models for.pdf}
}

@inproceedings{snelson_sparse_2005,
  title = {Sparse {{Gaussian}} Processes Using Pseudo-Inputs},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Snelson, Edward and Ghahramani, Zoubin},
  date = {2005},
  pages = {1257--1264},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2005_543.pdf},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\6U2PZBFH\\Snelson and Ghahramani - 2005 - Sparse Gaussian processes using pseudo-inputs.pdf}
}

@article{snelson_warped_2004,
  title = {Warped Gaussian Processes},
  author = {Snelson, Edward and Rasmussen, Carl Edward and Ghahramani, Zoubin},
  date = {2004},
  journaltitle = {Advances in neural information processing systems},
  volume = {16},
  pages = {337--344},
  url = {https://books.google.de/books?hl=de&lr=&id=0F-9C7K8fQ8C&oi=fnd&pg=PA337&dq=bayesian+warped+gaussian+processes&ots=THLrkTTe95&sig=-m3ipE2FEwJhMaXgmZmnzNQdRnE},
  urldate = {2016-12-06},
  file = {C\:\\Users\\pub\\Zotero\\storage\\SDTMP9GR\\Snelson et al. - 2004 - Warped gaussian processes.pdf;C\:\\Users\\pub\\Zotero\\storage\\AP3BH28T\\books.html}
}

@article{snoek_input_2014,
  title = {Input {{Warping}} for {{Bayesian Optimization}} of {{Non}}-Stationary {{Functions}}},
  author = {Snoek, Jasper and Swersky, Kevin and Zemel, Richard S. and Adams, Ryan P.},
  date = {2014-02-04},
  url = {http://arxiv.org/abs/1402.0929},
  urldate = {2017-07-31},
  abstract = {Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions. The ability to accurately model distributions over functions is critical to the effectiveness of Bayesian optimization. Although Gaussian processes provide a flexible prior over functions which can be queried efficiently, there are various classes of functions that remain difficult to model. One of the most frequently occurring of these is the class of non-stationary functions. The optimization of the hyperparameters of machine learning algorithms is a problem domain in which parameters are often manually transformed a priori, for example by optimizing in "log-space," to mitigate the effects of spatially-varying length scale. We develop a methodology for automatically learning a wide family of bijective transformations or warpings of the input space using the Beta cumulative distribution function. We further extend the warping framework to multi-task Bayesian optimization so that multiple tasks can be warped into a jointly stationary space. On a set of challenging benchmark optimization tasks, we observe that the inclusion of warping greatly improves on the state-of-the-art, producing better results faster and more reliably.},
  archivePrefix = {arXiv},
  eprint = {1402.0929},
  eprinttype = {arxiv},
  file = {C\:\\Users\\pub\\Zotero\\storage\\ELZ9CMFF\\Snoek et al. - 2014 - Input Warping for Bayesian Optimization of Non-sta.pdf;C\:\\Users\\pub\\Zotero\\storage\\5ECB3EET\\1402.html},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{soleimanzadeh_controller_2011,
  title = {Controller Design for a Wind Farm, Considering Both Power and Load Aspects},
  author = {Soleimanzadeh, Maryam and Wisniewski, Rafael},
  date = {2011-06-01},
  journaltitle = {Mechatronics},
  shortjournal = {Mechatronics},
  volume = {21},
  pages = {720--727},
  issn = {0957-4158},
  doi = {10.1016/j.mechatronics.2011.02.008},
  url = {http://www.sciencedirect.com/science/article/pii/S0957415811000328},
  urldate = {2020-03-05},
  abstract = {In this paper, a wind farm controller is developed that distributes power references among wind turbines while it reduces their structural loads. The proposed controller is based on a spatially discrete model of the farm, which delivers an approximation of wind speed in the vicinity of each wind turbine. The control algorithm determines the reference signals for each individual wind turbine controller in two scenarios based on low and high wind speed. In low wind speed, the reference signals for rotor speed are adjusted, taking the trade-off between power maximization and load minimization into account. In high wind speed, the power and pitch reference signals are determined while structural loads are minimized. To the best of authors’ knowledge, the proposed dynamical model is a suitable framework for control, since it provides a dynamic structure for behavior of the flow in wind farms. Moreover, the controller has been proven exceptionally useful in solving the problem of both power and load optimization on the basis of this model.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\VSDM6SDF\\S0957415811000328.html},
  keywords = {Load control,Wind farm control,Wind flow model,Wind turbine},
  langid = {english},
  number = {4}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018},
  edition = {2},
  publisher = {{MIT Press}},
  location = {{Cambridge, Mass}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\6EQF4BV3\\Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf},
  isbn = {978-0-262-19398-6},
  keywords = {Reinforcement learning},
  langid = {english},
  pagetotal = {322},
  series = {Adaptive Computation and Machine Learning}
}

@article{tensorflow2015-whitepaper,
  title = {{{TensorFlow}}: {{Large}}-{{Scale Machine Learning}} on {{Heterogeneous Systems}}},
  author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  date = {2015},
  url = {https://www.tensorflow.org/},
  file = {C\:\\Users\\pub\\Zotero\\storage\\WLKK3HI7\\Abadi et al. - 2015 - TensorFlow Large-Scale Machine Learning on Hetero.pdf},
  note = {Software available from tensorflow.org}
}

@inproceedings{titsias_bayesian_2010,
  title = {Bayesian {{Gaussian}} Process Latent Variable Model},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Titsias, Michalis K. and Lawrence, Neil D.},
  date = {2010},
  pages = {844--851},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_TitsiasL10.pdf},
  urldate = {2016-02-01},
  file = {C\:\\Users\\pub\\Zotero\\storage\\5HPG3ZG9\\Titsias and Lawrence - 2010 - Bayesian Gaussian process latent variable model.pdf}
}

@inproceedings{titsias_variational_2009,
  title = {Variational {{Learning}} of {{Inducing Variables}} in {{Sparse Gaussian Processes}}.},
  booktitle = {{{AISTATS}}},
  author = {Titsias, Michalis K.},
  date = {2009},
  volume = {5},
  pages = {567--574},
  url = {http://www.jmlr.org/proceedings/papers/v5/titsias09a/titsias09a.pdf},
  urldate = {2017-04-06},
  file = {C\:\\Users\\pub\\Zotero\\storage\\UTMCPPXS\\Titsias - 2009 - Variational Learning of Inducing Variables in Spar.pdf}
}

@inproceedings{tresp_mixtures_2001,
  title = {Mixtures of {{Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 13},
  author = {Tresp, Volker},
  editor = {Leen, T. K. and Dietterich, T. G. and Tresp, V.},
  date = {2001},
  pages = {654--660},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/1900-mixtures-of-gaussian-processes.pdf},
  urldate = {2018-09-26},
  file = {C\:\\Users\\pub\\Zotero\\storage\\C7SSU5TS\\Tresp - 2001 - Mixtures of Gaussian Processes.pdf;C\:\\Users\\pub\\Zotero\\storage\\7GPFZX3C\\1900-mixtures-of-gaussian-processes.html}
}

@article{tresp_wet_1994,
  title = {The Wet Game of Chicken},
  author = {Tresp, Volker},
  date = {1994},
  journaltitle = {Siemens AG, CT IC 4, Technical Report}
}

@inproceedings{zhou_generalized_2012,
  title = {Generalized Time Warping for Multi-Modal Alignment of Human Motion},
  booktitle = {Computer {{Vision}} and {{Pattern Recognition}} ({{CVPR}}), 2012 {{IEEE Conference}} On},
  author = {Zhou, Feng and De la Torre, Fernando},
  date = {2012},
  pages = {1282--1289},
  publisher = {{IEEE}},
  file = {C\:\\Users\\pub\\Zotero\\storage\\H9KUZQZQ\\Zhou und De la Torre - 2012 - Generalized time warping for multi-modal alignment.pdf;C\:\\Users\\pub\\Zotero\\storage\\A5RQ27H7\\auD.html}
}

@article{ziebart_modeling_2010,
  title = {Modeling {{Interaction}} via the {{Principle}} of {{Maximum Causal Entropy}}},
  author = {Ziebart, Brian D and Bagnell, J Andrew and Dey, Anind K},
  date = {2010},
  pages = {8},
  abstract = {The principle of maximum entropy provides a powerful framework for statistical models of joint, conditional, and marginal distributions. However, there are many important distributions with elements of interaction and feedback where its applicability has not been established. This work presents the principle of maximum causal entropy—an approach based on causally conditioned probabilities that can appropriately model the availability and influence of sequentially revealed side information. Using this principle, we derive models for sequential data with revealed information, interaction, and feedback, and demonstrate their applicability for statistically framing inverse optimal control and decision prediction tasks.},
  file = {C\:\\Users\\pub\\Zotero\\storage\\KJJBGJPC\\Ziebart et al. - Modeling Interaction via the Principle of Maximum .pdf},
  langid = {english}
}


