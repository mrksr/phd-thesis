\chapter{Introduction}
\label{toc:introduction}
In many machine learning problems, the task is to derive a model for observational data that has been generated by some process in nature that is not fully understood.
Starting with an input-vector~$\mat{x}$ describing its initial state, nature produces an output-vector~$\mat{y}$.
Given a set of observational pairs $(\mat{x}, \mat{y})$, we want to learn about the underlying, possibly stochastic, functional dependency $\mat{y} = f(\mat{x})$.
The goal of learning is often to either gain new knowledge about nature by describing the data or to be able to predict outputs $\mat{y}_\ast$ for previously unseen inputs $\mat{x}_\ast$.
While traditional statistics favor what \textcite{breiman_statistical_2001,shmueli_explain_2010} call the white-box modeling culture that focuses on explaining observational data using causal theoretic models, machine learning is more closely associated with the black-box culture.
When formulating a black-box model, we accept that we cannot model nature directly and instead focus on the functional dependency $f$ between inputs and outputs.
Using general function approximators like neural networks or Gaussian processes, any smooth dependency can be approximated given enough observations.
With growing computational power and the availability of large data sets, black-box approaches have led to impressive advancements in domains that are hard to formalize.
Successes have been seen in a wide range of digital tasks such as speech recognition~\parencite{hochreiter_long_1997,chorowski_attention-based_2015,bahdanau_neural_2014,chorowski_attention-based_2015}, computer vision~\parencite{russakovsky_imagenet_2015,lecun_backpropagation_1989}, video games~\parencite{berner_dota_2019,silver_mastering_2016}, or machine translation~\parencite{johnson_googles_2017}.
In such domains, data is often abundant, which shifts the focus away from traditional statistical models to approaches that are highly adaptable and scalable.
At the same time, the consequences of mistakes tend to be mild, enabling fast prototyping and heuristic design choices that can be validated in the deployed systems using the performance metrics of interest.

However, bridging the gap to applications in critical systems in the physical world has proved challenging.
Problem domains like robotics or industrial control introduce a new set of requirements.
In industrial control, machine learning is used to find more efficient or safer control strategies not obvious to the engineers designing a machine~\parencite{hein_benchmark_2017}.
In such scenarios, relevant data are often scarce.
The most valuable data, such as when a machine will fail or how it will behave in new situations, is never produced.
As applications become more safety-relevant, gathering data through exploration can be problematic due to the adverse consequences of failure.
If they are to operate in safety-critical areas, interact with people, or carry responsibility, machine learning systems must be robust, trustworthy, and assessable.
Besides optimizing for good average-case performance, a responsible system needs to reason about plausible worst-cases to be able to avoid them.
To ensure trust, ML-systems often need to be verified by domain experts before deployment, emphasizing a need for interpretability.
Models are required to incorporate and reproduce expert knowledge and make consistent predictions, while at the same time yielding new insights from data.

The highly adaptable and scalable models that lead to success in digital domains are not well-suited to be deployed in safety-critical environments.
Large neural networks generally suffer from low interpretability~\parencite{rudin_stop_2019}, the need for large amounts of data and their inability to quantify uncertainties~\parencite{goodfellow_deep_2016}.
In this work, we explore how to formulate machine learning models that can satisfy the requirements of physical environments.


\section{Bayesian structured models with Gaussian processes}
A key technique that allows us to cope with the requirements of critical systems in the physical world is probabilistic modeling.
Bayesian inference offers a framework to formalize expert knowledge in prior assumptions and combine them with data.
Expert
Using Bayes' rule, a distribution of plausible models is reweighed by their ability to generate data to yield a posterior distribution over plausible explanations.
To make predictions, all plausible models are taken into account, allowing us to both quantify confidence in predictions and take more unlikely but relevant scenarios into consideration.
Bayesian reasoning was pioneered by Pierre-Simon Laplace in the eighteenth century to extend the prevalent mechanistic world-view with the idea that knowledge is imperfect~\parencite{pulskamp_laplace_2020}.
In modern science, probabilistic models are used in large-scale experiments in particle physics~\parencite{khachatryan_observation_2015} or astronomy~\parencite{collaboration_first_2019,the_ligo_scientific_collaboration_observation_2016} where randomness plays a major role.
These Bayesian scientific models are generally formulated from a hypothesis testing perspective and follow the white-box philosophy.
Causal theoretic models are extended with statistical theory to handle uncertain data and lead to conclusions in terms of effect sizes in simple models with few parameters.
The focus lies on explaining the available observations instead of predictions of new situations.

For many applications of machine learning in the physical world, complete theoretic models are not available.
Instead, the knowledge of domain experts is based on an intuitive understanding of the underlying leading to coarse expectations about system-behavior on different layers of abstraction.
We will discuss how abstract knowledge can be exploited to formulate structured Bayesian models that reproduce expectations but are still able to learn from data

\begin{itemize}
    \item How do we formulate models that conform to expert expectations about system-behavior but that can also learn from data? How do we make sure that the result is understandable for domain experts?
    \item How do we use expert expectations to constrain models to only produce physically plausible solutions in the areas of the input space that matter? How do we make inference-schemes work in the Bayesian context when things usually do not scale?
    \item How do we evaluate models in settings where we do not have either sufficient theory or sufficient generalization data? How do we even know where to evaluate?
\end{itemize}

The black-box and white-box modelling cultures follow different approaches to evaluating models.
While a white-box model is evaluated through a combination of goodness-of-fit measures on observations used during inference and subjective measures based on interpretability, black-box approaches typically focus on performance measures on test-sets.
Since structured models combine white-box and black-box properties, evaluation strategies should be a combination of both approaches as well.
We will show that good predictive performance is often not sufficient to identify a desirable structured model.
To formalize the subjectiveness in the description of desirable models, we will explore how downstream tasks can be taken into account for model selection.

\begin{itemize}
    \item Models that are used in this context need to be simple to yield provable results based on statistical theory. That is, we use linear models. Which are not adequate for our use-case. White-box models that explain our training data well might not be enough as we have not seen all relevant parts of the system. Predictions are important.
    \item The standard approach to data-driven models does not work either. We usually claim that predictions are good enough and we can validate through generalization behavior. But that does not work either: We do not get to observe the more critical parts of the system and can not check if the generalization-behavior makes sense there.
    \item In physical industrial applications, we want to combine the two: We are not after causal explanations. We are after plausible explanations that allow us to trust data-driven models. \Textcite{shmueli_explain_2010} separate explanatory power and predictive power of a model. We need both.
    \item How to get there is not clear: How do you enforce plausible solutions if you cannot test if they are plausible? Our answers: The correct (abstract) priors and a new evaluation scheme based on downstream tasks.
    \item Standard approaches to priors do not really scale here: We cannot easily explain regularization, specific choices for optimizers and hyper-parameters to domain experts. Our ML-choices need to be clean enough to be understandable for experts, the magic needs to be in the model description. Post-hoc explainability is a real issue with black-box approaches~\parencite{rudin_stop_2019}.
    \item However, complicated models usually make inference very hard. Bayesian non-parametrics struggles with this, which is why black-box approaches are so successful which make more general-purpose decisions. One aspect of our work is to formulate priors carefully to allow us to use scalable inference schemes.
\end{itemize}


\section{Thesis outline}
In this thesis, we will study how to formulate and evaluate Bayesian structural models using methods of Bayesian nonparametrics.
While traditional white-box Bayesian models typically avoid general function approximators, we use Gaussian processes to formulate the black-box components in our structural models.
The content is structured as follows.
In \cref{toc:gp}, we provide a short introduction to Bayesian machine learning and Bayesian nonparametrics with a focus on Gaussian processes (GPs).
We discuss the connection between statistical learning theory and Bayesian inference and their application to hierarchical models.
After introducing GPs, we discuss variational approaches to sparse GPs, their generalization to deep GP models, and efficient inference schemes.

In \cref{toc:data_association}, we explore how to relax white-box structure in a structured hierarchical model using the data-association problem as an example.
We use the formal mathematical structure to separate the data-association problem into separate hierarchical components and derive a joint inference scheme, resulting in a fully Bayesian model.
We show that this model is able to factorize multi-modal data into independent processes, providing explicit models for both the processes and assignment probabilities.
Comparisons with previous models show the additional qualitative model capabilities of the structured approach and competitive black-box performance.
However, experiments also show that standard measures are not enough to identify desirable models.

In \cref{toc:alignment}, we formulate a structured Bayesian model using black-box models as a starting point.
We discuss how to introduce additional constraints arising from expert knowledge about the data generation process to black-box deep GPs.
We interpret the problem of modeling the power production of multiple wind-turbines in a wind-farm as a nonlinear alignment problem.
Based on expert knowledge about the underlying latent and turbulent wind field, we can derive a structural model that enforces a physically plausible dependency structure between multiple time-series.
We discuss how these constraints help to solve an otherwise highly ambiguous learning problem and show that the imposed structure leads to a rich internal model-structure.

In \cref{toc:interpretable_rl}, we consider how to formalize the subjectiveness introduced via expert knowledge and model ambiguities.
We include the task a model will be used to solve into model selection, which allows us to distinguish between qualitatively different models showing similar performance metrics.
We revisit the data-association problem and embed it into a reinforcement learning problem, where identifying the correct underlying dynamics, and therefore a desirable model, is critical to finding a successful policy.
We show that semantic hierarchical structure increases data efficiency and allows domain experts to influence agent behavior through detailed insights into the dynamics of a system.

\Cref{toc:discussion} concludes the work presented in this thesis and further explores the properties and evaluation of structured hierarchical models.
In \cref{toc:discussion:composition}, we present an intuitive argument for why inference schemes based on factorizations between layers cannot represent heterogeneous posteriors.
In \cref{toc:discussion:bo}, we argue why models with suboptimal marginal likelihoods can perform well in hierarchical systems.
In \cref{toc:discussion:mountaincar}, we explore this idea further and consider how tasks can be included in the inference problem directly.
Finally, we discuss possible further directions of research.


\section{Contributions}
This work presents novel Bayesian structured models inspired by industrial applications.
In \cref{contrib:data_association}, we formulate a fully Bayesian interpretation of the data association problem.
\Cref{contrib:patent_depeweg,contrib:patent_geipel} discuss an application of this model to the prediction of the combustion dynamics of a gas-turbine.
We formulate a hierarchical Bayesian model for a nonlinear alignment problem that is capable of representing the complex interactions between turbines in a wind farm in \cref{contrib:windpower,contrib:patent_egedal,contrib:patent_kaiser}.

Using these methods, we investigate the behavior of Bayesian structured models and discuss evaluation strategies.
In \cref{contrib:interpretable_rl,contrib:multi_modal_rl}, we apply our data association model to a reinforcement learning task and we show how a semantic decomposition of the dynamics reduces the data requirements and produces interpretable solutions.
In \cref{contrib:surrogates}, we argue why models with suboptimal marginal likelihoods can perform well in hierarchical systems such as Bayesian optimization problems.
In \cref{contrib:composition}, we present an argument for why inference schemes based on factorizations between layers cannot represent heterogeneous posteriors.


\subsubsection{Papers}
\begin{enumerate}
    \item \label[contribution]{contrib:windpower}\fullfullcite{kaiser_bayesian_2018}
    \item \label[contribution]{contrib:data_association}\fullfullcite{kaiser_data_2019}
    \item \label[contribution]{contrib:interpretable_rl}\fullfullcite{kaiser_interpretable_2019}
    \item \label[contribution]{contrib:multi_modal_rl}\fullfullcite{kaiser_bayesian_2020}
    \item \label[contribution]{contrib:surrogates}\fullfullcite{bodin_modulating_2020}
    \item \label[contribution]{contrib:composition}\fullfullcite{ustyuzhaninov_compositional_2020}
\end{enumerate}

\subsubsection{Patents}
\begin{enumerate}
    \setcounter{enumi}{6}
    \item \label[contribution]{contrib:patent_egedal}\fullfullcite{egedal_verfahren_2019}
    \item \label[contribution]{contrib:patent_kaiser}\fullfullcite{kaiser_verfahren_2019}
    \item \label[contribution]{contrib:patent_geipel}\fullfullcite{geipel_transferlernen_2020}
    \item \label[contribution]{contrib:patent_depeweg}\fullfullcite{depeweg_computer_2020}
\end{enumerate}
