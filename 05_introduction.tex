\chapter{Introduction}
\label{toc:introduction}
In many machine learning problems, the task is to derive a model for observations of some process in nature that is not fully understood.
While traditional statistics favor the white-box modeling culture that focuses on explaining observational data using causal theoretic models, machine learning is more closely associated with the black-box culture~\parencite{breiman_statistical_2001,shmueli_explain_2010}.
When formulating a black-box model, we accept that we cannot model nature directly and instead focus on the functional dependency between inputs and outputs.
Using general function approximators like neural networks or Gaussian processes, any smooth dependency can be modeled given enough observations.
With growing computational power and the availability of data sets, black-box approaches have led to impressive advancements in domains that are hard to formalize.
Successes have been seen in a wide range of digital tasks such as speech recognition~\parencite{hochreiter_long_1997,chorowski_attention-based_2015,bahdanau_neural_2014,chorowski_attention-based_2015}, computer vision~\parencite{russakovsky_imagenet_2015,lecun_backpropagation_1989}, non-cooperative games~\parencite{berner_dota_2019,silver_mastering_2016}, or machine translation~\parencite{johnson_googles_2017}.
The abundance of data in these domains shifts the focus away from traditional statistical models to approaches that are highly adaptabe and scalable.

However, bridging the gap to applications in critical systems in the physical world has proved challenging.
Problem domains like robotics or industrial control introduce a new set of requirements.
In industrial control, machine learning is used to find more efficient or safer control strategies not obvious to the engineers designing a machine~\parencite{hein_benchmark_2017}.
In such scenarios, relevant data are often scarce.
The most valuable data, such as when a machine will fail or how it will behave in new situations, is never produced.
As applications become more safety-relevant, gathering data through exploration can be problematic due to the adverse consequences of failure.
If they are to operate in safety-critical areas, interact with people, or carry responsibility, machine learning systems must be robust, trustworthy, and assessable.
Besides optimizing for good average-case performance, a responsible system needs to reason about plausible worst-cases to be able to avoid them.
To ensure trust, ML-systems often need to be verified by domain experts before deployment, emphasizing a need for interpretability.
Models are required to incorporate and reproduce expert knowledge and make consistent predictions, while at the same time yielding new insights from data.


\section{Bayesian structured models with Gaussian processes}

\subsubsection{White-box models in Bayesian statistics}
\blindtext

You can get more general with Gaussian processes, but at some point they are just black-box.

\begin{titledesc}{Pros:}
    \item[Trustworthy predictions] asd
    \item[Strong interpretability] asd
    \item[Safe generalization] asd
\end{titledesc}

\begin{titledesc}{Cons:}
    \item[Strong model bias] asd
    \item[Subjective model-selection] asd
    \item[Weak scalability] asd
\end{titledesc}


\subsubsection{Black-box models in deep learning}
The highly adaptable and scalable models that lead to success in digital domains are not well-suited to be deployed in safety-critical environments.
Large neural networks generally suffer from low interpretability~\parencite{rudin_stop_2019}, the need for large amounts of data and their inability to quantify uncertainties~\parencite{goodfellow_deep_2016}.
In this work, we explore how to formulate machine learning models that can satisfy the requirements of physical environments.

You can get more principled with BNNs, but you can never make them white-box.

\begin{titledesc}{Pros:}
    \item[Universal approximation] asd
    \item[Strong scalability] asd
    \item[Potential for automation] asd
\end{titledesc}

\begin{titledesc}{Cons:}
    \item[Strong data bias] asd
    \item[Weak interpretability] asd
    \item[Complex parametrization] asd
    \item[High data requirements] asd
    \item[Unsafe generalization] asd
\end{titledesc}


\subsubsection{Bayesian structured models}

\begin{table}[t]
    \caption[Research questions]{
        \label{tab:introduction:questions}
        Research questions
    }
    \centering
    \newcolumntype{Y}{>{\centering\arraybackslash}X}
    \newcommand{\yes}{✔}
    \newcommand{\no}{\textcolor{sRedLight}{✘}}
    \newcommand\tabnode[2]{%
        \tikz[remember picture, baseline] \node[inner sep=0pt, anchor=base] (#1) {#2};
    }
    \begin{tabularx}{.9\textwidth}{lYY}
        \toprule
                                 & Bayesian statistics & Deep learning    \\
        \midrule
        Data-driven insights     & \tabnode{a}{\no}    & \yes             \\
        Strong scalability       & \tabnode{b}{\no}    & \yes             \\
        \addlinespace
        Interpretable results    & \yes                & \tabnode{c}{\no} \\
        Trustworthy predictions  & \yes                & \tabnode{d}{\no} \\
        \addlinespace
        Semantic model-selection & \tabnode{e}{\no}    & \tabnode{f}{\no} \\
        \bottomrule
    \end{tabularx}
    \begin{tikzpicture}[remember picture, overlay]
        \tikzstyle{question} = [
        draw=sGreenDark, fill=sGreenDark,
        fill opacity=.2, very thick,
        inner xsep=6pt, inner ysep=3pt,
        ]
        \node[
            question,
            fit=(a)(b),
            label={[sGreenDark, anchor=east]west:RQ1}
        ] {};
        \node[
            question,
            fit=(c)(d),
            label={[sGreenDark, anchor=west]east:RQ2}
        ] {};
        \node[
            question,
            fit=(e)(f),
            label={[sGreenDark, anchor=west]east:RQ3}
        ] {};
    \end{tikzpicture}
\end{table}

\begin{titledesc}{Research questions}
    \item[RQ1:] How can we relax the model-bias of white-box models and efficiently learn from data?
    \item[RQ2:] How can we reduce the data-bias of black-box models and reliably reproduce expert expectations?
    \item[RQ3:] How can we improve model selection in situations where internal model structure and generalization behavior come into focus?
\end{titledesc}


\section{Contributions}
This work presents novel Bayesian structured models inspired by industrial applications.
Our main contributions are as follows:
\begin{compactitem}
    \item We formulate fully Bayesian interpretations of the data association problem and nonlinear timeseries alignment.
    \item We formulate efficient and scalable inference schemes for these models by extending the variational inference schemes for deep Gaussian processes to handle multi-output GPs and discrete structure.
    \item We present a Bayesian view on model-based reinforcement learning and formulate an efficient probabilistic policy search algorithm for structured dynamics models.
    \item We discuss the limitations of current variational inference schemes for hierarchical Gaussian process models.
    \item We argue why surrogate models with suboptimal marginal likelihoods can perform well in Bayesian optimization problems.
\end{compactitem}

The main contributions presented in this thesis appeared in a number of peer-reviewed publications and patents we outline below.
In \cref{contrib:data_association,contrib:patent_depeweg,contrib:patent_geipel}, we formulate a Bayesian interpretation of the data association problem and discuss an application of this model to the prediction of the combustion dynamics of a gas-turbine, where the structured model allows us to distinguish between different dynamical regimes.
In~\cref{contrib:windpower,contrib:patent_egedal,contrib:patent_kaiser}, we formulate a hierarchical Bayesian model for a nonlinear time-series alignment problem, show that this model is capable of representing the complex interactions between turbines in a wind farm and discuss how this model can be used to derive more efficient controllers for wind-turbines in.
In \cref{contrib:interpretable_rl,contrib:multi_modal_rl}, we apply our data association model to a reinforcement learning task and show how a semantic decomposition of the dynamics reduces the data requirements and produces interpretable solutions.
In \cref{contrib:surrogates}, we formulate surrogate models for Bayesian optimization problems that focus on the informative structure of the objective functions by sacrificing local accuracy and in \cref{contrib:composition}, we present an argument for why inference schemes based on factorizations between layers cannot represent heterogeneous posteriors.


\subsubsection{Own publications}
\begin{enumerate}
    \item \label[contribution]{contrib:windpower}\fullfullcite{kaiser_bayesian_2018}
    \item \label[contribution]{contrib:data_association}\fullfullcite{kaiser_data_2019}
    \item \label[contribution]{contrib:interpretable_rl}\fullfullcite{kaiser_interpretable_2019}
    \item \label[contribution]{contrib:multi_modal_rl}\fullfullcite{kaiser_bayesian_2020}
    \item \label[contribution]{contrib:surrogates}\fullfullcite{bodin_modulating_2020}
    \item \label[contribution]{contrib:composition}\fullfullcite{ustyuzhaninov_compositional_2020}
\end{enumerate}

\subsubsection{Patents}
\begin{enumerate}
    \setcounter{enumi}{6}
    \item \label[contribution]{contrib:patent_egedal}\fullfullcite{egedal_verfahren_2019}
    \item \label[contribution]{contrib:patent_kaiser}\fullfullcite{kaiser_verfahren_2019}
    \item \label[contribution]{contrib:patent_geipel}\fullfullcite{geipel_transferlernen_2020}
    \item \label[contribution]{contrib:patent_depeweg}\fullfullcite{depeweg_computer_2020}
\end{enumerate}


\section{Thesis outline}
The content of this thesis is structured as follows.
In \cref{toc:gp}, we provide a short introduction to Bayesian machine learning and Bayesian nonparametrics with a focus on Gaussian processes (GPs).
We discuss the connection between statistical learning theory and Bayesian inference and their application to hierarchical models.
After introducing GPs, we discuss variational approaches to sparse GPs, their extension to hierarchical GP models, and efficient inference schemes.

In \cref{toc:data_association}, we explore how to formulate structured hierarchical models using the data association problem as an example.
We separate the data association problem into hierarchical components and derive an efficient joint inference scheme, resulting in a fully Bayesian model.
We show that this model is able to factorize multi-modal data into independent processes, providing explicit models for both the processes and assignment probabilities.
Comparisons with previous models show the additional qualitative model capabilities of the structured approach and competitive black-box performance.
However, experiments also show that standard measures are not enough to identify desirable models.

In \cref{toc:alignment}, we formulate a structured Bayesian model that reproduces expert knowledge about wind propagation in a wind-farm.
We interpret the problem of modeling the power production of multiple wind-turbines as a nonlinear alignment problem and derive an efficient inference scheme.
Based on expert knowledge about the underlying latent and turbulent wind field, we can derive a structured model that enforces a physically plausible dependency structure between multiple time-series.
We discuss how these constraints help to solve an otherwise highly ambiguous learning problem and show that the imposed structure leads to a rich internal model-structure that can be interpreted by experts.

In \cref{toc:interpretable_rl}, we consider how to formalize the subjectiveness introduced via expert knowledge and model ambiguities to evalute structured models.
We include the task a model will be used to solve into model selection, which allows us to distinguish between qualitatively different models showing similar performance metrics.
We revisit the data association problem and embed it into a reinforcement learning problem, where identifying the correct underlying dynamics, and therefore a desirable model, is critical to finding a successful policy.
We show that semantic hierarchical structure increases data efficiency and allows domain experts to influence agent behavior through detailed insights into the dynamics of a system.

\Cref{toc:discussion} concludes the work presented in this thesis and interprets the results in a broader context by further exploring the properties and evaluation of structured hierarchical models.
In \cref{toc:discussion:composition}, we present an intuitive argument for why inference schemes based on factorizations between layers cannot represent heterogeneous posteriors.
In \cref{toc:discussion:bo}, we argue why models with suboptimal marginal likelihoods can perform well in hierarchical systems.
In \cref{toc:discussion:mountaincar}, we explore this idea further and consider how tasks can be included in the inference problem directly.
Finally, we discuss possible further directions of research.
