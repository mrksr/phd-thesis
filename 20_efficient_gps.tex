\chapter{Preliminaries}
\label{toc:gp}
In \cref{toc:introduction}, we described the black-box and white-box modeling cultures and introduced the idea of structured models.
Structured models share rich internal structure with white-box models while accepting that some aspects of a learning problem cannot be modeled in full, thereby introducing black-box model components.
In this thesis, we will use ideas from Bayesian nonparametrics to formulate structured models.
In this chapter, we introduce the theoretical foundations of Bayesian machine learning and then introduce Gaussian process models, a tool for Bayesian nonparametric function approximation.

We first provide a short introduction to statistical learning theory and Bayesian machine learning.
We then introduce Gaussian processes formally, describe how to formulate and select a GP prior, and how to derive a GP posterior.
Since GPs defined on many observations are computationally expensive, sparse approximations are used in practice.
We discuss one class of sparse approximations based on inducing observations and introduce approaches based on variational inference.
Next, two extensions to variational inference in hierarchical GP models are introduced that will be used to formulate more complex hierarchical models containing multiple Gaussian processes in the next chapters.

\section{Machine learning problems}
\label{toc:bayesian_ml}
One of the roots of machine learning (ML) lies in the study of algorithms in theoretical computer science.
An algorithm is a well-defined sequence of computational steps transforming a set of inputs to a set of outputs.
It is a tool for solving a computational problem, which is defined by an abstract problem of admissible inputs and expected outputs.
An algorithm solves such a problem if, for every possible input, the algorithm provably yields the correct output.

Consider the computational problem of sorting a list of numbers in ascending order.
One possible formulation~\parencite{cormen_introduction_2009} of the sorting problem is:
\begin{problem}[Sorting]
\label{prob:bayesian_ml:sorting}
\begin{labeling}{Output:}
    \item[Input:] A sequence of $N$ integers $\mat{I} = (i_1, \dots, i_N)$
    \item[Output:] A reordering of $\mat{I}$ called $\mat{O} = (o_1, \dots, o_N)$ such that $o_1 \leq \dots \leq o_N$.
\end{labeling}
\end{problem}
For example, for the input $\mat{\hat{I}}=(12, 8, 23, 4)$ the correct output is $\mat{\hat{O}}=(4, 8, 12, 23)$.
A concrete input $\mat{\hat{I}}$ is called an instance of a problem.
Importantly, such an instance contains all the required information to compute the unique output $\mat{\hat{O}}$.
The correctness of the output can be checked via the formal problem.
The various available (correct) sorting algorithms only differ in which and how many computational steps they take to arrive at the output, not in the output itself.

Machine learning can be seen as an extension of algorithmics towards problems where a formal description of a uniquely defined solution does not exist.
Instead, problems in ML are characterized by the observation of finitely many examples or data together with the objective to derive knowledge about how these examples were generated.
This knowledge can then be used to describe common patterns in the data or generate new examples that conform to previous observations.

\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_data}
        \caption{
            Dataset
            \label{fig:bayesian_ml:polynomials:data}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_lagrange}
        \caption{
            Algorithms for interpolation
            \label{fig:bayesian_ml:polynomials:lagrange}
        }
    \end{subfigure}
    \caption[Algorithms for interpolation]{
        \label{fig:bayesian_ml:polynoms}
        Computational problems defined on a dataset have a well-defined and unique solution.
        This solution can be characterized through algorithms such as linear interpolation or Lagrange polynomials.
    }
\end{figure}

As an example, consider the data shown in \cref{fig:bayesian_ml:polynomials:data}, a set of $N$ pairs of real numbers for which we assume that all $x_i$ are pairwise different.
The knowledge about this data to be uncovered is the functional dependency between the two data dimensions that were used to generate the data.
This is called a regression problem and is strongly under-specified:
There exist uncountably many functions on the real numbers that explain any finite set of observed points.
The problem
\begin{problem}[Regression]
\label{prob:bayesian_ml:regression}
\begin{labeling}{Output:}
    \item[Input:] A set of $N$ pairs of real numbers $\Dc = \Set*{\left(x_n, y_n\right)}_{n=1}^N \subseteq \Xc \times \Yc$
    \item[Output:] The functional dependency $f : \Xc \to \Yc$ used to generate the data.
\end{labeling}
\end{problem}
is therefore not a computational problem and asking for a solution or algorithm for this problem is not a well-posed question.

One can, however, derive computational problems from the regression problem by making assumptions about the nature of the function $f$ that characterize a unique solution.
For example, one could ask for the simplest polynomial that explains the data.
\begin{problem}[Lagrange Polynomial]
\label{prob:bayesian_ml:lagrange}
\begin{labeling}{Output:}
    \item[Input:] A set of $N$ pairs of real numbers $\Dc = \Set*{\left(x_n, y_n\right)}_{n=1}^N \subseteq \Xc \times \Yc$
    \item[Output:] The polynomial of smallest degree for which $f(x_n) = y_n$ holds for all $(x_n, y_n) \in \Dc$.
\end{labeling}
\end{problem}
It can be shown that this is indeed a well-posed computational problem whose unique solution is the Lagrange polynomial $L$~\parencite{waring_vii._1779} given by the explicit form
\begin{equation}
    \begin{split}
        L(x) &= \sum_{i=0}^N y_i \ell_i(x)\text{, with} \\
        \ell_i &= \prod_{\substack{j = 0\\j \neq i}}^N \frac{x - x_j}{x_i - x_j}.
    \end{split}
    \label{eq:bayesian_ml:lagrange}
\end{equation}
\Cref{fig:bayesian_ml:polynomials:lagrange} shows the Lagrange polynomial interpolating the example dataset.
The figure also shows another derived computational problem of linear interpolation.
Here, the function is defined as being piecewise linear between the different data points, thus also reproducing the data.

Both of these problems and derived algorithms would typically not be identified as ML approaches, as it can be argued that they do not derive new insights from data.
Because of the explicit nature of their algorithms as seen in \cref{eq:bayesian_ml:lagrange}, they much more closely resemble classical algorithms such as sorting algorithms.

\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_linear}
        \caption{
            Linear regression
            \label{fig:bayesian_ml:polynomials:linear}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_polynomial}
        \caption{
            Polynomial regression
            \label{fig:bayesian_ml:polynomials:polynomial}
        }
    \end{subfigure}
    \caption[Polynomial regression]{
        \label{fig:bayesian_ml:polynomials:ml}
        Problems in machine learning do not have a well-defined solution but require additional assumptions about the hypothesis space and error function.
        For the linear regression problem (left), the squared error (LS), absolute error (LAE) and Ridge error functions lead to different solutions.
        Similarly, alternating the hypothesis space between polynomials of different degrees (right) yields qualitatively different results.
    }
\end{figure}
A common approach to formulating a problem that lets the data speak for itself is to formulate more implicit requirements.
Instead of describing exactly one possible function (such as the Lagrange Polynomial), one can choose a broader set of candidate functions or hypotheses $\Hc$ and then select one of the candidates $f \in \Hc$ that is optimal with respect to some measure of performance.
For example, a common assumption about the process used to generate the data is that it separates into two additive components
\begin{align}
    \label{eq:bayesian_ml:additive_noise}
    y_n = f(x_n) + \epsilon(x_n).
\end{align}
The first summand $f$ captures the truly informative functional dependency between $x$ and $y$ that applies for all observations while the second term $\epsilon$ captures local error or noise that can be ignored.

A direct consequence of this assumption is that the output of the regression problem $f$ need no longer interpolate the data perfectly.
At the same time, additional reasoning is required about how far from the data $f$ is allowed to be or, equivalently, about the shape of $\epsilon$.
One can choose $f$ to be a linear function, giving rise to the linear regression problem.
\begin{problem}[Linear regression]
\begin{labeling}{Output:}
    \item[Input:] A set of $N$ pairs of real numbers $\Dc = \Set*{\left(x_n, y_n\right)}_{n=1}^N \subseteq \Xc \times \Yc$ and an error function $e : \Hc \times \Xc \times \Yc \to \Rb$
    \item[Output:] A function $f$ such that
    \begin{align}
        f \in \argmin_{f \in \Hc} e(f, \Dc)
    \end{align}
    with $\Hc$ being the set of linear functions.
\end{labeling}
\end{problem}
\Cref{fig:bayesian_ml:polynomials:linear} shows the different results of three algorithms with different choices of error functions $e$.
With $f(x) = \mat{W}x + b$ those are
\begin{labeling}{Absolute error:}
    \item[Squared error:] $e(f, \Dc) = \sum_{n=1}^N (y_n - f(x_n))^2$
    \item[Absolute error:] $e(f, \Dc) = \sum_{n=1}^N \abs{y_n - f(x_n)}$
    \item[Ridge:] $e(f, \Dc) = \sum_{n=1}^N (y_n - f(x_n))^2 - \norm{\mat{W}}_2 - b^2$.
\end{labeling}
Altering the set of hypotheses $\Hc$ between linear, quadratic and cubic polynomials together with the squared error function results in the functions shown in \cref{fig:bayesian_ml:polynomials:polynomial}.

It is important to note that none of these proposed algorithms and plotted functions is objectively the correct solution to the regression problem.
None of them is equal to the function that was used to generate the data, and even if it was, there would be no way to tell.
A core property of machine learning problems is that what characterizes the correct solution is an inherently subjective question.
This subjectiveness is represented in multiple choices:
\begin{enumerate}
    \item The assumed structure underlying the data.
    \item The space of hypotheses for valid solutions.
    \item The algorithm used to select from these hypotheses.
\end{enumerate}
In the following, statistical learning and Bayesian machine learning are introduced as tools to formalize these choices and establish a mathematical framework.

\section{Statistical learning}
\label{toc:bayesian_ml:statistical_learning}
\begin{figure}[t]
    \centering
    \includestandalonewithpath{figures/quantities_of_interest_statistical_learning}
    \caption[Statistical learning]{
        \label{fig:bayesian_ml:statistical_learning}
        Statistical learning characterizes machine learning through a latent data distribution $\Prob*{\mat{z}}$.
        The task is to learn about a functional of interest $F[\Prob*{\mat{z}}] = f \in \Fc$ based on a set of observations $S[\Prob*{\mat{z}}] \in \Sc$.
        A machine learning algorithm $A : \Sc \to \Fc$ is successful if it recovers $f$ such that $A \circ S \simeq F$.
    }
\end{figure}
The objective of machine learning is to derive knowledge about a process generating the data from a limited set of observations.
Knowledge is represented as a model that both explains existing observations and can generalize to new data points.
Statistical learning theory~\parencite{gareth_james_introduction_2013,trevor_hastie_elements_2013} offers a set of tools to reason about generalization and to formulate what it means for a model to be good.
This section presents an interpretation of statistical learning theory similar to the definition of probabilistic numerics in~\parencite{oates_modern_2019,cockayne_bayesian_2019}.

In the following, we denote the space of probability measures over a set $\Zc$ as $\Probs{\Zc}$.
We adopt an overloaded notation common in machine learning where $\Prob*{\mat{z}}$ can both refer to a probability measure and the evaluation of the same probability measure on a specific point $\mat{z} \in \Zc$ depending on the context.
Similarly, $\mat{z}$ can both refer to a random variable with the distribution $\Prob*{\mat{z}}$ and an element $\mat{z} \in \Zc$.
That is, $\mat{k} \sim \Prob*{\mat{z}}$ denotes that the distribution of the random variable $\mat{k}$ is $\Prob*{\mat{z}}$ and $\Moment*{\E}{\mat{z}} = \int \mat{z} \Prob*{\mat{z}} \diff \mat{z}$ denotes the expected value of the random variable $\mat{z}$ under the distribution $\Prob*{\mat{z}}$.
In less ambiguous notation one would assume a random variable $\mat{Z}$ with distribution $\Prob*{\mat{Z}}$ and denote the expected value as $\Moment*{\E}{\mat{Z}} = \int_{\Zc} z \Prob*{\mat{Z} = z} \diff z$.
Here, $\mat{Z}$ and $z$ are identified with each other as $\mat{z}$.

Assume an unknown data-generating distribution $\Prob{\mat{z}} \in \Probs{\Zc}$ over a known space of observations $\Zc$.
The task in a machine learning problem is to infer properties of this distribution given a set of observations $\Dc \in \Sc$ obtained via some sampling operator $S : \Probs{\Zc} \to \Sc$.
A common choice of $S$ is a draw of $N$ independent points $\Dc = \Set*{\mat{z}_1, \dots, \mat{z}_N \with \mat{z}_i \sim \Prob{\mat{z}}} \subseteq \Zc$ with $\Sc = \Zc^N$.
Other choices include $S$ into the learning problem such as in active learning~\parencite{murphy_machine_2012} or reinforcement learning~\parencite{sutton_reinforcement_2018} settings.
Depending on the machine learning problem, the task might not be to identify $\Prob*{\mat{z}}$ as a whole but instead identify some functional $f \in \Fc$ obtained via the functional operator $F : \Probs{\Zc} \to \Fc$.
This formulation encompasses a large number of different machine learning problems via different choices for the spaces and operators.
We give a few examples here.

\begin{problem}[Common machine learning problems]
\begin{description}
    \item[Parameter estimation]
          In a simple case, the functional $f$ can be chosen to be some constant statistic of $\Prob*{\mat{z}}$ such as the mean
          \begin{align}
              F_{\text{mean}}[\Prob*{\mat{z}}] = \int \mat{z} \Prob*{\mat{z}} \diff \mat{z},
          \end{align}
          with $\Fc = \Zc$.
    \item[Regression]
          Alternatively, assuming that $\Zc = \Xc \times \Yc$ separates into tuples of inputs $\Xc$ and continuous outputs $\Yc$ and choosing
          \begin{align}
              \label{eq:bayesian_ml:f_reg}
              F_{\text{reg}}[\Prob*{\mat{x}, \mat{y}}](\mat{x}_\ast) = \Prob*{\mat{y}_\ast \given \mat{x}_\ast}
          \end{align}
          with $\Fc = \Xc \to \Probs{\Yc}$ gives rise to the regression problem discussed in~\cref{toc:bayesian_ml}.
    \item[Classification]
          Classification is closely related to regression and assumes the same separation $\Zc = \Xc \times \Yc$.
          In contrast to regression, $\Yc$ is assumed to be discrete, often without a known order.
    \item[Dimensionality Reduction]
          In the dimensionality reduction or manifold learning problem, the assumption is that the support of the data distribution $\Prob*{\mat{z}}$ is a low dimensional manifold in $\Zc$.
          The task is to find a distribution $\Prob*{\mat{m}} \in \Probs{\Mc}$ and a mapping $f : \Mc \to \Zc$ with
          \begin{equation}
              \begin{split}
                  F_{\text{dim}}[\Prob*{\mat{z}}] & = (\Prob*{\mat{m}}, f)\text{, such that} \\
                  f(\Prob*{\mat{m}})              & = \Prob*{\mat{z}}
              \end{split}
          \end{equation}
          and $\Fc = \Probs{\Mc} \times (\Mc \to \Zc)$ and a space $\Mc$ of lower dimensionality than $\Zc$.
          If $\Mc = \Zc$ this problem is called density estimation.
\end{description}
\end{problem}

A machine learning Algorithm $A : \Sc \to \Fc$ is a function mapping from the space of samples to the space of functionals, thereby recovering structure from data.
Such an algorithm is successful if it recovers the correct functional $f \in \Fc$ given observations $\Dc$.
\Cref{fig:bayesian_ml:statistical_learning} shows the introduced components in a directed diagram.
An algorithm is successful if this diagram (approximately) commutes such that $A \circ S \simeq F$.
Similarly, a model $f_A \in \Fc$ can be considered good if it is similar to the true functional~$f$.

The similarity of two functionals can be measured via a distance measure in $\Fc$.
In the case of learning about global properties such as the mean of $\Prob*{\mat{z}}$, a possible choice is the standard Euclidean norm.
Considering the regression case, where $\Fc$ is a function space mapping inputs to (possibly distributions over) outputs, the effects of choosing a distance measure is more subtle.
While it is reasonable to require pointwise similarity, the choice of which points to evaluate allows us to define what we mean by generalization:
Besides explaining the observations in $\Dc$, a good model should yield correct predictions for the complete data distribution $\Prob*{\mat{z}} = \Prob*{\mat{x}, \mat{y}}$.
The concept of risk minimization is based on this observation.
\begin{definition}[Risk minimization in the regression problem]
    \label{def:risk_minimization}
    Given a loss function $\ell: \Yc \times \Yc \to \Rb$ and a data distribution $\Prob*{\mat{x}, \mat{y}}$ for a regression problem, the \emph{risk} relative to $\ell$ is defined as
    \begin{align}
        \risk_\ell & : \left\{
        \begin{aligned}
            \Fc & \to \Rb                                                                                              \\
            f_A & \mapsto \int \Fun*{\ell}{\mat{y}, f_A(\mat{x})} \Prob{\mat{x}, \mat{y}} \diff \mat{x} \diff \mat{y}.
        \end{aligned}
        \right.
    \end{align}
    \emph{Risk minimization} for a hypothesis space $\Hc \subseteq \Fc$ selects a hypothesis $\hat{f}_A$ with the smallest possible risk
    \begin{align}
        \hat{f}_A & \in \argmin_{f_A \in \Hc} \risk_\ell(f_A).
    \end{align}
\end{definition}

A model generalizes with respect to a loss function $\ell$ if it does not only explain the specific dataset $\Dc$ well but all possible choices of $\Dc$ via $S$.
While risk minimization is a theoretical tool to define generalization, it does not immediately yield a learning algorithm.
Since it includes an expectation over the unknown data distribution, the risk-term cannot be evaluated directly.
Instead, calculating a Monte-Carlo estimate over the training data $\Dc$ gives rise to a fundamental machine learning algorithm, empirical risk minimization.

\begin{definition}[Empirical risk minimization in the regression problem]
    \label{def:empirical_risk_minimization}
    Given a loss function $\ell: \Yc \times \Yc \to \Rb$ and a data distribution $\Prob*{\mat{x}, \mat{y}}$ for a regression problem, the \emph{empirical risk} relative to $\ell$ is defined as
    \begin{align}
        \risk^{\text{emp}}_\ell & : \left\{
        \begin{aligned}
            \Fc & \to \Rb                                                             \\
            f_A & \mapsto \frac{1}{N} \sum_{i=1}^N \Fun*{\ell}{\mat{y}, f_A(\mat{z})}
        \end{aligned}
        \right.
    \end{align}
    \emph{Empirical risk minimization} for a hypothesis space $\Hc \subseteq \Fc$ selects a hypothesis $\hat{f}_A$ with smallest possible risk
    \begin{align}
        \hat{f}_A & \in \argmin_{f_A \in \Hc} \risk^{\text{emp}}_\ell(f_A).
    \end{align}
\end{definition}

Intuitively, risk minimization describes the global properties of $f_A$ which get approximated by a number of local properties at the observations in empirical risk minimization.
The two questions
\begin{enumerate}
    \item under which conditions $\risk^{\text{emp}}$ converges to $\risk$ for $N \to \infty$ and
    \item if so, what the convergence rates are
\end{enumerate}
underpin (statistical) learning theory~\parencite{vapnik_principles_1992}.
It is safe to assume that for small $N$, $\risk^{\text{emp}}$ can significantly underestimate the true risk as the error on parts of the data distribution is not considered at all.
This problem is called overfitting to the available training data, an example of which can arguably be seen in~\cref{fig:bayesian_ml:polynomials:lagrange}.

In practice, if the collection of sufficient data is not possible, overfitting has to be avoided via problem-dependent choices for $\ell$ and $\Hc$.
Additionally, the empirical risk minimization algorithm is often extended to regularizing loss functions of the form $\ell^\prime : \Fc \times \Yc \times \Yc \to \Rb$ which depend on the structure of the candidate as well as its predictions.
The Ridge error function shown in~\cref{fig:bayesian_ml:polynomials:linear} is an example of extending the least-squares error function with a preference for parameters with small absolute value.
Regularization~\parencite{oates_modern_2019} adds back a global component to empirical risk minimization and has a close relation to the choice of hypothesis space $\Hc$.
The constraint that $f_A \in \Hc \subseteq \Fc$ can be thought of as a binary regularization term that adds infinite loss to the set $\Fc \setminus \Hc$.
Conversely, continuous regularization terms formulate softer and less rigorous preferences within $\Hc$, for example for structurally simpler solutions~\parencite{bishop_pattern_2007,thorburn_occams_1915}.

For complex loss functions and hypothesis spaces, finding the true minimum $\hat{f}_A$ is often unfeasible.
Another common extension is to modify the optimization scheme to increase the likelihood of selecting a favorable solution.
Examples include the usage of test sets or validations sets~\parencite{bishop_pattern_2007}, cross validation~\parencite{stone_cross-validatory_1974}, early stopping~\parencite{morgan_generalization_1990} or specific parameter choices~\parencite{daniely_toward_2016}.

It is often possible to reformulate choices in loss functions as changes in the optimization scheme or constrain a more general hypothesis space with stricter regularization.
It is not clear how a machine learning algorithm should be formulated from a formal perspective.
However, problem-dependent adjustments to learning algorithms are generally informed by the knowledge provided by domain-experts about the process that generated the data.
An additional dimension is therefore given by the need to communicate assumptions and effects of choices with stakeholders that do not have a deep understanding of the field.
If a learning algorithm should be interpretable and understandable, assumptions should be explicit, and optimization schemes should be simple.
The next section introduces Bayesian machine learning as a rigorous formal framework that builds on statistical learning theory and enables the principled formulation of complex hypothesis spaces.

\section{Bayesian machine learning}
\label{toc:bayesian_ml:bayesian_ml}
The central modeling assumption of statistical learning theory is the unknown data distribution $\Prob*{\mat{z}}$.
Based on a limited number of observations, the task is to learn a model of either the complete distribution or some functional $F[\Prob*{\mat{z}}]$.
Instead of selecting one solution (for example, using some regularization scheme), Bayesian approaches accept the inevitability of uncertainty due to the underspecified ML problem and represent it explicitly.
Following the definition of functionals $F$, a Bayesian model is often formulated through independence assumptions in the data distribution.
Independence assumptions induce a factorization of the data distribution.
Structural constraints can then be put on the different factors to encode expert knowledge.

Since the available data is not sufficient to identify the unique correct solution, the goal of Bayesian machine learning is to infer a distribution of plausible solutions weighed by their likelihood given the data instead.
The posterior distribution of plausible models is a combination of the prior assumptions about the underlying structure and their capability of explaining the observations.
This section introduces probabilistic generative models as a natural consequence of the assumptions in statistical learning theory.
Probabilistic generative models form a principled way to formulate interpretable hypothesis spaces with carefully chosen assumptions.
With Bayes' rule, a structurally simple and formally consistent learning algorithm can be formulated to infer knowledge from data.

\subsubsection{Directed graphical models}
Any joint probability distribution $\Prob*{\mat{a}, \mat{c}, \mat{e}}$ can be formulated as a product of partial distributions via the chain rule of probability~\parencite{murphy_machine_2012} as
\begin{equation}
    \begin{split}
        \Prob*{\mat{a}, \mat{c}, \mat{e}}
        &= \Prob*{\mat{a} \given \mat{c}, \mat{e}}\Prob*{\mat{c} \given \mat{e}}\Prob*{\mat{e}} \\
        &= \Prob*{\mat{e} \given \mat{a}, \mat{c}}\Prob*{\mat{a} \given \mat{c}}\Prob*{\mat{c}} \\
        &= \dots
    \end{split}
\end{equation}
The chain rule is symmetric in the sense that the order of chaining random variables does not matter.
All possible interdependencies between random variables are represented in the expansion of conditional probabilities.

A factorization along the chain rule does not impose any structure on the underlying distribution.
The first step in Bayesian modeling is to impose such structure by assuming conditional independence between variables.
Directed graphical models are a readable visualization of such independence assumptions.
A directed graphical model is a directed graph where every random variable is a node
\tikz[x=2.5em,baseline=(a.base)]{
    \node[inline random variable] (a) {$\mat{a}$};
}
and an edge
\tikz[x=2.5em,baseline=(a.base)]{
    \node[inline random variable] (a) {$\mat{a}$};
    \node[inline random variable] (b) at (1, 0) {$\mat{c}$};
    \draw[edge, directed] (a) -- (b);
}
denotes a dependency of $\mat{c}$ on $\mat{a}$.
In contrast to the factorization according to the chain rule, the graphical model below encodes the assumption that $\mat{e}$ is independent of $\mat{a}$ given $\mat{c}$.
\begin{align}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, -1);
        \node[random variable] at (0, 0) (X) {$\mat{a}$};
        \node[random variable] at (0, -1) (Y) {$\mat{c}$};
        \node[random variable] at (0, -2) (Z) {$\mat{e}$};
        \draw[edge, directed] (X) -- (Y);
        \draw[edge, directed] (Y) -- (Z);
    \end{tikzpicture}
     &   &
    \Prob*{\mat{a}, \mat{c}, \mat{e}}
     & =
    \left\{
    \begin{aligned}
         & \Prob*{\mat{a}}                      \\
         & \cdot \Prob*{\mat{c} \given \mat{a}} \\
         & \cdot \Prob*{\mat{e} \given \mat{c}}
    \end{aligned}
    \right.
\end{align}

More formally, the factorization belonging to a graphical model with nodes $\mat{a}_1, \dots, \mat{a}_N$ is given by
\begin{equation}
    \begin{split}
        \Prob*{\mat{a}_1, \dots, \mat{a}_N}
        &= \prod_{n=1}^N \Prob*{\mat{a}_n \given \mathrm{parents}(\mat{a}_n)}
    \end{split}
\end{equation}
with $\mathrm{parents}(\mat{a}_n)$ denoting the set of nodes with an edge towards $\mat{a}_n$.
A circle
\tikz[x=2.5em,baseline=(a.base)]{
    \node[inline random variable] (a) {$\mat{a}$};
    \node[inline random variable] (b) at (1, 0) {$\mat{c}$};
    \draw[edge, directed, bend left=15] (a) to (b);
    \draw[edge, directed, bend left=15] (b) to (a);
}
denotes that the variables $\mat{a}$ and $\mat{c}$ need to be considered jointly as $\Prob*{\mat{a}, \mat{c}}$.
Graphical models have been studied in great detail.
We refer to~\parencite{murphy_machine_2012,bishop_pattern_2007,david_barber_bayesian_2012,trevor_hastie_elements_2013} for additional information.

\subsubsection{Generative models}
In the algorithmic view of machine learning problems formulated in~\cref{toc:bayesian_ml}, the observational data is used to formulate an error function to select a model from a set of candidates without the notion of explaining said data.
To formulate what it means to generalize, statistical learning theory in~\cref{toc:bayesian_ml:statistical_learning} bases the learning problem on an unknown or latent data distribution, with the observational data being just one of many possible samples from that distribution.
A generative model captures this sampling process and formulates an algorithm to generate arbitrary data sets from the data distribution $\Prob*{\mat{z}}$.
A generative model is successful if the observational data set is a likely draw from the model.
Using the regression problem as an example, we will now introduce the building blocks of a Bayesian generative model.

The functional of interest $F_{\text{reg}} = \Prob*{\mat{y} \given \mat{x}}$ for the regression problem formulated in~\cref{eq:bayesian_ml:f_reg} is the conditional probability of the output $\mat{y}$ given the input $\mat{x}$ for the data distribution $\Prob*{\mat{z}} = \Prob*{\mat{x}, \mat{y}}$.
Formulating a generative model starts with the factorization of the data distribution such that this conditional is made explicit:
\begin{align}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, -0.5);
        \node[random variable, observed] at (0, 0) (X) {$\mat{x}$};
        \node[random variable, observed] at (0, -1) (Y) {$\mat{y}$};
        \draw[edge, directed] (X) -- (Y);
    \end{tikzpicture}
     &   &
    \Prob*{\mat{x}, \mat{y}}
     & =
    \left\{
    \begin{aligned}
         & \Prob*{\mat{x}}                      \\
         & \cdot \Prob*{\mat{y} \given \mat{x}}
    \end{aligned}
    \right.
\end{align}
Here, the color of the nodes
\tikz[x=2.5em,baseline=(x.base)]{
    \node[inline random variable, observed] (x) {$\mat{x}$};
}
indicates that both nodes are part of the data distribution and are directly observed in a data set.
We will introduce other nodes below.
Closed forms for both $\Prob*{\mat{x}}$ and $\Prob*{\mat{y} \given \mat{x}}$ would fully characterize the data distribution.
To solve the regression problem, however, only the second term is required.

In the notation typically employed in machine learning, the variables $\mat{x}$ and $\mat{y}$ in the graphical model denote both the factorization of the generative process as well as the functional dependencies between concrete realizations of $\mat{x}$ and $\mat{y}$.
Assuming a regression problem over the reals $\Xc = \Yc = \Rb$, the functional $\Prob*{\mat{y} \given \mat{x}}$ is an infinite-dimensional object.
To simplify reasoning about this object, we will reason about finitely many realizations of this process, the pairs $(\mat{x}_1, \mat{y}_1), \ldots, (\mat{x}_N, \mat{y}_N)$.
This formulation describes both the model search and prediction steps:
If the $N$ pairs are all part of some data set $\Dc$, model-search can be formulated as finding a closed-form for $\Prob*{\mat{y}_n \given \mat{x}_n}$ that explains the observations well.
For prediction, the graphical model is typically augmented with a new pair $(\mat{x}_\ast, \mat{y}_\ast) \not\in \Dc$, an arbitrary but previously unseen point.
Making a prediction is equivalent to evaluating the closed form $\Prob*{\mat{y}_\ast \given \mat{x}_\ast}$.
To simplify notation, we identify $\mat{x} = (\mat{x}_1, \dots, \mat{x}_N)$ and $\mat{y} = (\mat{y}_1, \dots, \mat{y}_N)$.
Using this formulation, the graphical model is reformulated as
\begin{align}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, -0.5);
        \node[random variable, observed] at (0, 0) (X1) {$\mat{x}_1$};
        \node[random variable, observed] at (0, -1) (Y1) {$\mat{y}_1$};
        \node[random variable, observed] at (1, 0) (X2) {$\mat{x}_2$};
        \node[random variable, observed] at (1, -1) (Y2) {$\mat{y}_2$};
        \node at (1.5, -0.5) (dots) {$\cdots$};
        \draw[edge, directed] (X1) -- (Y1);
        \draw[edge, directed] (X1) -- (Y2);
        \draw[edge, directed] (X2) -- (Y2);
        \draw[edge, directed] (X2) -- (Y1);
        \draw[edge, directed, bend left=20] (Y1) to (Y2);
        \draw[edge, directed, bend left=20] (Y2) to (Y1);
    \end{tikzpicture}
     &   &
    \Prob*{\mat{x}, \mat{y}}
     & =
    \left\{
    \begin{aligned}
         & \Prob*{\mat{x}_1, \mat{x}_2}                                    \\
         & \cdot \Prob*{\mat{y}_1, \mat{y}_2 \given \mat{x}_1, \mat{x}_2}.
    \end{aligned}
    \right.
\end{align}
Similar to the assumption of independent draws for a data set $\Dc$ in statistical learning theory, the inputs $\mat{x}_1, \mat{x}_2, \dots$ to a generative regression model are typically assumed to be independent.
The outputs $\mat{y}_1, \mat{y}_2, \dots$ are not independent however because they have been generated using the same functional dependency $f$ we wish to model.

In the next step, we expand the model to reflect the fact that the $\mat{y}_N$ are conditionally independent given this functional dependency $f$.
The rule of total probability~\parencite{bishop_pattern_2007} states that for any random variable $\mat{f}$, it holds that
\begin{equation}
    \begin{split}
        \Prob*{\mat{y} \given \mat{x}} = \int \Prob*{\mat{y} \given \mat{f}, \mat{x}} \Prob*{\mat{f}} \diff \mat{f}.
    \end{split}
\end{equation}
This rule allows us to add arbitrary nodes to the graphical model and recover the original distribution via marginalization, the calculation of the expectation with respect to the variable that should be removed.
Introducing a new variable is helpful because it allows us to formulate independence assumptions explicitly.
To achieve conditional independence of the $\mat{y}_n$, we add variables $\mat{f}_n$ which represent the function values $\mat{f}_n = f(\mat{x}_n)$.
Since the input $\mat{x}_n$ is no longer relevant once this function value is known, we assume that $\Prob*{\mat{y}_n \given \mat{f}_n, \mat{x}_n} = \Prob*{\mat{y}_n \given \mat{f}_n}$.
This leads to the new graphical model
\begin{align}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, -1);
        \node[random variable, observed] at (0, 0) (X1) {$\mat{x}_1$};
        \node[random variable, latent] at (0, -1) (F1) {$\mat{f}_1$};
        \node[random variable, observed] at (0, -2) (Y1) {$\mat{y}_1$};
        \node[random variable, observed] at (1, 0) (X2) {$\mat{x}_2$};
        \node[random variable, latent] at (1, -1) (F2) {$\mat{f}_2$};
        \node[random variable, observed] at (1, -2) (Y2) {$\mat{y}_2$};
        \node at (1.5, -1) (dots) {$\cdots$};
        \draw[edge, directed] (X1) -- (F1);
        \draw[edge, directed] (F1) -- (Y1);
        \draw[edge, directed, bend left=20] (F1) to (F2);
        \draw[edge, directed] (X2) -- (F2);
        \draw[edge, directed] (F2) -- (Y2);
        \draw[edge, directed, bend left=20] (F2) to (F1);
    \end{tikzpicture}
     &   &
    \Prob*{\mat{x}, \mat{y}}
     & =
    \left\{
    \begin{aligned}
         & \Prob*{\mat{x}_1, \mat{x}_2}                                                      \\
         & \cdot \int \Prob*{\mat{f}_1, \mat{f}_2 \given \mat{x}_1, \mat{x}_2}               \\
         & \quad \cdot \Prob*{\mat{y}_1 \given \mat{f}_1} \Prob*{\mat{y}_2 \given \mat{f}_2} \\
         & \quad \cdot \diff \mat{f}_1 \diff \mat{f}_2
    \end{aligned}
    \right.
\end{align}
with the additional latent nodes
\tikz[x=2.5em,baseline=(f.base)]{
    \node[inline random variable, latent] (f) {$\mat{f}$};
}
which are never directly observed.
The function values are (usually) assumed to not be directly observed due to the additive noise assumption in~\cref{eq:bayesian_ml:additive_noise}, which states that only a noisy version $\mat{y}_n = f(\mat{x}_n) + \epsilon_n$ of the true functional dependency is observed to motivate regularization terms.
In the graphical model above, both terms are represented via
\begin{labeling}{$\Prob*{\mat{f}_n \given \mat{x}_n}$}
    \item [$\Prob*{\mat{f}_n \given \mat{x}_n}$:] The true functional dependency $f$, the functional of interest.
    \item [$\Prob*{\mat{y}_n \given \mat{f}_n}$:] The independent noise term $\epsilon$.
\end{labeling}
In this model, the noise term contains all part of the generative process for $\mat{y}_n$ which cannot be explained by $\mat{x}_n$ and is assumed to be fully independent given $\mat{f}_n$.

The graphical model now contains a chain of nodes $\mat{x}_n$, $\mat{f}_n$, $\mat{y}_n$ that is repeated for every data point.
This is a common pattern for generative models.
While the functional of interest $f$ changes depending on the position in the input space $\mat{x}$, the structure of the generative process is assumed to be the same for all observations.
To explicitly represent this repetition, we introduce plate notation:
\begin{align}
    \label{eq:bayesian_ml:non_parametric}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, -1);
        \node[random variable, observed] at (0, 0) (X) {$\mat{x}_n$};
        \node[random variable, latent] at (0, -1) (F) {$\mat{f}_n$};
        \node[random variable, observed] at (0, -2) (Y) {$\mat{y}_n$};
        \draw[edge, directed] (X) -- (F);
        \draw[edge, directed] (F) -- (Y);
        \draw[edge, directed] (F) to[loop left] (F);
        %
        \begin{scope}[on background layer]
            \node[
                align plate,
                inner xsep=10pt,
                fit=(X)(F)(Y),
                label={[anchor=south east]south east:$N$}
            ] {};
        \end{scope}
    \end{tikzpicture}
     &   &
    \Prob*{\mat{x}, \mat{y}}
     & =
    \left\{
    \begin{aligned}
         & \Prob*{\mat{x}_1, \dots, \mat{x}_N}                                               \\
         & \cdot \int \Prob*{\mat{f}_1, \dots, \mat{f}_N \given \mat{x}_1, \dots, \mat{x}_N} \\
         & \quad \cdot \prod_{n=1}^N \Prob*{\mat{y}_n \given \mat{f}_n}                      \\
         & \quad \cdot \diff \mat{f}_1 \dots \diff \mat{f}_N
    \end{aligned}
    \right.
\end{align}
Within the plate, all nodes indexed with $n$ are repeated $N$ times.
The loop on $\mat{f}_n$ denotes that all $\mat{f}_n$ are dependent on each other.
Having separated the functional of interest from the noise term, the next step is to formulate a representation of $\Prob*{\mat{f}_1, \dots, \mat{f}_N \given \mat{x}_1, \dots, \mat{x}_N}$.
The choice of this distribution over functions is informed by two challenges.
First, the support of the distribution can be restricted to a specific class of functions to shrink the hypothesis space.
And second, for a model to be the result of an algorithm, it needs to be represented via finitely many parameters.

There exist two paths to achieving a finite representation:
\begin{labeling}{Non-parametric:}
    \item[Non-parametric:] The functional dependency is represented implicitly using the existing observations $\Dc$. Examples include nearest neighbor models, Gaussian processes or polynomial splines.
    \item[Parametric:] The functional dependency is represented explicitly using a specific class of functions with parameters $\mat{\theta}$. Examples include neural networks, linear regression or polynomial regression.
\end{labeling}
The model in~\cref{eq:bayesian_ml:non_parametric} is non-parametric because it does not contain any parameters besides the $N$ pairs $(\mat{x}_n, \mat{y}_n)$.
Because the functional $f$ is implicit, all $\mat{f}_n$ inform each other.

If $f$ is represented completely and explicitly via a set of parameters $\mat{\theta}$ in a parametric model, the $\mat{f}_n$ are independent given $\mat{\theta}$.
If this conditional independence
\begin{equation}
    \begin{split}
        \Prob*{\mat{f}_n \given \mat{\theta}, \Set{\mat{f}_m \with m \in \Set{1, \dots, N}, m \neq n}}
        =
        \Prob*{\mat{f}_n \given \mat{\theta}}
    \end{split}
\end{equation}
holds, $\mat{\theta}$ is called a sufficient statistic for the functional $f$.
In the parametric graphical model
\begin{align}
    \label{eq:bayesian_ml:parametric}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, -1);
        \node[random variable, observed] at (0, 0) (X) {$\mat{x}_n$};
        \node[random variable, latent] at (0, -1) (F) {$\mat{f}_n$};
        \node[random variable, latent] at (-1, -1) (theta) {$\mat{\theta}$};
        \node[random variable, observed] at (0, -2) (Y) {$\mat{y}_n$};
        \draw[edge, directed] (X) -- (F);
        \draw[edge, directed] (F) -- (Y);
        \draw[edge, directed] (theta) -- (F);
        %
        \begin{scope}[on background layer]
            \node[
                align plate,
                inner xsep=10pt,
                fit=(X)(F)(Y),
                label={[anchor=south east]south east:$N$}
            ] {};
        \end{scope}
    \end{tikzpicture}
     &   &
    \Prob*{\mat{x}, \mat{y}}
     & =
    \left\{
    \begin{aligned}
         & \Prob*{\mat{x}}                                                           \\
         & \cdot \int \prod_{n=1}^N \Prob*{\mat{f}_n \given \mat{x}_n, \mat{\theta}} \\
         & \quad \cdot \Prob*{\mat{\theta}} \diff \mat{\theta}                       \\
         & \quad \cdot \prod_{n=1}^N \Prob*{\mat{y}_n \given \mat{f}_n}              \\
         & \quad \cdot \diff \mat{f}_1 \dots \diff \mat{f}_N                         \\
    \end{aligned}
    \right.
\end{align}
the joint distribution of the $\mat{f}_n$ factorizes given the parameters $\mat{\theta}$ and the loop no longer exists.
Taking linear regression as an example, we assume the functional to be of the shape $f(x) = \mat{W}x + \mat{b}$ and choose the parameters $\mat{\theta} = (\mat{W}, \mat{b})$.
The functional dependency in the graphical model is then given by
\begin{equation}
    \begin{split}
        \Prob*{\mat{f}_n \given \mat{x}_n, \mat{\theta}} = \delta(\mat{W}\mat{x}_n + \mat{b}),
    \end{split}
\end{equation}
where $\delta(\cdot)$ denotes the Dirac delta distribution~\parencite{murphy_machine_2012}.
Since $\mat{\theta}$ is a sufficient statistic, other observations are not required to evaluate the linear function.
Even though we made the strong structural assumption about the linearity of the function resulting in a delta distribution, the marginalization of $\Prob*{\mat{\theta}}$ can still lead to more complicated $\Prob*{\mat{y} \given \mat{x}}$ distributions.

Additional latent structure can be introduced to formulate more complex generative models.
Assume, for example, that the functional of interest is known to have the shape $f(x) = h(k(x), g(x))$.
The corresponding graphical model
\begin{align}
    \begin{tikzpicture}[align graphical model]
        \coordinate (baseline) at (0, -1.5);
        \node[random variable, observed] at (0.5, 0) (X) {$\mat{x}_n$};
        \node[random variable, latent] at (0, -1) (F) {$\mat{k}_n$};
        \node[random variable, latent] at (1, -1) (G) {$\mat{g}_n$};
        \node[random variable, latent] at (-1, -1) (theta) {$\mat{\theta}$};
        \node[random variable, latent] at (0.5, -2) (H) {$\mat{h}_n$};
        \node[random variable, observed] at (0.5, -3) (Y) {$\mat{y}_n$};
        \draw[edge, directed] (X) -- (F);
        \draw[edge, directed] (F) -- (H);
        \draw[edge, directed] (X) -- (G);
        \draw[edge, directed] (G) -- (H);
        \draw[edge, directed] (H) -- (Y);
        \draw[edge, directed] (theta) -- (F);
        \draw[edge, directed, bend left=45] (theta) to (G);
        \draw[edge, directed] (theta) to (H);
        %
        \begin{scope}[on background layer]
            \node[
                align plate,
                inner xsep=10pt,
                fit=(X)(F)(G)(H)(Y),
                label={[anchor=south east]south east:$N$}
            ] {};
        \end{scope}
    \end{tikzpicture}
     &   &
    \Prob*{\mat{x}, \mat{y}}
     & =
    \left\{
    \begin{aligned}
         & \Prob*{\mat{x}}                                                                       \\
         & \cdot \int \prod_{n=1}^N \Prob*{\mat{k}_n \given \mat{x}_n, \mat{\theta}}             \\
         & \quad \cdot \prod_{n=1}^N \Prob*{\mat{g}_n \given \mat{x}_n, \mat{\theta}}            \\
         & \quad \cdot \prod_{n=1}^N \Prob*{\mat{h}_n \given \mat{k}_n, \mat{g}_n, \mat{\theta}} \\
         & \quad \cdot \Prob*{\mat{\theta}} \diff \mat{\theta}                                   \\
         & \quad \cdot \prod_{n=1}^N \Prob*{\mat{y}_n \given \mat{k}_n, \mat{g}_n, \mat{h}_n}    \\
         & \quad \cdot \diff \Set*{\mat{k}_n, \mat{g}_n, \mat{h}_n}_{n=1}^N                      \\
    \end{aligned}
    \right.
\end{align}
contains separate nodes for the different functions.
By constructing a hypothesis space as a combination of multiple sub-hypothesis spaces, generative models can represent detailed assumptions while remaining formally principled through marginalization.
Having formulated structural assumptions about how a data set has been generated, the next step is interpret these assumptions in a statistical learning context and connect them with data.

\subsubsection{Bayesian inference}
The generative model for linear regression formulated in~\cref{eq:bayesian_ml:parametric} formulates a factorization of the unknown data distribution $\Prob*{\mat{x}, \mat{y}}$.
By introducing the parameters $\mat{\theta} = (\mat{W}, \mat{b})$ which are sufficient statistics for the functional dependency between $\mat{x}$ and $\mat{y}$ under our assumptions, learning about the regression functional $f$ has been reduced to learning about finitely many parameters.
In other words, the formulation of a generative model yields a formal description of what we mean by linear regression in a statistical learning context.
The next step is to derive a learning algorithm that connects this model to observations.

In~\cref{toc:bayesian_ml:statistical_learning}, a good solution to a learning problem was characterized with the risk minimization algorithm, which demands that any data that could be observed form the data distribution would be well-explained by the solution.
In the context of generative models, we can formulate the same idea by demanding that sampling from the true data distribution or from the generative model leads to the same data sets.
Or equivalently, that samples drawn from the data distribution have high probability under the generative model and vice versa.
Since we do not have access to the full data distribution, we use the available observations as an empirical estimate again.

To make statements about the interaction of the parameters $\mat{\theta}$ and the observations $\Dc = \Set*{(\mat{x}_n, \mat{y}_n)}_{n=1}^N$, we consider their joint probability distribution $\Prob*{\mat{\theta}, \Dc}$.
Using the chain rule, this distribution can be written as a product
\begin{align}
    \Prob*{\mat{\theta}, \Dc} & = \Prob*{\Dc \given \mat{\theta}}\Prob*{\mat{\theta}},
\end{align}
whose terms are called the likelihood $\Prob*{\Dc \given \mat{\theta}}$ and the prior $\Prob*{\mat{\theta}}$.
The structural assumptions formulated via the generative model allow us to directly evaluate the likelihood $\Prob*{\Dc \given \mat{\theta}} = \Prob*{\mat{x}, \mat{y} \given \mat{\theta}}$.
The prior $\Prob*{\mat{\theta}}$ is a distribution over all possible parameters $\mat{\theta}$ and is part of the joint formulated in the generative model.
Applying the chain rule on the joint again yields the posterior
\begin{align}
    \Prob*{\mat{\theta} \given \Dc}
     & = \frac{\Prob*{\Dc \given \mat{\theta}}\Prob*{\mat{\theta}}}{\Prob*{\Dc}},
\end{align}
where $\Prob*{\Dc} = \int \Prob*{\Dc \given \mat{\theta}} \Prob*{\mat{\theta}} \diff \mat{\theta}$ is a combination of the likelihood and and prior terms.
Because all terms on the right-hand-side are known, this posterior can be evaluated, yielding a combination of modeling assumptions and observations.
If a closed-form solution cannot be found, Bayesian inference is often implemented using approximation techniques such as sampling or variational approaches~\parencite{bishop_pattern_2007}.

Having found a posterior $\Prob{\mat{\hat{\theta}}} = \Prob*{\mat{\theta} \given \Dc}$, Bayesian predictions for previously unseen points can be made by replacing $\Prob*{\mat{\theta}}$ with $\Prob{\mat{\hat{\theta}}}$ in the graphical model.
In the regression case, this predictive posterior is given by
\begin{align}
    \Prob*{\mat{y}_\ast \given \mat{x}_\ast} & = \int \Prob*{\mat{y}_\ast \given \mat{f}_\ast} \Prob{\mat{f}_\ast \given \mat{x}_\ast, \mat{\hat{\theta}}} \Prob{\mat{\hat{\theta}}} \diff \mat{f}_\ast \diff \mat{\hat{\theta}}.
\end{align}
Because Bayesian linear regression is a parametric model and we assumed that $\mat{\theta}$ is a sufficient statistic for $\mat{f}$, predictions can be made independently of the training data in this case.

In~\cref{toc:bayesian_ml:statistical_learning} we argued that considering finitely many observations instead of the full data distribution can lead to overfitting.
An overfitted model explains the observed data well but might have a high risk for unseen parts of the data distribution.
The empirical risk minimization learning algorithm can be subject to overfitting because the algorithm selects a single model from the hypothesis space that explains the data well.
Bayesian inference is fundamentally different:
A Bayesian posterior $\Prob*{f \given \Dc}$ for a functional of interest $f$ is a distribution over hypotheses rather than one single candidate.
This distribution can be thought of as a subset of hypotheses weighed by how well the different hypotheses explain the data.
Instead of selecting a good candidate, a Bayesian inference step can be thought of as removing all bad candidates from a hypothesis space.
As a consequence, a Bayesian posterior contains both the models in the original hypothesis space that overfit to the observations as well as the more desirable models with similar data likelihoods.
Their relative weights in the posterior are dependent on the prior assumptions formulated via the structure of the generative model and the prior $\Prob{\mat{\theta}}$.


\subsubsection{Bayesian model selection}
\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_bayes_broad}
        \caption{
            Uncertain prior $\mat{W}, \mat{b} \sim \Gaussian{0, 5^2}$
            \label{fig:bayesian_ml:polynomials:bayes_broad}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_bayes_narrow}
        \caption{
            Certain prior $\mat{W}, \mat{b} \sim \Gaussian{1, 1^2}$
            \label{fig:bayesian_ml:polynomials:bayes_narrow}
        }
    \end{subfigure}
    \caption[Bayesian linear regression posteriors]{
        \label{fig:bayesian_ml:polynomials:bayes}
        Two predictive posteriors for a Bayesian linear regression on the same data.
        Different choices for prior distributions lead to different predictive uncertainties.
        Which posterior to use is problem-dependent and a subjective choice.
    }
\end{figure}
Statements about Bayesian models are made in terms of probabilities, including predictive posteriors for new inputs $\Prob*{\mat{y}_\ast \given \mat{x}_\ast, \Dc}$ in a regression problem or a posterior distributions $\Prob*{\mat{\theta} \given \Dc}$ for a parameter.
This distribution of predictions or parameters can be interpreted as uncertainty due to insufficient knowledge about the generative process.
A Bayesian posterior is derived as the combination of the prior knowledge and the data.
Since a Bayesian posterior is the combination of the data likelihood and the prior assumptions, the posterior uncertainties are not objective but depend on the prior assumptions.

\Cref{fig:bayesian_ml:polynomials:bayes} shows two predictive posteriors for a Bayesian linear regression on the data introduced in~\cref{fig:bayesian_ml:polynomials:data} using the graphical model in~\cref{eq:bayesian_ml:parametric}.
Both models of linear regression are based on the same observations but differ in the prior assumptions about $\mat{\theta} = (\mat{W}, \mat{b})$.
Choosing a broad prior $\mat{W}, \mat{b} \sim \Gaussian{0, 5^2}$ results in higher posterior uncertainties about both $\mat{W}$ and $\mat{b}$ than choosing a more narrow prior $\mat{W}, \mat{b} \sim \Gaussian{1, 1^2}$.
Samples drawn from the posterior show why this is the case:
A broader prior contains linear functions with steep slopes that could explain the data but are never considered by the narrow prior.
Not considering these functions might be the desired behavior if implied by the expert knowledge.
This situation of having to choose a model from a set of plausible models is a model selection problem.

In principle, model selection is not necessary in Bayesian machine learning.
Because the result of inference is a distribution over models weighed by their plausibility, model selection is already included in a Bayesian posterior.
However, this is only true for model components that receive a Bayesian treatment by calculating a posterior.
No prior is typically placed on the structure of the graphical model itself or the parametric forms of its components, such as the linearity assumption in Bayesian linear regression.
For fully Bayesian treatments, priors would also have to be placed on priors, forming hyper-priors.
Because such a fully Bayesian treatment is generally computationally intractable, certain parameters or structural assumptions are typically fixed to one specific instantiation, a point estimate.
Introducing point estimates also introduces a model selection problem.

Many strategies exist for Bayesian model selection~\parencite{andrew_gelman_bayesian_2013,murphy_machine_2012,david_barber_bayesian_2012} which are not discussed in detail here.
Most strategies are related to the ideas of empirical risk minimization and consider a performance measure on the observed data.
Given two hypotheses $H_1$ and $H_2$, it is common to compare the marginal likelihoods $\Prob*{\Dc \given H_1}$ and $\Prob*{\Dc \given H_2}$ where $\Prob*{\Dc \given H} = \prod_{n=1}^N \Prob*{\mat{y}_n \given \mat{x}_n, H}$.
Choosing the hypothesis $H_i$ with a higher marginal likelihood is called a maximum likelihood estimation.
Observing that
\begin{equation}
    \begin{split}
        \frac{\Prob*{H_1 \given \Dc}}{\Prob*{H_2 \given \Dc}} = \frac{\Prob*{\Dc \given H_1}}{\Prob*{\Dc \given H_2}} \frac{\Prob*{H_1}}{\Prob*{H_2}},
    \end{split}
\end{equation}
a common extension to maximum likelihood estimation is to also consider how well the hypotheses conform to the prior assumption through the prior odds $\sfrac{\Prob*{H_1}}{\Prob*{H_2}}$.

While strategies based on marginal likelihoods often work well in practice, they are limited in scope.
Since marginal distributions are considered, models are not evaluated with respect to the latent components in the generative model, such as the shape of $\Prob*{\mat{f}}$ in Bayesian linear regression.
Selecting models using maximum likelihood approaches enforces good predictive uncertainties around the observations in $\Dc$.
No requirements are enforced concerning uncertainties away from the data, the shape of the individual samples drawn from the model or the uncertainties in the latent part of the generative model.

\section{Gaussian processes}
Bayesian graphical models encode structural assumptions about a machine learning problem.
The central assumption in the regression problem is a latent function $f$ that maps inputs to outputs.
In the Bayesian linear regression example, a posterior over linear functions could be derived by calculating a posterior over parameters instead, since every parameter explicitly represents a linear function.
A Gaussian process (GP) is a non-parametric model that directly represents a distribution over functions~\parencite{bernardo_regression_1998,rasmussen_gaussian_2006}.
Instead of formulating an explicit parameterized formula, a GP prior encodes more general assumptions about $f$ such as differentiability or variability.
A GP posterior can be calculated analytically, making Bayesian inference over GPs computationally feasible.

Gaussian processes are a generalization of the Gaussian distribution to function spaces.
A multivariate Gaussian $\mat{x} \sim \Gaussian{\mat{\mu}, \mat{\Sigma}}$ describes a distribution over the finitely many elements in the vector $\mat{x}$.
Every such element $\rv{x}_i$ is normally distributed according to $\rv{x}_i \sim \Gaussian{\mu_i, \Sigma_{ii}}$ and every linear combination of the $\mat{x}_i$ is also normally distributed~\parencite{astrom_introduction_1971}.
For every pair $(\rv{x}_i, \rv{x}_j)$, their covariance is given by $\Moment{\cov}{\rv{x}_i, \rv{x}_j} = \mat{\Sigma}_{ij}$.
The probability density of $\mat{x}$ is given by
\begin{equation}
    \begin{split}
        \label{eq:gaussian_pdf}
        \Prob*{\mat{x}}
        &= \Gaussian{\mat{x} \given \mat{\mu}, \mat{\Sigma}} \\
        &= \frac{1}{\sqrt{\det(2\pi\mat{\Sigma})}}\Fun*{\exp}{-\frac{1}{2}(\mat{x} - \mat{\mu})\tran \mat{\Sigma}\inv (\mat{x} - \mat{\mu})}.
    \end{split}
\end{equation}

Multivariate Gaussians have several convenient closure properties~\parencite{astrom_introduction_1971}.
Assume a split $\mat{x} = (\mat{x}_1, \mat{x}_2)$ into two partial vectors and denote
\begin{equation}
    \begin{split}
        \begin{pmatrix}
            \mat{x}_1 \\
            \mat{x}_2
        \end{pmatrix}
        \sim \Gaussian*{
            \begin{pmatrix}
                \mat{\mu}_1 \\
                \mat{\mu}_2
            \end{pmatrix},
            \begin{pmatrix}
                \mat{\Sigma}_{11} & \mat{\Sigma}_{12} \\
                \mat{\Sigma}_{21} & \mat{\Sigma}_{22}
            \end{pmatrix}
        }
    \end{split}
\end{equation}
the same split of the mean vector $\mat{\mu}$ and covariance matrix $\mat{\Sigma}$.
Then, the marginal distribution of $\mat{x}_1$ is also a Gaussian with
\begin{equation}
    \begin{split}
        \label{eq:gaussian_marginal}
        \Prob*{\mat{x}_1}
        &= \int \Prob*{\mat{x}_1, \mat{x}_2} \diff \mat{x}_2 \\
        &= \Gaussian{\mat{\mu}_1, \mat{\Sigma}_1}
    \end{split}
\end{equation}
and the conditional of $\mat{x}_1$ given $\mat{x}_2$ is a Gaussian as well with
\begin{equation}
    \begin{split}
        \label{eq:gaussian_conditional}
        \Prob*{\mat{x}_1 \given \mat{x}_2}
        &= \frac{\Prob*{\mat{x}_1, \mat{x}_2}}{\Prob*{\mat{x}_2}} \\
        &= \Gaussian{\hat{\mat{\mu}}, \hat{\mat{\Sigma}}}\text{, and} \\[\smallskipamount]
        \hat{\mat{\mu}}
        &= \mat{\mu}_1 + \mat{\Sigma}_{12}\mat{\Sigma}\inv_{22} (\mat{x}_2 - \mat{\mu}_2) \\[\smallskipamount]
        \hat{\mat{\Sigma}}
        &= \mat{\Sigma}_{11} - \mat{\Sigma}_{21} \mat{\Sigma}\inv_{22} \mat{\Sigma}_{12} \\
        &= \mat{\Sigma}_{11} - \mat{\Sigma}\tran_{12} \mat{\Sigma}\inv_{22} \mat{\Sigma}_{12}.
    \end{split}
\end{equation}

Modeling functions over infinite sets requires an infinite number of random variables, one for every function value.
Such structure of a number of possibly dependent random variables mapping from the same probability space to the same value space is called a stochastic process and is represented via a function.
\begin{definition}[Stochastic Process]
    \label{def:gp:stochastic_process}
    Given a probability space $(\Omega, \mathcal{F}, P)$, an index set $T$ and a measurable space $Y$, a stochastic process $\rv{X}$ is a function
    \begin{align}
        \rv{X} \colon \left\{\begin{aligned}
            T \times \Omega & \to Y                    \\
            (t, \omega)     & \mapsto \rv{X}_t(\omega)
        \end{aligned}\right.
    \end{align}
    mapping indices $t$ to $Y$-valued random-variables.
    For a fixed $\omega \in \Omega$, $\rv{X}(\cdot, \omega)$ is called a trajectory of the process \cite{astrom_introduction_1971}.
\end{definition}

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[x=4em, y=5em]
        \node[random variable, observed] at (0, 0) (X) {$\mat{x}_n$};
        \node[random variable, latent] at (0, -1) (F) {$\mat{f}_n$};
        \node[random variable, hyperparameter] at (-1, -0.75) (mean) {$\mu$};
        \node[random variable, hyperparameter] at (-1, -1.25) (kernel) {$\K$};
        \node[random variable, observed] at (0, -2) (Y) {$\mat{y}_n$};
        \draw[edge, directed] (X) -- (F);
        \draw[edge, directed] (F) -- (Y);
        \draw[edge, directed, loop right] (F) to (F);
        \draw[edge, directed] (mean) -- (F);
        \draw[edge, directed] (kernel) -- (F);
        %
        \begin{scope}[on background layer]
            \node[
                align plate,
                inner xsep=10pt,
                fit=(X)(F)(Y),
                label={[anchor=south east]south east:$N$}
            ] {};
        \end{scope}
    \end{tikzpicture}
    \caption[Graphical model: Gaussian process]{
        \label{fig:gp:graphical_model}
        The graphical model of a Gaussian process with $N$ observations $(\mat{x}_n, \mat{y}_n)$.
        Different observations are independent given the latent function values $\mat{f}_n$ which are all jointly Gaussian.
        They are informed by the mean function $\mu$ and kernel $\K$.
    }
\end{figure}
The index set of a stochastic process can be an arbitrary set.
It is often interpreted as a time index which can be both discrete and continuous.
A Gaussian process is a particular stochastic process.
\begin{definition}[Gaussian Process]
    \label{def:gp:gaussian_process}
    A stochastic process $\rv{X}$ is called a Gaussian process if for any finite subset $\tau \subseteq T$ of its index set, the random variables $\rv{X}_\tau$ have a joint Gaussian distribution \cite{astrom_introduction_1971}.
\end{definition}
When using a Gaussian process $\rv{X}$ to model a function $f \colon A \to B$, the index set $T$ is assumed to be $A$ and all random variables are $B$-valued.
The random variable $\rv{X}_a$ then models the function value $f(a)$ for all $a \in A$.
Sampling a trajectory from $\rv{X}$ corresponds to sampling one possible function $f^\ast$.

Similar to the finite case, the random variables share a dependency structure.
Instead of a mean vector $\mat{\mu}$ and a covariance matrix $\mat{\Sigma}$, a Gaussian process is completely determined by a mean function $\mu(a) = \Moment{\E}{f(a)}$ and a covariance function
\begin{equation}
    \begin{split}
        \K(a, a^\prime) &\coloneqq \Moment{\E}{(f(a) - \mu(a))(f(a^\prime) - \mu_f(a^\prime))} \\
        &= \Moment{\cov}{f(a), f(a^\prime)} \\
        &= \Moment{\cov}{\rv{X}_a, \rv{X}_{a^\prime}}
    \end{split}
\end{equation}
with $a, a^\prime \in A$.
The mean function encodes the point-wise mean over all trajectories that could be sampled from $\rv{X}$.
The covariance function is also called a kernel and describes the interaction between different parts of the function.
A function that is distributed according to a Gaussian process is denoted as $f \sim \GP\Cond{\mu, \K}$.

A GP can be used as a distribution over functions in the graphical model for regression problems in~\cref{eq:bayesian_ml:non_parametric}.
Because the random variables $\mat{f}_n$ modeling the function value $f(\mat{x}_n)$ are jointly Gaussian, they are not independent and thus are connected in the graphical model in~\cref{fig:gp:graphical_model}.
The choice of the mean function
\tikz[x=2.5em,baseline=(f.base)]{
    \node[inline random variable, hyperparameter] (f) {$\mu$};
}
and the kernel
\tikz[x=2.5em,baseline=(f.base)]{
    \node[inline random variable, hyperparameter] (f) {$\K$};
}
describe the GP prior and are often referred to as hyper-parameters.

For convenience, the prior mean function $\mu$ is often assumed to be constant zero.
This assumption is without loss of generality \cite{rasmussen_gaussian_2006} since otherwise, the observations $\left( \mat{X}, \mat{y} \right)$ can be transformed to $\mat{y^\prime} = \mat{y} - \mu(\mat{X})$.
The Gaussian process based on the observations $\left( \mat{X}, \mat{y^\prime} \right)$ then only models the differences to the mean function.
It is the covariance functions that encode the assumptions about the underlying function.

\subsubsection{Kernels}
The covariance for any pair of random variables $(\rv{X}_i, \mat{X}_j)$ in a GP is given by the kernel $\Moment{\cov}{\rv{X}_i, \rv{X}_j} = \K(i, j)$.
A kernel, therefore, can not be any arbitrary function but must yield valid covariance matrices $\mat{\Sigma}$.
The matrix obtained by applying a kernel pairwise to finitely many random variables is called the Gram matrix.
Given two sets $\mat{A} = \Set*{\mat{a}_i \with i \in [n]}$ and $\mat{B} = \Set*{\mat{b}_j \with j \in [m]}$ and $[n] = \Set*{1, \dots, n}$, the Gram matrix with respect to $\mat{A}$ and $\mat{B}$ using kernel $\K$ is given by
\begin{align}
    \K(\mat{A}, \mat{B}) = \mat{K_{\mat{A}\mat{B}}} \coloneqq \bigg( \K(\mat{a}_i, \mat{b}_j) \bigg)_{\substack{i \in [n], \\ j \in [m]}}.
\end{align}

For the Gram matrix to be a valid covariance matrix $\mat{\Sigma}$ of a Gaussian distribution, it must be positive definite.
Kernels are functions that fulfill the property that for every possible subset of random variables, or more generally every set of elements in their domain, their induced Gram matrix is positive definite.
\begin{definition}[Kernel]
    Given a non-empty set $A$, a function
    \begin{align}
        \K \colon A^2 \to \Rb
    \end{align}
    is called a (positive definite) kernel or covariance function, if for any finite subset $X \subseteq A$, the Gram matrix $\K(X, X)$ is positive definite.
\end{definition}
The kernel is crucial in encoding the assumptions about the function a Gaussian process should estimate.
It is a measure of how much different points in the GP's domain inform each other.
A natural assumption to make is that the closer together in the domain two points lie, the more similar their function values will be.
Similarly, to predict a test point, training points close to it are probably more informative than those further away.

But closeness is not the only possible reason two points could be similar.
Assume a function that is a possibly noisy sinusoidal wave with a known frequency.
Then, two points that are a multiple of wavelengths apart should also have similar function values.
Such a kernel which is not only dependent on the distance between two points but also their position in the input space is called non-stationary.
A simple example of such a non-stationary kernel is the linear kernel.
\begin{definition}[Linear Kernel]
    For a finite dimensional euclidean vector space $\Rb^d$, the linear kernel is defined as
    \begin{align}
        \K_{\text{linear}}(\mat{x}_i, \mat{x}_j) \coloneqq \mat{x}_i\tran \mat{x}_j = \left\langle \mat{x}, \mat{x}_j\right\rangle.
    \end{align}
\end{definition}
Consider a function $f \colon \Rb \to \Rb$ which is distributed according to a GP with the linear kernel $f \sim \GP\Cond{\mat{0}, \K_{\text{linear}}}$.
According to the definition of GPs, for any two input numbers $x_i, x_j \in \Rb$ their corresponding random variables $\rv{f}_{x_i}$ and $\rv{f}_{x_j}$ have a joint Gaussian distribution
\begin{align}
    \begin{pmatrix}
        \rv{f}_{x_i} \\ \rv{f}_{x_j}
    \end{pmatrix} \sim \Gaussian*{\mat{0}, \begin{bmatrix}
            \K(x_i, x_i) & \K(x_i, x_j) \\
            \K(x_j, x_i) & \K(x_j, x_j)
        \end{bmatrix}}.
\end{align}
Assuming that both $x_i$ and $x_j$ are not equal to zero, the correlation coefficient $\rho$ of these two variables is given by
\begin{equation}
    \begin{split}
        \Moment{\rho}{\rv{f}_{x_i}, \rv{f}_{x_j}}
        &= \frac{\Moment{\cov}{\rv{f}_{x_i}, \rv{f}_{x_j}}}{\sqrt{\Moment{\var}{\rv{f}_{x_i}\vphantom{\rv{f}_{x_j}}}}\sqrt{\Moment{\var}{\rv{f}_{x_j}}}} \\
        &= \frac{\K(x_i, x_j)}{\sqrt{\K(x_i, x_i)} \sqrt{\K(x_j, x_j)}} = \frac{x_ix_j}{\sqrt{\vphantom{x_j^2}x_i^2}\sqrt{\vphantom{x_j^2}x_j^2}} \in \left\{ -1, 1 \right\}.
    \end{split}
\end{equation}
A correlation coefficient of plus or minus one implies that the value of one of the random variables is a linear function of the other.
Any function drawn from this Gaussian process, such as the ones shown in \cref{fig:gp:gp_samples:linear}, is, therefore, a linear function.
This observation generalizes to higher dimensions \cite{rasmussen_gaussian_2006}.
Gaussian process regression with a linear kernel is equivalent to Bayesian linear regression as discussed in~\cref{toc:bayesian_ml:bayesian_ml}.
\begin{figure}[p]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_prior_linear}
        \caption{
            Linear
            \label{fig:gp:gp_samples:linear}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_prior_rbf}
        \caption{
            SE with $\sigma_f = 1$ and $l=1$
            \label{fig:gp:gp_samples:rbf_normal}
        }
    \end{subfigure}\\[\figureskip]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_prior_rbf_amplitude}
        \caption{
            SE with $\sigma_f = \sqrt{2}$ and $l=1$
            \label{fig:gp:gp_samples:rbf_noisy}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_prior_rbf_lengthscale}
        \caption{
            RSE with $\sigma_f = 1$ and $l = \sfrac{1}{4}$
            \label{fig:gp:gp_samples:rbf_lengthscale}
        }
    \end{subfigure}\\[\figureskip]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_prior_arccos_1}
        \caption{
            Arc-cos with order 1
            \label{fig:gp:gp_samples:arccos:1}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_prior_arccos_2}
        \caption{
            Arc-cos with order 2
            \label{fig:gp:gp_samples:arccos:2}
        }
    \end{subfigure}
    \caption[Samples from GP priors]{
        \label{fig:gp:gp_samples}
        A comparison of samples drawn from GP priors with different kernels and hyper-parameters.
        Dashed lines are single samples and the shaded area depicts two standard deviations around the mean.
        While samples from the linear kernel are always linear functions, samples from squared exponential (SE) kernels are smooth functions of different variability.
        The arc cosine kernel mimics the behavior of an infinitely wide neural network with a single hidden layer.
    }
\end{figure}

It is often of interest to also represent non-linear dependencies.
A common approach is to restrict information to local neighborhoods.
A kernel which is a function of $\norm{\mat{x}_i - \mat{x}_j}$ is called stationary and is invariant to translations in the input space.
The most important stationary kernel is the squared exponential kernel.
\begin{definition}[Squared Exponential Kernel]
    \label{def:gp:rbf_kernel}
    For a finite dimensional euclidean vector space $\Rb^d$, the squared exponential kernel or RBF kernel is defined as
    \begin{equation}
        \K_{\text{SE}}(\mat{x}_i, \mat{x}_j) \coloneqq \sigma_f^2 \cdot \exp\!\left( -\frac{1}{2} (\mat{x}_i - \mat{x}_j)\tran \mat{\Lambda}^{-1} (\mat{x}_i - \mat{x}_j) \right).
    \end{equation}
    The parameter $\sigma_f^2 \in \Rb_{>0}$ is called the signal variance and $\mat{\Lambda} = \diag(l_1^2, \dots, l_d^2)$ is a diagonal matrix of the squared length scales $l_i \in \Rb_{>0}$.
\end{definition}
The similarity of two data points approaches one when they are close together and for larger distances approaches zero with exponential drop off.
It can be shown that this kernel represents all infinitely differentiable functions \cite{rasmussen_gaussian_2006}.
Gaussian processes with this covariance function are universal function approximators.

The squared exponential kernel is dependent on multiple parameters that influence its behavior.
In contrast to weight parameters in linear regression or constants in physical models, these parameters do not specify the estimated function but rather the prior belief about this function and are therefore hyper-parameters.

The hyper-parameters of the RBF kernel describe the expected dynamic range of the function.
The signal variance $\sigma_f^2$ specifies the average distance of function values from the mean function.
The different length scale parameters $l_i$ roughly specify the distance of data points along their respective axis required for the function values to change considerably.
\Cref{fig:gp:gp_samples} compares sample functions drawn from Gaussian processes with the linear kernel, squared exponential kernels with different hyper-parameters, and the arc cosine kernel~\parencite{cho_kernel_2009}.
Arc cosine kernels mimic the behavior of infinitely wide neural networks with a single hidden layer.
The order of an arc cosine kernel specifies the activation function of such a neural network.
The first three orders assume step-functions, rectified linear units, or rectified quadratic units respectively.
Arc cosine kernels of low order behave similarly to squared exponential kernels in practice but have different generalization properties.
While GPs with an RBF kernel return to the prior away from data yielding predictions roughly in the area $\mu_f \pm 2\sigma_f$, the arc cosine kernel of order one generalizes linearly using the derivative at the closest data points.

\subsubsection{Predictions and posterior}
To use Gaussian processes for regression, observations need to be combined with a Gaussian process prior $f \sim \GP\Cond{\mat{0}, \K}$ to obtain a predictive posterior.
We assume Gaussian noise $\epsilon \sim \Gaussian{0, \sigma^2}$ with $\mat{y}_n = f(\mat{x}_n) + \epsilon$ and denote the $N$ observations as $\mat{X} = (\mat{x}_1, \dots, \mat{x}_N)$ and $\mat{y} = (\mat{y}_1, \dots, \mat{y}_N)$.
The likelihood of the observations given the latent function values $\mat{f}$ is given by
\begin{equation}
    \begin{split}
        \Prob{\mat{y} \given f, \mat{X}, \sigma}
        = \Prob{\mat{y} \given \mat{f}, \sigma}
        &= \prod_{n = 1}^N \Gaussian{\mat{y}_n \given \mat{f}_n, \sigma^2} \\
        &= \Gaussian{\mat{y} \given \mat{f}, \sigma^2 \Eye}.
    \end{split}
\end{equation}
Given a vector of hyper-parameters $\mat{\theta}$, the definition of Gaussian processes yields a joint Gaussian distribution for the latent function values $\mat{f}$ given by
\begin{align}
    \Prob{\mat{f} \given \mat{X}, \mat{\theta}} = \Gaussian*{\mat{f} \given \mat{0}, \mat{K}_{\mat{f}\mat{f}}}
\end{align}
where $\mat{K}_{\mat{f}\mat{f}} = \K(\mat{X}, \mat{X})$ denotes the Gram matrix of the observed data.
Combining the two distributions according to the law of total probability yields the probability distribution of the outputs conditioned on the inputs and is given by
\begin{equation}
    \begin{split}
        \label{eq:gp:gp_marginal_likelihood}
        \Prob{\mat{y} \given \mat{X}, \mat{\theta}} &= \int \Prob{\mat{y} \given \mat{f}} \Prob{\mat{f} \given \mat{X}, \mat{\theta}} \diff \mat{f} \\
        &= \int \Gaussian{\mat{y} \given \mat{f}, \sigma^2 \Eye} \Gaussian*{\mat{f} \given \mat{0}, \mat{K}_{\mat{f}\mat{f}}} \diff \mat{f} \\
        &= \Gaussian{\mat{y} \given \mat{0}, \mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye}.
    \end{split}
\end{equation}
Note that this distribution is obtained by integrating over all possible latent function values $\mat{f}$ and thereby taking all possible function realizations into account.
The closed-form solution of the integral is obtained using well-known results about Gaussian distributions, wich are, for example, detailed in \cite{petersen_matrix_2008}.

\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_posterior_prior}
        \caption{GP Prior}
        \label{fig:gp:gp_posterior:prior}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/gp_posterior_posterior}
        \caption{GP Posterior}
        \label{fig:gp:gp_posterior:posterior}
    \end{subfigure}
    \caption[GP posterior]{
        \label{fig:gp:gp_posterior}
        A GP prior with an RBF kernel (left) and posterior given a six observations (right).
        The posterior has a non-zero mean function that interpolates the data exactly.
        Samples from the posterior interpolate the data but vary in-between.
    }
\end{figure}
Now consider a set of new points $\mat{X}_\ast$ for which the predictive posterior should be obtained.
By definition of GPs, the latent function values $\mat{f}$ of the data set and the latent function values at the new points $\mat{f}_\ast = f(\mat{X}_\ast)$ have the joint Gaussian distribution
\begin{align}
    \Prob*{\begin{pmatrix}
            \mat{f} \\
            \mat{f}_\ast
        \end{pmatrix} \given \mat{X}, \mat{X}_\ast, \mat{\theta}} & = \Gaussian*{\begin{pmatrix}
            \mat{f} \\
            \mat{f}_\ast
        \end{pmatrix} \given \mat{0}, \begin{bmatrix}
            \mat{K}_{\mat{f}\mat{f}} & \mat{K}_{\mat{f}\ast} \\
            \mat{K}_{\ast\mat{f}}    & \mat{K}_{\ast\ast}
        \end{bmatrix}}.
\end{align}
Adding the noise model to this distribution leads to the joint Gaussian of training outputs $\mat{y}$ and test outputs $\mat{f}_\ast$ which is given by
\begin{align}
    \label{eq:gp:predictive_joint}
    \Prob*{\begin{pmatrix}
            \mat{y} \\
            \mat{f}_\ast
        \end{pmatrix} \given \mat{X}, \mat{X}_\ast, \mat{\theta}} & = \Gaussian*{\begin{pmatrix}
            \mat{y} \\
            \mat{f}_\ast
        \end{pmatrix} \given \mat{0}, \begin{bmatrix}
            \mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye & \mat{K}_{\mat{f}\ast} \\
            \mat{K}_{\ast\mat{f}}                    & \mat{K}_{\ast\ast}
        \end{bmatrix}}.
\end{align}
In this distribution, the training outputs $\mat{y}$ are known.
The predictive posterior for the test outputs $\mat{f}_\ast$ can therefore be directly obtained by applying~\cref{eq:gaussian_conditional} to~\cref{eq:gp:predictive_joint} and is also a Gaussian.
\begin{lemma}[GP predictive posterior]
    \label{lem:gp:gp_posterior}
    Assume a latent function with a Gaussian process distribution $f \sim \GP(\mat{0}, \K)$ and $N$ training points $\mat{X}$ with noisy observations of the form $\mat{y} = f(\mat{X}) + \Gaussian{\mat{0}, \sigma^2 \Eye}$.
    The predictive posterior $\mat{f}_\ast$ of the test points $\mat{X}_\ast$ is then given by
    \begin{equation}
        \begin{split}
            \Prob{\mat{f}_\ast \given \mat{X}, \mat{y}, \mat{X}_\ast}
            &= \Gaussian*{\mat{f}_\ast \given \mat{\mu}_\ast, \mat{\Sigma}_\ast} \text{, where} \\
            \mat{\mu}_\ast
            &= \mat{K}_{\ast \mat{f}} \left( \mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye \right)\inv \mat{y} \\
            \mat{\Sigma}_\ast
            &= \mat{K}_{\ast\ast} - \mat{K}_{\ast \mat{f}} \left( \mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye \right)\inv \mat{K}_{\mat{f}\ast}.
        \end{split}
    \end{equation}
\end{lemma}

This predictive posterior makes it possible to evaluate the function approximation based on the input at arbitrary points in the input space.
Since any set of these points always has a joint Gaussian distribution, the predictive posterior defines a new Gaussian process, the posterior Gaussian process, given the observations.
This posterior process $\GP(\mu_\text{post}, \K_\text{post})$ has new mean and covariance functions given by
\begin{equation}
    \begin{split}
        \mu_\text{post}(\mat{a}) &= \K(\mat{a}, \mat{X}) \left(\mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye \right)\inv \mat{y} \\
        \K_\text{post}(\mat{a}, \mat{b}) &= \K(\mat{a}, \mat{b}) - \K(\mat{a}, \mat{X}) \left( \mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye \right)\inv \K(\mat{X}, \mat{b}).
    \end{split}
\end{equation}
Note that the posterior mean function is not necessarily the constant zero function.
\Cref{fig:gp:gp_posterior} shows samples from a pair of prior and posterior Gaussian processes with an RBF kernel.

Computing the inverse $\left(\mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye \right)\inv$ takes $\Oh(N^3)$ time but can be done as a preprocessing step since it is independent of the test points.
Predicting the mean function value of a single test point is a weighted sum of $N$ basis functions $\mu_\ast = \mat{K_{\ast \mat{f}}} \mat{\beta}$ where $\mat{\beta} = \left(\mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye \right)\inv \mat{y}$ which can be precomputed.
After this pre-computation, predicting the mean of a single test point takes $\Oh(N)$ time.
To predict the variance, it is still necessary to perform a vector-matrix multiplication, which takes $\Oh(N^2)$ time for a single prediction.
Since all of these operations are dependent on the number of training points, evaluating Gaussian processes on large data sets can be computationally expensive.
Before introducing sparse approximations with better asymptotic complexity, we first consider how to choose good values for the hyper-parameters $\mat{\theta}$.

\subsubsection{Choosing hyper-parameters}
In the previous section, we derived the posterior GP given constant hyper-parameters $\mat{\theta}$.
In this case, Gaussian process models do not have to be trained or optimized at all as the posterior GP can be computed analytically.
Usually, however, the correct choice of hyper-parameters is not clear a priori.
In a fully Bayesian setup we place a prior on the hyper-parameters $\Prob{\mat{\theta}}$ and marginalize it to derive the dependent distributions
\begin{equation}
    \begin{split}
        \label{eq:gp:theta_posterior_integration}
        \Prob{f}
        & = \int \Prob{f \given \mat{\theta}} \Prob{\mat{\theta}} \diff \mat{\theta}                                                           \\
        \Prob{\mat{y} \given \mat{X}}
        & = \int \Prob{\mat{y} \given \mat{f}} \Prob*{\mat{f} \given \mat{X}, \mat{\theta}} \Prob{\mat{\theta}} \diff \mat{f} \diff \mat{\theta}.
    \end{split}
\end{equation}
Updating the belief about the distribution of the hyper-parameters then becomes part of calculating the posterior using Bayes' theorem.
However, the integration required in \cref{eq:gp:theta_posterior_integration} is expensive as no closed form solution exists.
While true posteriors can be obtained for GPs with few observations such as in Bayesian optimization or probabilistic numerics contexts~\parencite{shahriari_taking_2016,oates_modern_2019}, the required calculations are often not tractable for larger problems.

A common approximation is to use maximum-a-posteriori point-estimates instead.
These estimates are obtained by maximizing $\Prob{\mat{\theta} \given \mat{X}, \mat{y}}$.
This posterior is proportional to the numerator of Bayes' theorem and given by
\begin{equation}
    \begin{split}
        \Prob{\mat{\theta} \given \mat{X}, \mat{y}}
        &\propto \Prob*{\mat{\theta}} \Prob{\mat{y} \given \mat{X}, \mat{\theta}} \\
        &= \int \Prob*{\mat{\theta}}\Prob{\mat{y} \given \mat{f}, \mat{\theta}} \Prob{\mat{f} \given \mat{X}, \mat{\theta}} \diff \mat{f}.
    \end{split}
\end{equation}
If $\Prob*{\mat{\theta}}$ is set to a flat distribution, the prior term vanishes and only the likelihood term remains.
Choosing hyper-parameters by maximizing the likelihood term is called a type II maximum likelihood estimate.

The marginal likelihood is the integral of the product of Gaussians in~\cref{eq:gp:gp_marginal_likelihood} given by
\begin{equation}
    \begin{split}
        \Prob{\mat{y} \given \mat{X}, \mat{\theta}}
        &= \Gaussian{\mat{y} \given \mat{0}, \mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye}.
    \end{split}
\end{equation}
Instead of maximizing the marginal likelihood directly, it is numerically convenient to minimize the negative logarithm of the likelihood
\begin{equation}
    \begin{split}
        \Lc(\mat{\theta}) &= -\log\Prob{\mat{y} \given \mat{X}, \mat{\theta}} \\
        &=
        \frac{1}{2} \mat{y}\tran \left( \mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye \right)\inv \mat{y} +
        \frac{1}{2} \log \abs*{\mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye} +
        \frac{N}{2} \log(2\pi).
    \end{split}
\end{equation}
Since the logarithm is a monotonous function it does not change the position of optima.
The maximum likelihood estimate is the solution of the optimization problem
\begin{align}
    \mat{\theta}^\ast & \in \argmin_{\mat{\theta}} \Lc(\mat{\theta})
\end{align}
and is calculated using standard approaches to non-convex optimization.
The computational complexity of evaluating the likelihood term and its derivatives is dominated by the inversion of $\mat{K}_{\mat{f}\mat{f}} + \sigma^2 \Eye$ with a time complexity of $\Oh(N^3)$.

\section{Sparse Gaussian processes with inducing points}
A drawback of Gaussian processes in real-world applications is their high computational cost for large data sets.
Assume a data set $(\mat{X}, \mat{y})$ with $N$ training samples, then the operations on a posterior Gaussian process are usually dominated by the inversion of the kernel matrix $\mat{K}_{\mat{f}\mat{f}}$ which takes $\Oh(N^3)$ time.
While this inversion can be pre-computed, the cost of predicting the mean and variance of one test point remains $\Oh(N)$ and $\Oh(N^2)$, respectively.
Additionally, these operations have a space requirement of $\Oh(N^2)$.
The goal of sparse approximations of Gaussian processes is to find model representations that avoid the cubic complexities or at least restrict them to the training phase of finding hyper-parameters.
This section introduces one type of approximation based on representing the complete data set through a smaller set of points~\parencite{snelson_flexible_2007}.
The next section will place this approximation in a principled variational context~\parencite{titsias_variational_2009,hensman_gaussian_2013}.

The most direct approach to reducing the computational cost of inverting $\mat{K}_{\mat{f}\mat{f}}$ is to restrict observations to a small subset of size $M \ll N$ of the original data.
Calculating the posterior GP relative to these $M$ observations only has a time complexity of $\Oh(M^3)$.
This approach can work for data sets with a very high level of redundancy but does impose the problem of choosing an appropriate subset.
While choosing a random subset can be effective \cite{snelson_flexible_2007}, the optimal choice is dependent on the hyper-parameters.
Both the subset and the hyper-parameters should, therefore, be chosen in a joint optimization scheme.
The selection of an appropriate subset defines a combinatorial optimization problem and is very hard to solve.

To overcome this problem, inducing observation approximations lift the restriction of choosing points from the data set and instead allow arbitrary positions in the input space.
The original data set is replaced by an inducing data set $(\mat{Z}, \mat{u})$ of inducing inputs $\mat{Z}$ and inducing variables $\mat{u} = f(\mat{Z})$ which are equal to the true latent values of the function $f \sim \GP(\mat{0}, \K)$.
Since they are not true observations, they are assumed to be noise-free.
Given an inducing data set and hyper-parameters $\mat{\theta}$, the predictive posterior of this approximation is a standard GP posterior
\begin{align}
    \Prob{\mat{f}_\ast \given \mat{X}_\ast, \mat{Z}, \mat{u}, \mat{\theta}}
     & = \Gaussian{
    \mat{K}_{\ast \mat{u}}\mat{K}_{\mat{u}\mat{u}}\inv \mat{u},
    \mat{K}_{\ast\ast} - \mat{K}_{\ast \mat{u}} \mat{K}_{\mat{u}\mat{u}}\inv \mat{K}_{\mat{u}\ast}
    }
\end{align}
with $\mat{K}_{\mat{u}\mat{u}} = \K(\mat{Z}, \mat{Z})$ denoting the Gram matrix of the inducing inputs.

The true data set is independent given the latent function $f$ and can be assumed independent given the inducing data set if it represents $f$ well.
The likelihood of the original data under the Gaussian process trained on the inducing data set is given by
\begin{equation}
    \begin{split}
        \Prob{\mat{y} \given \mat{X}, \mat{Z}, \mat{u}, \mat{\theta}}
        &= \prod_{n=1}^N \Prob{\mat{y}_n \given \mat{x}_n, \mat{Z}, \mat{u}, \mat{\theta}} \\
        &= \prod_{n=1}^N \Gaussian*{
        \mat{y}_n \given \mat{K_{\mat{f}_n\mat{u}}}\mat{K}_{\mat{u}\mat{u}}\inv \mat{u},
        \mat{K}_{\mat{f}_n \mat{f}_n} - \mat{K}_{\mat{f}_n \mat{u}} \mat{K}_{\mat{u}\mat{u}}\inv\mat{K}_{\mat{u} \mat{f}_n} + \sigma^2
        } \\
        &= \Gaussian*{
        \mat{y} \given \mat{K_{\mat{f}\mat{u}}}\mat{K}_{\mat{u}\mat{u}}\inv \mat{u},
        \Fun{\diag}{\mat{K}_{\mat{f}\mat{f}} - \mat{K}_{\mat{f} \mat{u}} \mat{K}_{\mat{u}\mat{u}}\inv\mat{K}_{\mat{u} \mat{f}}} + \sigma^2\Eye
        } \\
        &= \Gaussian*{
        \mat{y} \given \mat{K_{\mat{f}\mat{u}}}\mat{K}_{\mat{u}\mat{u}}\inv \mat{u},
        \Fun{\diag}{\mat{K}_{\mat{f}\mat{f}} - \mat{Q}_{\mat{f}\mat{f}}} + \sigma^2\Eye
        }
    \end{split}
\end{equation}
with $\mat{Q}_{\mat{f}\mat{f}} \coloneqq \mat{K}_{\mat{f} \mat{u}} \mat{K}_{\mat{u}\mat{u}}\inv\mat{K}_{\mat{u} \mat{f}}$.

Since the inducing variables $\mat{u}$ are latent function values, the original GP prior for $f$ is a reasonable prior for their values given by
\begin{align}
    \Prob*{\mat{u} \given \mat{Z}} = \Gaussian{\mat{u} \given \mat{0}, \mat{K}_{\mat{u}\mat{u}}}.
\end{align}
Using this prior, the inducing variables can be marginalized through an integral of a product of Gaussians in
\begin{equation}
    \begin{split}
        \Prob*{\mat{y} \given \mat{X}, \mat{Z}}
        &= \int \Prob*{\mat{y} \given \mat{X}, \mat{Z}, \mat{u}} \Prob*{\mat{u} \given \mat{Z}} \diff \mat{u} \\
        &= \int \Prob*{\mat{y} \given \mat{X}, \mat{Z}, \mat{u}} \Gaussian{\mat{u} \given \mat{0}, \mat{K}_{\mat{u}\mat{u}}} \diff \mat{u} \\
        &= \Gaussian{\mat{y} \given \mat{0}, \mat{Q}_{\mat{f}\mat{f}} + \Fun{\diag}{\mat{K}_{\mat{f}\mat{f}} - \mat{Q}_{\mat{f}\mat{f}}}},
    \end{split}
\end{equation}
dropping the conditioning on $\mat{\theta}$ for notational simplicity.
Due to the conditional independence assumption of the data given the inducing variables, this formulation is called the fully independent training conditional (FITC) and was introduced by \citeauthor{snelson_sparse_2005}~\parencite{snelson_sparse_2005,snelson_flexible_2007}.

\begin{figure}[tp]
    \begin{subfigure}{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/spgp_full}
        \caption{
            \label{fig:sparse_gp:spgp_example:gp}
            Full GP
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/spgp_sparse}
        \caption{
            \label{fig:sparse_gp:spgp_example:spgp}
            FITC approximation
        }
    \end{subfigure}
    \caption[FITC sparse GP approximation]{
        \label{fig:sparse_gp:spgp_example}
        A FITC GP approximation compared to a full GP posterior with data from a noisy sine function.
        The inducing inputs are located at the dart positions.
        Since the inducing outputs are marginalized, only their $x$-coordinate is meaningful.
        Three inducing inputs are enough to approximate the full GP with reasonable accuracy.
    }
\end{figure}
This approximate marginal likelihood can be interpreted as the marginal likelihood of a specific GP defined on the original data set $(\mat{X}, \mat{y})$.
In this GP, the original kernel $\K$ is replaced by the kernel $\K_{\text{FITC}}$.
With $\Ind$ denoting the indicator function, it is defined as
\begin{equation}
    \begin{split}
        \Qc(\mat{a}, \mat{b}) &\coloneqq \mat{K}_{\mat{a}\mat{u}} \mat{K}_{\mat{u}\mat{u}}\inv\mat{K}_{\mat{u}\mat{b}} \\
        \K_{\text{FITC}}(\mat{a}, \mat{b}) &\coloneqq \Qc(\mat{a}, \mat{b}) + \Indicator{\mat{a} = \mat{b}} \left( \K(\mat{a}, \mat{b}) - \Qc(\mat{a}, \mat{b}) \right).
    \end{split}
\end{equation}
This kernel is equal to $\K$ when both arguments are identical and equal to $\Qc$ everywhere else.
For well-chosen inducing inputs, $\mat{Q}_{\mat{f}\mat{f}}$ is a low-rank approximation of $\mat{K}_{\mat{f}\mat{f}}$~\parencite{snelson_flexible_2007}.
The inducing inputs $\mat{Z}$ are hidden in the kernel matrix $\mat{K}_{\mat{u}\mat{u}}$ and are additional hyper-parameters to this kernel.
The predictive posterior $\mat{f}_\ast$ of the test points $\mat{X}_\ast$ is then given by
\begin{equation}
    \begin{split}
        \Prob{\mat{f}_\ast \given \mat{X}_\ast, \mat{X}, \mat{y}, \mat{Z}}
        &= \Gaussian*{\mat{f}_\ast \given \mat{\mu}_\ast, \mat{\Sigma}_\ast} \text{, where} \\
        \mat{\mu}_\ast
        &= \mat{Q}_{\ast \mat{f}} \left( \mat{Q}_{\mat{f}\mat{f}} + \diag(\mat{K}_{\mat{f}\mat{f}} - \mat{Q}_{\mat{f}\mat{f}}) + \sigma^2 \Eye \right)\inv \mat{y} \\
        \mat{\Sigma}_\ast
        &= \mat{K}_{\ast\ast} - \mat{Q}_{\ast \mat{f}} \left( \mat{Q}_{\mat{f}\mat{f}} + \diag(\mat{K}_{\mat{f}\mat{f}} - \mat{Q}_{\mat{f}\mat{f}}) + \sigma^2 \Eye \right)\inv \mat{Q}_{\mat{f} \ast}.
    \end{split}
\end{equation}
and $\mat{Q}_{\mat{f}\mat{f}} \coloneqq \mat{K}_{\mat{f} \mat{u}} \mat{K}_{\mat{u}\mat{u}}\inv\mat{K}_{\mat{u} \mat{f}}$ obtained by inserting the kernel definition into~\cref{lem:gp:gp_posterior}.
This formulation of the predictive posterior for the FITC approximation still requires the inversion of matrices of size $N \times N$ and therefore does not offer computational improvements.
Using the matrix inversion lemma \cite{petersen_matrix_2008}, they can be rewritten in the form
\begin{equation}
    \begin{split}
        \mat{\mu}_\ast
        &= \mat{K}_{\ast \mat{u}}\mat{B}\inv \mat{K}_{\mat{u}\mat{f}}\left(\Fun{\diag}{\mat{K}_{\mat{f}\mat{f}} - \mat{Q}_{\mat{f}\mat{f}}} + \sigma^2\Eye\right)\inv \mat{y} \\
        \mat{\Sigma}_\ast
        &= \mat{K}_{\ast\ast} - \mat{K}_{\ast \mat{u}} \left(\mat{K}_{\mat{u}\mat{u}}\inv - \mat{B}\inv \right) \mat{K}_{\mat{u}\ast} \\
        \mat{B}
        &= \mat{K}_{\mat{u}\mat{u}} + \mat{K}_{\mat{u}\mat{f}} \left(\Fun{\diag}{\mat{K}_{\mat{f}\mat{f}} - \mat{Q}_{\mat{f}\mat{f}}} + \sigma^2\Eye\right)\inv \mat{K}_{\mat{f}\mat{u}},
    \end{split}
\end{equation}
which only involves the inversion of $M \times M$ matrices and one diagonal $N \times N$ matrix.
Implemented this way, the calculation of all terms independent of the test points has a complexity of $\Oh(NM^2)$ and predicting individual means and variances takes $\Oh(M)$ and $\Oh(M^2)$ time respectively.
The space requirement also drops to $\Oh(M^2)$.
Since the positions of the inducing inputs $\mat{Z}$ are additional hyper-parameters in $\K_{\text{FITC}}$, they can be chosen together with the hyper-parameters of the original kernel $\mat{\theta}$ using maximum likelihood.
This optimization chooses the positions in such a way that together with appropriate other hyper-parameters, the original data is represented as well as possible.
\Cref{fig:sparse_gp:spgp_example} shows that a surprisingly small number of inducing inputs can be enough to represent the dynamics of a function.

However, with a large number of inducing inputs, the number of hyper-parameters can grow large as well.
A large number of hyperparameters implies a danger of overfitting since the altered Gaussian process has no direct connection to the original Gaussian process over the complete training set.
It is also not clear what properties a set of training inputs $\mat{Z}$ must fulfill such that it recovers an original GP well.
To address these issues, a variational formulation of sparse GPs using inducing observations formulated originally in~\parencite{titsias_variational_2009,hensman_gaussian_2013} is discussed next.

\section{Variational sparse Gaussian process approximations}
\label{toc:gp:sparse_gps}
\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \begin{tikzpicture}[x=4em, y=5em]
            \node[random variable, observed] at (0, 0) (X) {$\mat{x}_n$};
            \node[random variable, latent] at (0, -1) (F) {$\mat{f}_n$};
            \node[random variable, hyperparameter] at (-1, -1) (theta) {$\mat{\theta}$};
            \node[random variable, variational] at (1, -0.75) (Z) {$\mat{Z}$};
            \node[random variable, variational] at (1, -1.25) (u) {$\mat{u}^\ast$};
            \node[random variable, observed] at (0, -2) (Y) {$\mat{y}_n$};
            \draw[edge, directed] (X) -- (F);
            \draw[edge, directed] (F) -- (Y);
            \draw[edge, directed] (theta) -- (F);
            \draw[edge, directed] (Z) -- (F);
            \draw[edge, directed] (u) to (F);
            \draw[edge, directed] (Y) to (u);
            %
            \begin{scope}[on background layer]
                \node[
                    align plate,
                    inner xsep=10pt,
                    fit=(X)(F)(Y),
                    label={[anchor=south east]south east:$N$}
                ] {};
            \end{scope}
        \end{tikzpicture}
        \medskip
        \caption{
            SGPR
            \label{fig:gp:sparse_graphical_model:sgpr}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \begin{tikzpicture}[x=4em, y=5em]
            \node[random variable, observed] at (0, 0) (X) {$\mat{x}_n$};
            \node[random variable, latent] at (0, -1) (F) {$\mat{f}_n$};
            \node[random variable, hyperparameter] at (-1, -1) (theta) {$\mat{\theta}$};
            \node[random variable, variational] at (1, -0.75) (Z) {$\mat{Z}$};
            \node[random variable, variational] at (1, -1.25) (u) {$\mat{u}$};
            \node[random variable, observed] at (0, -2) (Y) {$\mat{y}_n$};
            \draw[edge, directed] (X) -- (F);
            \draw[edge, directed] (F) -- (Y);
            \draw[edge, directed] (theta) -- (F);
            \draw[edge, directed] (Z) -- (F);
            \draw[edge, directed] (u) -- (F);
            %
            \begin{scope}[on background layer]
                \node[
                    align plate,
                    inner xsep=10pt,
                    fit=(X)(F)(Y),
                    label={[anchor=south east]south east:$N$}
                ] {};
            \end{scope}
        \end{tikzpicture}
        \medskip
        \caption{
            SVGP
            \label{fig:gp:sparse_graphical_model:svgp}
        }
    \end{subfigure}
    \caption[Graphical model: Variational sparse GP]{
        \label{fig:gp:sparse_graphical_model}
        Graphical models of variational sparse GP approximations with inducing inputs.
        The latent function values $\mat{f}_n$ are assumed to be independent given the variational parameters $\mat{Z}$, $\mat{u}$ and hyper-parameters $\mat{\theta}$.
        Since the observations $\mat{y}_n$ inform the optimal $\q^\ast(\mat{u})$ distribution in the SGPR approximations, they are not conditionally independent in the SGPR approximation.
        In the SVGP approximation, the parameters for $\q(\mat{u})$ are part of the variational bound, which implies conditional independence between observations and enables stochastic optimization techniques.
    }
\end{figure}
\Textcite{titsias_variational_2009} introduced a variational interpretation of sparse GPs with inducing observations $(\mat{Z}, \mat{u})$.
Similar to the FITC approximation discussed above, inducing observations are assumed to be generated from the latent function $\mat{u} = f(\mat{Z})$.
Due to the consistency of GPs, the true data and inducing data are jointly Gaussian.
\begin{align}
    \label{eq:gp:augmented_model}
    \Prob{\mat{f}, \mat{u}} & = \Gaussian*{\begin{pmatrix} \mat{f} \\ \mat{u}\end{pmatrix} \given \mat{0}, \begin{pmatrix} \mat{K_{ff}} & \mat{K_{fu}} \\ \mat{K_{uf}} & \mat{K_{uu}} \end{pmatrix}}
\end{align}
Instead of defining a new GP on the inducing data, we want to choose the $M$ inducing locations such that the original GP defined on the $N$ true data points is approximated as closely as possible.

More formally, we consider the predictive posterior of the augmented GP containing both the true and inducing data given by
\begin{align}
    \Prob*{\mat{f^\ast} \given \mat{y}}
     & = \int \Prob*{\mat{f^\ast} \given \mat{f}, \mat{u}} \Prob*{\mat{f}, \mat{u} \given \mat{y}} \diff \mat{f} \diff \mat{u},
\end{align}
where we drop the conditioning on $\mat{X}$ and $\mat{Z}$ for notational simplicity.
We adopt this convention for the rest of this chapter.
The inducing observations are optimal if $\mat{f^\ast}$ and $\mat{f}$ are independent given $\mat{u}$ in
\begin{equation}
    \begin{split}
        \Prob*{\mat{f^\ast} \given \mat{f}, \mat{u}}
        & = \Prob*{\mat{f^\ast} \given \mat{u}}                           \\
        \Prob*{\mat{f}, \mat{u} \given \mat{y}}
        & = \Prob*{\mat{f} \given \mat{u}} \Prob*{\mat{u} \given \mat{y}}.
    \end{split}
\end{equation}
In this case, $\mat{u}$ is said to be a sufficient statistic for $\mat{f}$, capturing all information contained in the latter.
In practice, it is hard to find $(\mat{Z}, \mat{u})$ that are indeed a sufficient statistic for $\mat{f}$.
We will approximate this situation with a variational distribution $\Variat*{\mat{f}, \mat{u}}$, thereby formulating a variational approximation to the original GP.
Due to the joint Gaussian distribution in~\cref{eq:gp:augmented_model}, it is convenient to consider the factorization
\begin{align}
    \Variat*{\mat{f}, \mat{u}}
     & = \Prob*{\mat{f} \given \mat{u}} \Variat*{\mat{u}},
\end{align}
where $\Prob*{\mat{f} \given \mat{u}}$ is a standard Gaussian conditional.
Assuming $\mat{u}$ is indeed a sufficient statistic for $\mat{f}$, the variational predictive posterior reduces to
\begin{equation}
    \begin{split}
        \label{eq:gp:sgpr_predictive_posterior}
        \Variat*{\mat{f^\ast}}
        & = \int \Prob*{\mat{f^\ast} \given \mat{f}, \mat{u}} \Variat*{\mat{f}, \mat{u}} \diff \mat{f} \diff \mat{u}              \\
        & = \int \Prob*{\mat{f^\ast} \given \mat{u}} \Prob*{\mat{f} \given \mat{u}} \Variat*{\mat{u}} \diff \mat{f} \diff \mat{u} \\
        & = \int \Prob*{\mat{f^\ast} \given \mat{u}} \Variat*{\mat{u}} \diff \mat{u}.
    \end{split}
\end{equation}
To formulate a variational lower bound on the original marginal likelihood $\Lc^{\text{GP}}$ in~\cref{eq:gp:gp_marginal_likelihood}, we have to decide how to choose $\mat{Z}$ and $\Variat*{\mat{u}}$ such that as much information of $\mat{f}$ is captured as possible.
We will discuss two approaches.
First, we derive the optimal choice of $\Variat*{\mat{u}}$ given a set of inducing inputs $\mat{Z}$.
Because calculating this optimal choice is computationally expensive, we will then show how to improve performance through further approximation.

\subsubsection{Optimal inducing outputs}
We assume a free-form variational distribution $\Variat*{\mat{u}}$ to derive the variational lower bound to the likelihood $\Lc^{\text{SGPR}}$ of the augmented model.
\begin{equation}
    \begin{split}
        \Lc^{\text{SGPR}}(\mat{\theta}, \mat{Z}, \Variat*{\mat{u}})
        & = -\log \Prob*{\mat{y} \given \mat{\theta}, \mat{Z}, \Variat*{\mat{u}}}                                                                                                                                                       \\
        & = -\log \int \Prob*{\mat{y} \given \mat{f}} \Prob*{\mat{f} \given \mat{u}} \Prob*{\mat{u}} \diff \mat{f} \diff \mat{u}                                                                                                        \\
        & = -\log \int \Variat*{\mat{f}, \mat{u}} \frac{\Prob*{\mat{y} \given \mat{f}} \Prob*{\mat{f} \given \mat{u}} \Prob*{\mat{u}}}{\Variat*{\mat{f}, \mat{u}}} \diff \mat{f} \diff \mat{u}.
    \end{split}
\end{equation}
We bound the likelihood using Jensen's inequality~\parencite{bishop_pattern_2007} which states that for convex functions $f$ and integrable functions $g$ it holds that
\begin{equation}
    \begin{split}
        \label{eq:jensens_inequality}
        \Fun*{f\,}{\int g(x) \diff x} \leq \int f(g(x)) \diff x.
    \end{split}
\end{equation}
Since the natural logarithm is concave we have
\begin{equation}
    \begin{split}
        \label{eq:gp:sgpr_bound_1}
        \Lc^{\text{SGPR}}(\mat{\theta}, \mat{Z}, \Variat*{\mat{u}})
        & \geq -\int \Variat*{\mat{f}, \mat{u}} \log\frac{\Prob*{\mat{y} \given \mat{f}} \Prob*{\mat{f} \given \mat{u}} \Prob*{\mat{u}}}{\Variat*{\mat{f}, \mat{u}}} \diff \mat{f} \diff \mat{u}                                        \\
        & = -\int \Prob*{\mat{f} \given \mat{u}}\Variat*{\mat{u}} \log\frac{\Prob*{\mat{y} \given \mat{f}} \Prob*{\mat{f} \given \mat{u}} \Prob*{\mat{u}}}{\Prob*{\mat{f} \given \mat{u}}\Variat*{\mat{u}}} \diff \mat{f} \diff \mat{u} \\
        & = -\int \Prob*{\mat{f} \given \mat{u}}\Variat*{\mat{u}} \log\frac{\Prob*{\mat{y} \given \mat{f}} \Prob*{\mat{u}}}{\Variat*{\mat{u}}} \diff \mat{f} \diff \mat{u}                                                              \\
        & = -\int \Variat*{\mat{u}} \left( \int\Prob*{\mat{f} \given \mat{u}} \log\Prob*{\mat{y} \given \mat{f}} \diff \mat{f} + \log\frac{\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \right) \diff \mat{u} \\
        & = -\int \Variat*{\mat{u}} \left( \Moment*{\E_{\Prob*{\mat{f} \given \mat{u}}}}{\log \Prob*{\mat{y} \given \mat{f}}} + \log\frac{\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \right) \diff \mat{u} \\
        & = \Moment*{\E_{\Variat{\mat{f}}}}{\log \Prob*{\mat{y} \given \mat{f}}} - \KL*{\Variat*{\mat{u}}}{\Prob*{\mat{u}}},
    \end{split}
\end{equation}
dropping the conditioning on $\mat{\theta}$ and $\mat{Z}$.
We assume a Gaussian likelihood $\Prob*{\mat{y} \given \mat{f}} = \Gaussian{\mat{y} \given \mat{f}, \sigma_n^2\Eye}$.
The expectation of the log-likelihood can be evaluated analytically~\parencite{petersen_matrix_2008} and is given by
\begin{equation}
    \begin{split}
        \label{eq:gp:expected_log_likelihood}
        \Moment*{\E_{\Prob*{\mat{f} \given \mat{u}}}}{\log \Prob*{\mat{y} \given \mat{f}}}
        &= \int \Prob*{\mat{f} \given \mat{u}} \log\Prob*{\mat{y} \given \mat{f}} \diff \mat{f} \\
        &= \int \log \Gaussian*{\mat{y} \given \mat{f}, \sigma_n^2 \Eye} \Gaussian*{\mat{f} \given \mat{K_{fu}} \mat{K_{uu}}\inv \mat{u}, \mat{K_{ff}} - \mat{Q_{ff}}} \diff \mat{f} \\
        &= \log \Gaussian*{\mat{y} \given \mat{K_{fu}} \mat{K_{uu}}\inv \mat{u}, \sigma_n^2\Eye} - \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &= \log G(\mat{u}) - \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}},
    \end{split}
\end{equation}
where we set $G(\mat{u}) = \Gaussian*{\mat{y} \given \mat{K_{fu}} \mat{K_{uu}}\inv \mat{u}, \sigma_n^2\Eye}$, $\mat{Q_{ff}} = \mat{K_{fu}} \mat{K_{uu}}\inv \mat{K_{uf}}$ and where $\tr(\mat{M})$ denotes the trace of the matrix $\mat{M}$.
Inserting~\cref{eq:gp:expected_log_likelihood} into~\cref{eq:gp:sgpr_bound_1} yields
\begin{equation}
    \begin{split}
        \label{eq:gp:sgpr_bound_2}
        \Lc^{\text{SGPR}}(\mat{\theta}, \mat{Z}, \Variat*{\mat{u}})
        &\geq -\int \Variat*{\mat{u}} \left( \Moment*{\E_{\Prob*{\mat{f} \given \mat{u}}}}{\log \Prob*{\mat{y} \given \mat{f}}} + \log\frac{\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \right) \diff \mat{u} \\
        &= -\int \Variat*{\mat{u}} \left( \log G(\mat{u}) - \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} + \log\frac{\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \right) \diff \mat{u} \\
        &= -\int \Variat*{\mat{u}} \log\frac{G(\mat{u})\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \diff \mat{u} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}}.
    \end{split}
\end{equation}

The distribution $\Variat*{\mat{u}}$ is chosen optimally if it maximizes the variational bound $\Lc^{\text{SGPR}}$ in
\begin{equation}
    \begin{split}
        \MoveEqLeft\argmax_{\Variat*{\mat{u}}}\Lc^{\text{SGPR}}(\mat{\theta}, \mat{Z}, \Variat*{\mat{u}})                                                                                             \\
        & = \argmax_{\Variat*{\mat{u}}} -\int \Variat*{\mat{u}} \log\frac{G(\mat{u})\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \diff \mat{u} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        & = \argmax_{\Variat*{\mat{u}}} -\int \Variat*{\mat{u}} \log\frac{G(\mat{u})\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \diff \mat{u}                                                                \\
        & = \argmin_{\Variat*{\mat{u}}} \KL*{\Variat*{\mat{u}}}{G(\mat{u})\Prob*{\mat{u}}},
    \end{split}
\end{equation}
where $\KL{\q}{\p}$ denotes the Kullback-Leibler divergence.
This divergence is minimized if $\q$ and $\p$ are proportional and $G(\mat{u})\Prob*{\mat{u}}$ is a product of Gaussians
\begin{align}
    \q^\ast(\mat{u}) \propto G(\mat{u}) \Prob*{\mat{u}}
     & = \Gaussian*{\mat{y} \given \mat{K_{fu}} \mat{K_{uu}}\inv \mat{u}, \sigma_n^2\Eye}\Gaussian*{\mat{u} \given \mat{0}, \mat{K_{uu}}},
\end{align}
where $\Prob*{\mat{u}}$ is due to the GP prior in~\cref{eq:gp:augmented_model}.
Since the product of two Gaussian densities is an un-normalized Gaussian density, the optimal choice $\q^\ast(\mat{u})$ is a Gaussian given by
\begin{equation}
    \begin{split}
        \label{eq:gp:sgpr_optimal_q}
        \q^\ast(\mat{u})
        & = \frac{G(\mat{u}) \Prob*{\mat{u}}}{\int G(\mat{u}) \Prob*{\mat{u}} \diff \mat{u}}
        = \Gaussian*{\mat{u} \given \mat{\mu}_u, \mat{\Sigma}_u}\text{, with}                 \\[\smallskipamount]
        \mat{\mu}_u
        & = \sigma_n^{-2} \mat{K_{uu}}\mat{B}\inv \mat{K_{uf}}\mat{y}                        \\
        \mat{\Sigma}_u
        & = \mat{K_{uu}}\mat{B}\inv\mat{K_{uu}}                                              \\[\smallskipamount]
        \mat{B}
        & = \mat{K_{uu}} + \sigma_n^{-2} \mat{K_{uf}}\mat{K_{fu}}.
    \end{split}
\end{equation}
Inserting $\q^\ast$ into~\cref{eq:gp:sgpr_bound_2} yields the final bound
\begin{equation}
    \begin{split}
        \label{eq:gp:sgpr_bound}
        \Lc^{\text{SGPR}}(\mat{\theta}, \mat{Z})
        &\geq \Lc^{\text{SGPR}}(\mat{\theta}, \mat{Z}, \q^\ast(\mat{u})) \\
        &= -\int \q^\ast(\mat{u}) \log\frac{G(\mat{u})\Prob*{\mat{u}}}{\q^\ast(\mat{u})} \diff \mat{u} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &= -\int \frac{G(\mat{u}) \Prob*{\mat{u}}}{\int G(\mat{u}) \Prob*{\mat{u}} \diff \mat{u}} \log\frac{G(\mat{u})\Prob*{\mat{u}}}{\frac{G(\mat{u}) \Prob*{\mat{u}}}{\int G(\mat{u}) \Prob*{\mat{u}} \diff \mat{u}}} \diff \mat{u} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &= -\log \int G(\mat{u}) \Prob*{\mat{u}} \diff \mat{u} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &= -\log \Gaussian*{\mat{y} \given \mat{0}, \mat{Q_{ff}} + \sigma_n^2\Eye} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}},
    \end{split}
\end{equation}
which can be maximized to jointly find hyper-parameters $\mat{\theta}$ and the variational parameters $\mat{Z}$.
Evaluating this bound takes $\Oh(NM^2)$ time.
In contrast to the FITC approximation, where inducing inputs are additional hyper-parameters to the model that can lead to overfitting, adding inducing inputs $\mat{Z}$ to the variational model only improves the approximation to the full GP.

\Cref{fig:gp:sparse_graphical_model:sgpr} shows the graphical model for the variational GP approximation using the optimal $\q^\ast(\mat{u})$ distribution.
The inducing inputs
\tikz[x=2.5em,baseline=(f.base)]{
    \node[inline random variable, variational] (f) {$\mat{Z}$};
}
are referred to as variational parameters.
While the SGPR model is more computationally efficient, the dependencies between different observations are not wholely removed.
The optimal choice for $\q^\ast(\mat{u})$ depends on the observations in~\cref{eq:gp:sgpr_optimal_q}.
Because of this, the bound does not factorize along the data, preventing stochastic variational inference techniques~\parencite{hensman_gaussian_2013} and requiring the bound to be evaluated jointly for all data points.
Additionally, evaluating the predictive posterior~\cref{eq:gp:sgpr_predictive_posterior} takes $\Oh(M^3 + MN)$ time.
For large $N$, linear growth with the amount of observations can be prohibitive.
We will now consider the SVGP model introduced by~\textcite{hensman_gaussian_2013} that avoids this growth.

\subsubsection{Approximate inducing outputs}
\begin{figure}[p]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/variational_gp_spgp}
        \caption{
            \label{fig:gp:variational:spgp}
            FITC with $M = 10$
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/variational_gp_spgp_huge}
        \caption{
            \label{fig:gp:variational:spgp_huge}
            FITC with $M = 300$
        }
    \end{subfigure}\\[\figureskip]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/variational_gp_sgpr}
        \caption{
            \label{fig:gp:variational:sgpr}
            SGPR with $M = 10$
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/variational_gp_sgpr_huge}
        \caption{
            \label{fig:gp:variational:sgpr_huge}
            SGPR with $M = 300$
        }
    \end{subfigure}\\[\figureskip]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/variational_gp_svgp}
        \caption{
            \label{fig:gp:variational:svgp}
            SVGP with $M = 10$
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/variational_gp_svgp_huge}
        \caption{
            \label{fig:gp:variational:svgp_huge}
            SVGP with $M = 300$
        }
    \end{subfigure}
    \caption[Variational GP posteriors]{
        \label{fig:gp:variational}
        A comparison of sparse GP approximations with inducing inputs.
        While the FITC approximation does not converge to the full GP for large $M$, both variational approaches do.
        For SGPR and FITC, the inducing outputs are marginalized and only the $x$-coordinates are meaningful.
        For SVGP, inducing outputs are optimized as well.
    }
\end{figure}
In the SGPR approximation, a linear dependency on the number of points in the true data set is introduced through $\q^\ast(\mat{u})$.
We have shown in~\cref{eq:gp:sgpr_optimal_q} that this optimal choice is a Gaussian.
Instead of using $\q^\ast(\mat{u})$, the SVGP model shown in~\cref{fig:gp:sparse_graphical_model:svgp} uses a free-form Gaussian $\Variat*{\mat{u}} = \Gaussian*{\mat{u} \given \mat{m}, \mat{S}}$.
The $\Oh(M^2)$ variational parameters in $\mat{m}$ and $\mat{S}$ are optimized jointly with $\mat{Z}$ in a variational bound.
Poor choices of $\mat{m}$ and $\mat{S}$ can only worsen the variational approximation compared to the optimal choice in $\Lc^{\text{SGPR}}$ but does not alter the model.

To derive the variational bound $\Lc^{\text{SVGP}}$, we start with the bound in~\cref{eq:gp:sgpr_bound_2}.
Instead of inserting the optimal $\q^\ast(\mat{u})$, we reformulate the bound to recover an expectation and KL-divergence in
\begin{equation}
    \begin{split}
        \label{eq:gp:svgp_bound_1}
        \MoveEqLeft\Lc^{\text{SVGP}}(\mat{\theta}, \mat{Z}, \mat{m}, \mat{S}) \\
        &\geq -\int \Variat*{\mat{u}} \log\frac{G(\mat{u})\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \diff \mat{u} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &= -\int \Variat*{\mat{u}} \log\frac{G(\mat{u})\Prob*{\mat{u}}}{\Variat*{\mat{u}}} \diff \mat{u} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &= -\int \log G(\mat{u}) \Variat*{\mat{u}} \diff \mat{u} + \KL{\Variat*{\mat{u}}}{\Prob*{\mat{u}}} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &= -\Moment*{\E_{\Variat*{\mat{u}}}}{\log G(\mat{u})} + \KL{\Variat*{\mat{u}}}{\Prob*{\mat{u}}} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}}.
    \end{split}
\end{equation}
Similar to~\cref{eq:gp:expected_log_likelihood}, this expectation can be evaluated analytically and is given by
\begin{equation}
    \begin{split}
        \label{eq:gp:expected_log_likelihood_svgp}
        \Moment*{\E_{\Variat*{\mat{u}}}}{\log G(\mat{u})}
        &= \int \log G(\mat{u}) \Variat*{\mat{u}} \diff \mat{u} \\
        &= \int \log \Gaussian*{\mat{y} \given \mat{K_{fu}} \mat{K_{uu}}\inv \mat{u}, \sigma_n^2\Eye} \Gaussian*{\mat{u} \given \mat{m}, \mat{S}} \diff \mat{u} \\
        &= \log \Gaussian*{\mat{y} \given \mat{K_{fu}}\mat{K_{uu}}\inv \mat{m}, \sigma_n^2 \Eye} - \frac{1}{2\sigma_n^2} \Fun*{\tr}{\mat{K_{fu}}\mat{K_{uu}}\inv \mat{S}\mat{K_{uu}}\inv \mat{K_{uf}}}.
    \end{split}
\end{equation}
Inserting~\cref{eq:gp:expected_log_likelihood_svgp} into~\cref{eq:gp:svgp_bound_1} yields the final bound for the SVGP model given by
\begin{equation}
    \begin{split}
        \label{eq:gp:svgp_bound_2}
        \MoveEqLeft\Lc^{\text{SVGP}}(\mat{\theta}, \mat{Z}, \mat{m}, \mat{S}) \\
        &\geq -\Moment*{\E_{\Variat*{\mat{u}}}}{\log G(\mat{u})} + \KL{\Variat*{\mat{u}}}{\Prob*{\mat{u}}} + \frac{1}{2\sigma_n^2}\Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}} \\
        &=
        -\log \Gaussian*{\mat{y} \given \mat{K_{fu}}\mat{K_{uu}}\inv \mat{m}, \sigma_n^2 \Eye}
        + \KL{\Variat*{\mat{u}}}{\Prob*{\mat{u}}} \\
        &\quad + \frac{1}{2\sigma_n^2} \Fun*{\tr}{\mat{K_{ff}} - \mat{Q_{ff}}}
        + \frac{1}{2\sigma_n^2} \Fun*{\tr}{\mat{K_{fu}}\mat{K_{uu}}\inv \mat{S}\mat{K_{uu}}\inv \mat{K_{uf}}}.
    \end{split}
\end{equation}
The KL-divergence is a divergence of two Gaussians which can be evaluated analytically.
While evaluating the complete bound still takes $\Oh(NM^2)$, the data-likelihood term is given by a diagonal Gaussian which factorizes along the data.
This enables stochastic optimization techniques like mini-batching, greatly increasing the scalability of the variational GP approximation to large data sets~\parencite{hensman_gaussian_2013}.
Similarly, evaluating the predictive posterior~\cref{eq:gp:sgpr_predictive_posterior} now takes $\Oh(M^3)$ time, removing the dependency on the training data completely.
\cref{fig:gp:variational} compares predictive posteriors for the FITC, SGPR and SVGP approximations.
While the approximations show similar results with small amounts of inducing inputs, both SGPR and SVGP converge to the original GP for large $M$ while the FITC approximation does not.
Because the SGPR approximation does not explicitly represent inducing outputs, only the input locations are meaningful, similar to the FITC approximation.
SVGP directly infers a posterior about latent function values, which are shown in the figure.

The $\Oh(M^3)$ computational cost due to the inversion of $\mat{K_{uu}}$ can still be prohibitive for models with a large number of inducing points.
To reduce the number of required points, recent work explored how Bayesian inference can be employed in the search for good inducing point locations~\parencite{hensman_mcmc_2015,rossi_rethinking_2020}.
There have also been multiple extensions to the variational inducing point approach to reduce the computational cost further.
One approach is to impose a grid structure on the inducing inputs $\mat{Z}$~\parencite{wilson_kernel_2015}.
Instead of optimizing their location, the position of a large number of inputs is fixed to perform fast computations exploiting the structure.
While this approach suffers from the curse of dimensionality, it can increase performance for low input dimensionalities.
Another approach is to orthogonally decouple the computations for predictive means and variances~\parencite{shi_sparse_2020,salimbeni_orthogonally_2018,cheng_variational_2017}.
This decoupling allows the calculation of the predictive mean in linear time, allowing for a larger number of inducing points for the mean.


\section{Hierarchical Gaussian processes}
\label{toc:dgp}
While GPs offer a principled non-parametric approach to representing distributions over functions, they can be restrictive in the functions they can represent.
Many extensions to standard GPs have been studied which introduce additional hierarchical structure such as GP latent variable models~\parencite{titsias_bayesian_2010,damianou_variational_2014}, warped GP models~\parencite{snelson_warped_2004,lazaro-gredilla_bayesian_2012} or mixtures of experts models~\parencite{tresp_mixtures_2001,rasmussen_infinite_2002,lazaro-gredilla_overlapping_2012}.
The hierarchical structure typically introduces the requirement to propagate uncertainties through Gaussian processes, which is analytically intractable in most cases.
Many extensions can be interpreted as special cases of the hierarchical or deep Gaussian process model~\parencite{lawrence_hierarchical_2007,damianou_deep_2013,damianou_deep_2015} in which a nested generative process is formulated by using the output of one GP as the input of another.

In a deep GP, rather than assuming that observational data is generated through a draw from a single GP which is then corrupted by noise in $\mat{y} = f(\mat{x}) + \epsilon$, we assume that data is generated via a composition of $L$ functions
\begin{equation}
    \begin{split}
        \mat{y} = f_L(f_{L-1}(\cdots(f_1(\mat{X})))) + \epsilon,
    \end{split}
\end{equation}
where each $f_l$ is drawn from a GP.
This new compositional prior $f_L \circ \cdots \circ f_1$ is no longer a Gaussian process~\parencite{duvenaud_avoiding_2014} and can represent a broader set of functions.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[x=4em, y=5em]
        \node[random variable, observed] at (0, 0) (X) {$\mat{x}_n$};
        %
        \node[random variable, latent] at (0, -1) (F1) {$\mat{f}_{n,1}$};
        \node[random variable, variational] at (1, -0.75) (Z1) {$\mat{Z}_1$};
        \node[random variable, variational] at (1, -1.25) (u1) {$\mat{u}_1$};
        %
        \node[random variable, latent] at (0, -2) (Fl) {$\mat{f}_{n,L}$};
        \node[random variable, variational] at (1, -1.75) (Zl) {$\mat{Z}_L$};
        \node[random variable, variational] at (1, -2.25) (ul) {$\mat{u}_L$};
        %
        \node[random variable, hyperparameter] at (-1, -1.5) (theta) {$\mat{\theta}$};
        \node[random variable, observed] at (0, -3) (Y) {$\mat{y}_n$};
        \draw[edge, directed] (X) -- (F1);
        %
        \draw[edge, directed] (theta) |- (F1);
        \draw[edge, directed] (u1) -- (F1);
        \draw[edge, directed] (Z1) -- (F1);
        %
        \draw[edge, directed, densely dashed] (F1) -- (Fl);
        %
        \draw[edge, directed] (theta) |- (Fl);
        \draw[edge, directed] (ul) -- (Fl);
        \draw[edge, directed] (Zl) -- (Fl);
        %
        \draw[edge, directed] (Fl) -- (Y);
        %
        \begin{scope}[on background layer]
            \node[
                align plate,
                inner xsep=10pt,
                fit=(X)(Y),
                label={[anchor=south east]south east:$N$}
            ] {};
        \end{scope}
    \end{tikzpicture}
    \caption[Graphical model: Deep GP]{
        \label{fig:dgp:graphical_model}
        Graphical model of a variational deep GP approximation with inducing inputs.
        A deep GP is a composition of $L$ functions where each $f_l$ is drawn from a GP.
        Similar to shallow variational GP approximations, each GP is augmented with inducing observations.
    }
\end{figure}
\Cref{fig:dgp:graphical_model} shows the graphical model of a deep GP.
We represent the result of applying the first $l$ functions $f_l \circ \cdots \circ f_1$ on the inputs $\mat{X}$ as $\mat{f}_l$ and identify $\mat{f}_0 = \mat{X}$.
The joint probability of the function values and the outputs can then be written as
\begin{equation}
    \begin{split}
        \label{eq:dgp:full_model}
        \Prob{\mat{y}, \mat{f}_1, \dots, \mat{f}_L \given \mat{X}}
        &= \Prob*{\mat{y} \given \mat{f}_L} \prod_{l=1}^L \Prob*{\mat{f}_l \given \mat{f}_{l-1}}\text{, with} \\
        \mat{y} \mid \mat{f}_L &\sim \Gaussian{\mat{f}_L, \sigma^2_n \Eye} \\
        \mat{f}_l \mid \mat{f}_{l-1} &\sim \Gaussian{\mat{0}, \mat{K}_{\mat{f}_l\mat{f}_l} + \sigma^2_l \Eye}.
    \end{split}
\end{equation}
The structure of the conditionals $\Prob*{\mat{f}_l \given \mat{f}_{l-1}}$ is implied by the assumption that the $f_l$ are drawn from GPs and~\cref{eq:gp:gp_marginal_likelihood}.
Note that following the original formulation in~\parencite{damianou_deep_2013}, we assume independent Gaussian noise $\sigma^2_l$ as part of every function $f_l$.
We will denote the kernel matrix at the $\nth{l}$ level as $\mat{K}_{ll} = \mat{K}_{\mat{f}_l\mat{f}_l}$.
The $\mat{f}_l$ are all latent variables, and inference over them is very challenging.
Because they capture the result of applying multiple GPs, they are highly dependent on each other, thereby implying dependence between the different functions as well.
We will now derive two extensions of the sparse variational approximations in~\cref{toc:gp:sparse_gps} to the hierarchical case, nested variational compression~\parencite{hensman_nested_2014} and doubly stochastic variational inference~\parencite{salimbeni_doubly_2017} to approximate the marginal likelihood of the deep Gaussian process model
\begin{equation}
    \begin{split}
        \label{eq:dgp:marginal_likelihood}
        \Prob*{\mat{y} \given \mat{X}} = \int \Prob*{\mat{y}, \mat{f}_1, \dots, \mat{f}_L \given \mat{X}} \diff \mat{f}_1 \dots \diff \mat{f}_L.
    \end{split}
\end{equation}


\subsubsection{Nested variational compression}
The challenge in deriving the marginal likelihood for deep GPs stems from the propagation of uncertainties through multiple GPs.
In~\cref{toc:gp:sparse_gps} and~\cref{fig:gp:sparse_graphical_model:svgp} we showed how to achieve conditional independence between data points in a single GP by augmenting the model with a set of inducing observations.
The SVGP model is a compressed representation in which a small number of inducing points represents the full GP defined on all observations.
In nested variational compression (NVC) presented by~\textcite{hensman_nested_2014}, this compression is applied recursively for all GPs in the deep GP hierarchy.
To derive a tractable variational lower bound, an additional step is necessary to avoid the inter-layer cross-dependencies.

The bound in \Cref{eq:gp:expected_log_likelihood_svgp} can be used directly to approximate $\Prob*{\mat{f}_1 \given \mat{X}}$ as the innermost function in a deep GP is a standard GP model.
Our next goal is to derive a bound on the outputs of the second layer
\begin{equation}
    \begin{split}
        \log \Prob{\mat{f}_2 \given \mat{u}_2}
        &= \log \int \Prob{\mat{f}_2, \mat{f}_1, \mat{u}_1 \given \mat{u}_2} \diff \mat{f}_1 \diff \mat{u}_1 \\
        &= \log \int \Prob{\mat{f}_2, \given \mat{u}_2, \mat{f}_1, \mat{u}_1} \Prob*{\mat{f}_1, \mat{u}_1} \diff \mat{f}_1 \diff \mat{u}_1,
    \end{split}
\end{equation}
that is, an expression in which the uncertainty about the $\mat{f}_1$ and the cross-layer dependencies on the $\mat{u}_1$ are both marginalized.
We start by considering the relevant terms from \cref{eq:dgp:full_model} and apply \cref{eq:gp:expected_log_likelihood_svgp} to marginalize $\mat{f}_1$ in
\begin{equation}
    \begin{split}
        \log\Prob{\mat{f}_2 \given \mat{u}_2, \mat{u}_1}
        &= \log\int\Prob{\mat{f}_2, \mat{f}_1 \given \mat{u}_2, \mat{u}_1}\diff\mat{f}_1 \\
        &\geq \log\int \aProb{\mat{f}_2 \given \mat{u}_2, \mat{f}_1} \aProb{\mat{f}_1 \given \mat{u}_1} \\
        &\qquad\qquad \cdot \Fun*{\exp}{-\frac{1}{2\sigma_1^2} \Fun*{\tr}{\mat{K}_{11} - \mat{Q}_{11}} - \frac{1}{2\sigma_2^2} \Fun*{\tr}{\mat{K}_{22} - \mat{Q}_{22}}} \diff \mat{f}_1 \\
        &\geq \Moment{\E_{\aProb{\mat{f}_1 \given \mat{u}_1}}}{\log \aProb{\mat{f}_2 \given \mat{u}_2, \mat{f}_1}} \\
        &\qquad\qquad - \Moment*{\E_{\aProb{\mat{f}_1 \given \mat{u}_1}}}{\frac{1}{2\sigma_2^2} \Fun*{\tr}{\mat{K}_{22} - \mat{Q}_{22}}}
        - \frac{1}{2\sigma_1^2} \Fun*{\tr}{\mat{K}_{11} - \mat{Q}_{11}},
    \end{split}
\end{equation}
where we write $\aProb{\mat{f}_1 \given \mat{u}_1} = \Gaussian*{\mat{f}_1 \given \mat{\mu}_1, \sigma_1^2 \Eye}$ to incorporate the Gaussian noise in the latent space.
Due to our assumption that $\mat{u}_1$ is a sufficient statistic for $\mat{f}_1$ we choose the Gaussians
\begin{equation}
    \label{eq:var_compression:variational_assumption}
    \begin{split}
        \Variat{\mat{f}_1 \given \mat{u}_1}
        &= \aProb{\mat{f}_1 \given \mat{u}_1}\text{, and}\\
        \Variat{\mat{f}_1} &= \int \aProb{\mat{f}_1 \given \mat{u}_1} \Variat{\mat{u}_1} \diff \mat{u}_1,
    \end{split}
\end{equation}
and use another variational approximation to marginalize $\mat{u}_1$.
This yields
\begin{equation}
    \begin{split}
        \label{eq:var_compression:f_marginal_likelihood}
        \log \Prob{\mat{f}_2 \given \mat{u}_2}
        &= \log \int \Prob{\mat{f}_2, \mat{u}_1 \given \mat{u}_2} \diff \mat{u}_1 \\
        &= \log \int \Prob{\mat{f}_2 \given \mat{u}_2, \mat{u}_1} \Prob{\mat{u}_1} \diff \mat{u}_1 \\
        &\geq \int \Variat{\mat{u}_1} \log\frac{\Prob{\mat{f}_2 \given \mat{u}_2, \mat{u}_1} \Prob{\mat{u}_1}}{\Variat{\mat{u}_1}} \diff \mat{u}_1 \\
        &= \Moment*{\E_{\Variat{\mat{u}_1}}}{\log \Prob{\mat{f}_2 \given \mat{u}_1, \mat{u}_2}}
        - \KL{\Variat{\mat{u}_1}}{\Prob{\mat{u}_1}} \\
        &\geq \Moment*{\E_{\Variat{\mat{u}_1}}}{\Moment*{\E_{\aProb{\mat{f}_1 \given \mat{u}_1}}}{\log \aProb{\mat{f}_2 \given \mat{u}_2, \mat{f}_1}}}
        - \KL{\Variat{\mat{u}_1}}{\Prob{\mat{u}_1}} \\
        &\quad {} - \frac{1}{2\sigma_1^2} \Fun*{\tr}{\mat{K}_{11} - \mat{Q}_{11}}
        - \Moment*{\E_{\Variat{\mat{u}_1}}}{\Moment*{\E_{\aProb{\mat{f}_1 \given \mat{u}_1}}}{\frac{1}{2\sigma_2^2} \Fun*{\tr}{\mat{K}_{22} - \mat{Q}_{22}}}} \\
        &\geq \Moment*{\E_{\Variat{\mat{f}_1}}}{\log \aProb{\mat{f}_2 \given \mat{u}_2, \mat{f}_1}},
        - \KL{\Variat{\mat{u}_1}}{\Prob{\mat{u}_1}} \\
        &\quad {} - \frac{1}{2\sigma_1^2} \Fun*{\tr}{\mat{K}_{11} - \mat{Q}_{11}}
        - \frac{1}{2\sigma_2^2} \Moment*{\E_{\Variat{\mat{f}_1}}}{\Fun*{\tr}{\mat{K}_{22} - \mat{Q}_{22}}},
    \end{split}
\end{equation}
where we apply Fubini's theorem~\parencite{fubini_sugli_1907} to exchange the order of integration in the expected values.
The expectations with respect to $\Variat{\mat{f}_1}$ involve expectations of kernel matrices, also called $\Psi$-statistics, in the same way as in \parencites{damianou_deep_2013} and are given by
\begin{equation}
    \begin{split}
        \label{eq:var_compression:psi_statistics}
        \psi_2 &= \Moment*{\E_{\Variat{\mat{f}_1}}}{\Fun*{\tr}{\mat{K}_{\mat{f}_2 \mat{f}_2}}}, \\
        \mat{\Psi_2} &= \Moment*{\E_{\Variat{\mat{f}_1}}}{\mat{K}_{\mat{f}_2 \mat{u}_2}}, \\
        \mat{\Phi_2} &= \Moment*{\E_{\Variat{\mat{f}_1}}}{\mat{K}_{\mat{u}_2 \mat{f}_2}\mat{K}_{\mat{f}_2 \mat{u}_2}}. \\
    \end{split}
\end{equation}
These $\Psi$-statistics can be computed analytically for multiple kernels, including the squared exponential kernel.
To obtain the final formulation of the desired bound for $\log \Prob{\mat{f}_2 \given \mat{u}_2}$ we substitute \cref{eq:var_compression:psi_statistics} into \cref{eq:var_compression:f_marginal_likelihood} and get the analytically tractable bound
\begin{equation}
    \begin{split}
        \log \Prob{\mat{f}_2 \given \mat{u}_2}
        &\geq \log\Gaussian*{\mat{f}_2 \given \mat{\Psi}_2\mat{K}_{\mat{u}_2\mat{u}_2}\inv \mat{m}_2, \sigma_2^2\Eye}
        - \KL{\Variat{\mat{u}_1}}{\Prob{\mat{u}_1}} \\
        &\quad {} - \frac{1}{2\sigma_1^2} \Fun*{\tr}{\mat{K}_{11} - \mat{Q}_{11}}
        - \frac{1}{2\sigma_2^2} \left( \psi_2 - \Fun*{\tr}{\mat{\Psi_2}\mat{K}_{\mat{u}_2\mat{u}_2}\inv} \right) \\
        &\quad {} - \frac{1}{2\sigma_2^2} \tr\left(\left(\mat{\Phi}_2 - \mat{\Psi}_2\tran\mat{\Psi}_2\right) \mat{K}_{\mat{u}_2\mat{u}_2}\inv \left(\mat{m}_2\mat{m}_2\tran + \mat{S}_2\right)\mat{K}_{\mat{u}_2\mat{u}_2}\inv\right).
    \end{split}
\end{equation}
This bound is structurally similar to the SVGP bound in~\cref{eq:gp:expected_log_likelihood_svgp} but contains additional terms introduced by the propagation of uncertainties from $\Prob*{\mat{f}_1}$.
To derive a variational bound for~\cref{eq:dgp:marginal_likelihood} where all $\mat{f}_l$ have been marginalized, we can apply the same steps as described above recursively, resulting in the nested variational compression lower bound given by
\begin{equation}
    \begin{split}
        \label{eq:var_compression:full_bound}
        \MoveEqLeft \Lc_{\text{NVC}} \geq \\
        &\log\Gaussian*{\mat{y} \given \mat{\Psi}_L\mat{K}_{\mat{u}_L\mat{u}_L}\inv \mat{m}_L, \sigma_n^2\Eye}
        - \sum_{l=1}^L \KL{\Variat{\mat{u}_l}}{\Prob{\mat{u}_l}} \\
        &- \frac{1}{2\sigma_1^2} \Fun*{\tr}{\mat{K}_{11} - \mat{Q}_{11}}
        - \sum_{l=2}^L \frac{1}{2\sigma_l^2} \left( \psi_l - \Fun*{\tr}{\mat{\Psi_l}\mat{K}_{\mat{u}_l\mat{u}_l}\inv} \right) \\
        &- \sum_{l=2}^L \frac{1}{2\sigma_l^2} \tr\left(\left(\mat{\Phi}_l - \mat{\Psi}_l\tran\mat{\Psi}_l\right) \mat{K}_{\mat{u}_l\mat{u}_l}\inv \left(\mat{m}_l\mat{m}_l\tran + \mat{S}_l\right)\mat{K}_{\mat{u}_l\mat{u}_l}\inv\right).
    \end{split}
\end{equation}
Just as the SVGP bound, it factorizes along the data and enables stochastic optimization.
However, depending on the kernel, the calculation of psi-statistics can be computationally expensive or analytically intractable, limiting the applicability of this bound to a limited set of hierarchical GP models.

Since the nested variational compression bound introduces a conditional independence assumption between layers given the approximations of the latent variables $\Variat*{\mat{f}_l}$, approximate predictions can be derived by recursively calculating $\Variat*{\mat{f}_1}$ to $\Variat*{\mat{f}_L}$, all of which are Gaussian.
Given inputs $\mat{X_{\ast}}$, we recursively marginalize the intermediate layers
\begin{equation}
    \begin{split}
        \Variat{\mat{f}_{l, \ast}}
        &= \int \Variat{\mat{f}_{l, \ast}, \mat{f}_{l-1, \ast}} \diff \mat{f}_{l-1, \ast} \\
        &= \int \Variat{\mat{f}_{l, \ast} \given \mat{f}_{l-1, \ast}} \Variat{\mat{f}_{l-1, \ast}} \diff \mat{f}_{l-1, \ast} \\
        &= \Moment*{\E_{\Variat{\mat{f}_{l-1, \ast}}}}{\Variat{\mat{f}_{l, \ast} \given \mat{f}_{l-1, \ast}}} \\
        &= \Gaussian*{\mat{f}_{l, \ast} \given \mat{\bar{\mu}}_{l, \ast}, \mat{\bar{\Sigma}}_{l, \ast}}
    \end{split}
\end{equation}
with
\begin{align*}
    \mat{\bar{\mu}}_{l, \ast}    & = \mat{\Psi}_{l\ast} \mat{K}_{\mat{u}_l \mat{u}_l    }\inv \mat{m}_l                                                    \\
    \mat{\bar{\Sigma}_{l, \ast}} & = \mat{\Psi}_{l\ast}\mat{K}_{\mat{u}_l \mat{u}_l}\inv\mat{S}_l\mat{K}_{\mat{u}_l \mat{u}_l}\inv\mat{\Psi}_{l\ast}\tran.
\end{align*}
For the first layer, the expectation collapses to usual SVGP predictions.
The final function values are given by $\Variat*{\mat{f}_L}$.
While passing Gaussian variational messages through a deep GP to achieve conditional independence between layers is computationally convenient, it is a strong simplification as propagating a Gaussian distribution through a Gaussian process generally does not result in a Gaussian distribution.
As discussed in more detail in~\parencite{hensman_nested_2014}, nested variational compression tends to underestimate uncertainties, especially when the functions $f_l$ become more non-linear.

\subsubsection{Doubly stochastic variational inference}
\label{toc:dsvi}
To overcome some of the limitations of nested variational compression,~\textcite{salimbeni_doubly_2017} proposed the doubly stochastic variational inference (DSVI) approximation.
Instead of relying on an explicit variational approximation for the $\mat{f}_l$, DSVI is based on the observation that due to the conditional independence assumption of the data introduced by the $\mat{u}_l$, function values $\mat{f}_{n,l}$ can be sampled independently and efficiently.
Under the SVGP variational sufficient statistics assumptions, the posterior of a single GP can be written as
\begin{equation}
    \begin{split}
        \label{eq:dsvi:variational_marginals}
        \Variat*{\mat{f}_l \given \mat{f}_{l-1}}
        &=
        \int \Variat*{\mat{u}_l}
        \Prob*{\mat{f}_l \given \mat{u}_l, \mat{f}_{l-1}}
        \diff \mat{u}_l
        \\
        &=
        \int \Variat*{\mat{u}_l}
        \prod_{n=1}^N \Prob*{\mat{f}_{l,n} \given \mat{u}_l, \mat{f}_{l-1, n}}
        \diff \mat{u}_l,
    \end{split}
\end{equation}
which can be evaluated analytically, since it is a convolution of Gaussians.
For the hierarchical case, we choose the same variational independence assumptions as in the NVC approximation above
\begin{equation}
    \begin{split}
        \label{eq:dsvi:variational_distribution}
        \Variat*{\mat{f}_1, \mat{u}_1, \dots, \mat{f}_L, \mat{u}_L}
        &= \prod_{l=1}^L \Prob*{\mat{f}_l \given \mat{u}_l, \mat{f}_{l-1}} \Variat*{\mat{u}_l}.
    \end{split}
\end{equation}
The factorization along the data still holds in the variational hierarchical case.
We can formulate the marginal function value of the $\nth{n}$ data point by inserting~\cref{eq:dsvi:variational_marginals} into~\cref{eq:dgp:marginal_likelihood} to obtain
\begin{equation}
    \begin{split}
        \label{eq:dsvi:variational_layers}
        \Variat*{\mat{f}_{L,n}}
        &= \int \prod_{l=1}^{L-1} \Variat*{\mat{f}_{l,n} \given \mat{f}_{l-1,n}} \diff \mat{f}_{l, n} \\
        &= \int \prod_{l=1}^{L-1} \int \Variat*{\mat{u}_l} \Prob*{\mat{f}_{l,n} \given \mat{u}_l, \mat{f}_{l-1, n}} \diff \mat{u}_l \diff \mat{f}_{l, n}.
    \end{split}
\end{equation}
A consequence of this formulation is that drawing a sample from $\Variat*{\mat{f}_{L,n}}$ can be achieved via ancestral sampling through the hierarchical GP model by drawing samples from the different $\Variat*{\mat{f}_{l, n}}$ in turn.
Instead of passing Gaussian messages as in the NVC approximation, we can draw Monte-Carlo samples from $\Variat*{\mat{f}_{L, n}}$ directly.
Because the data are independent under the variational assumptions, we can draw independent univariate samples per observation.
Sampling $\Variat*{\mat{f}_{L, n}}$ directly also allows us to sample the bound in~\cref{eq:gp:sgpr_bound_1} that generalizes to the hierarchical case and is given by
\begin{equation}
    \begin{split}
        \label{eq:dsvi:full_bound}
        \Lc_{\text{DSVI}} \geq
        \sum_{n=1}^N \Moment*{\E_{\Variat*{\mat{f}_{L, n}}}}{\log \Prob*{\mat{y}_n \given \mat{f}_{L, n}}}
        - \sum_{l=1}^L \KL*{\Variat*{\mat{u}_l}}{\Prob*{\mat{u}_l}}.
    \end{split}
\end{equation}
Evaluating this bound requires $\Oh(NM^2D)$ computations.
Through local reparametrization~\parencite{kingma_variational_2015}, the gradients of the variational bound can be sampled directly.
Evaluations of the DSVI-bound and its gradients have two unbiased sources of stochasticity~\parencite{salimbeni_doubly_2017}.
First, the expected likelihood is approximated through Monte Carlo samples and second, since the likelihood factorizes along the data, mini-batching is possible.
Drawing Monte Carlo samples of the likelihood directly elimates the need for Psi-statistics as in~\cref{eq:var_compression:full_bound}, making DSVI applicable to a broader set of hierarchical Gaussian process models.

Monte Carlo samples of approximate predictions can be drawn using~\cref{eq:dsvi:variational_layers} as well.
The free-form predictive density can be approximated using the mixture
\begin{equation}
    \begin{split}
        \label{eq:dsvi:approximate_predictions}
        \Variat*{\mat{f}_{L, \ast} \given \mat{X}_{\ast}}
        &\approx \frac{1}{S} \sum_{s=1}^S \Variat*{\mat{q}_{L, \ast} \given \mat{u}_L, \mat{f}_{L-1, \ast}},
    \end{split}
\end{equation}
with $S$ samples propagated through the hierarchical GP starting from the $\mat{X}_{\ast}$.
\todoi{Add some thoughts about the bigger picture here?}
