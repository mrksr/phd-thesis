\chapter{Bayesian Machine Learning}
\label{toc:bayesian_ml}
Everything is awesome, everything is cool when you're part of a team.
Everything is awesome, when you're living out a dream.

\begin{itemize}
    \item Machine Learning is an extension of algorithmics (and numerics) with different goals
    \item In algorithmics the goal is usually clear and well-defined
    \item Use polynomial interpolation as an example.
          We can use that for curve-fitting, but we generally do not associate it with machine learning because the goal is not to minimize some loss function but rather to find a uniquely defined solution.
          The "ML-variant" of polynomial interpolation would be generalized linear models with polynomial basis functions and there, the correct answer is much less clear. (PCA, "least squares" quadratic fit, etc)
    \item In ML, an essential element is a lack of information and our freedom to handle that lack of information. A subjective \emph{goal} is integral to machine learning. ML is about making the best of the situation and is inherently subjective. And inherently uncertain.
    \item The lack of information is generally introduced via finite information for an infinite object
    \item We can formulate this situation nicely in terms of probability distributions
          \begin{itemize}
              \item A ML problem is an inference problem
              \item We always care about some kind of joint distribution
              \item Depending on which structure we assume, different branches of ML arise
              \item The two main strategies to formulate finiteness (parameters and data) give rise to the (non-)parametric split
          \end{itemize}
    \item Things like learning theory try for objectivity but that does not scale
    \item In practice, ML is about the step from risk minimization to empirical risk minimization and ways to handle the annoying results
    \item We get back to global arguments via encoding knowledge in
          \begin{itemize}
              \item information operator
              \item hypothesis space
              \item loss function
              \item optimization algorithm
          \end{itemize}
    \item Turns out the problem statement and hypothesis space (same thing?) are generally preferred
    \item Bayesian ML then offers a nice language to formulate such hypotheses
    \item Maybe insert the core probabilistic numerics argument about algorithms here?
    \item Most importantly, Bayes' rule offers a consistent and simple way to combine local and global assumptions that does not break probability theory.
          And has a nice semantic interpretation.
          It is not a unique solution though.
    \item With Bayes' rule, a structurally simple \emph{learning algorithm} can be formulated where the complexity is in the hypothesis space (prior) and loss function (likelihood). No optimization needed at first glance.
    \item Turns out this is a nice fit for problems in the physical word which has a few notable properties:
          \begin{itemize}
              \item Inevitability of uncertainty
              \item Availability of knowledge
              \item Need for interpretability
          \end{itemize}
    \item Discuss the differences between Web ML and Industrial ML
    \item Industrial ML motivates our thesis, where we want to formulate models that are inherently interpretable and trustworthy
    \item It will turn out that in this space, judging whether a model is good is actually very tricky.
\end{itemize}

\begin{figure}[tp]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_data}
        \caption{
            Data
            \label{fig:bayesian_ml:polynoms:data}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_lagrange}
        \caption{
            \enquote{Algorithms}
            \label{fig:bayesian_ml:polynoms:lagrange}
        }
    \end{subfigure}
    \caption[Algorithmic Interpolation]{
        ???
    }
\end{figure}

\begin{figure}[tp]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_linear}
        \caption{
            Linear ML
            \label{fig:bayesian_ml:polynoms:data}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_polynomial}
        \caption{
            Polynomial ML
            \label{fig:bayesian_ml:polynoms:polynomial}
        }
    \end{subfigure}
    \caption[ML Interpolation]{
        ???
    }
\end{figure}
