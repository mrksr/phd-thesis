\chapter{Introduction}
\label{toc:introduction}

Machine learning (ML) methods have seen great success recently in a wide range of digital domains such as speech recognition~\parencite{chorowski_attention-based_2015}, computer vision~\parencite{russakovsky_imagenet_2015}, video games~\parencite{berner_dota_2019}, or machine translation~\parencite{johnson_googles_2017}.
In such domains, data is often abundant, which shifts the focus away from traditional statistical models to approaches that are highly adaptable and scalable.
At the same time, the consequences of mistakes tend to be mild, enabling fast prototyping and heuristic design choices that can be validated in the deployed systems using the performance metrics of interest.

However, bridging the gap to applications in critical systems in the physical world has proved challenging.
Problem domains like robotics or industrial control introduce a new set of requirements.
In industrial control, machine learning is used to find more efficient or safer control strategies not obvious to the engineers designing a machine~\parencite{hein_benchmark_2017}.
In such scenarios, relevant data for finding new strategies is often scarce, since the most valuable data such as when a machine will fail, or how it will behave in new situations, is never produced.
As applications become more safety-relevant, gathering data through exploration can be problematic due to the adverse consequences of failure.
As a consequence, machine learning systems are required to handle low data regimes and uncertainties about a system's true behavior.

If they are to operate in safety critical areas, interact with people or carry responsibility, machine learning systems must be robust, trustworthy, and assessable.
Besides optimizing for good average-case performance, a responsible system needs to reason about plausible worst-cases to be able to avoid them.
To ensure trust, ML-systems often need to be verified by domain experts before deployment and are used to test hypotheses or make impactful decisions, emphasizing a need for interpretability.
Models are required to incorporate and reproduce expert knowledge and make consistent predictions, while at the same time yielding new insights from data.

A key technique that allows us to cope with these requirements is probabilistic modeling.
Bayesian inference offers a framework to formalize expert knowledge as in prior assumptions and combine them with potentially uncertain data to represent knowledge about a system.
Using Bayes' rule, a distribution of plausible models is reweighed by their ability to explain observed data to yield a posterior distribution over plausible explanations.
To make predictions, all plausible models are taken into account, allowing us to both quantify confidence in predictions and take more unlikely but relevant scenarios into consideration.

For applications in the physical world, expert knowledge is often based on an intuitive understanding of the underlying physics leading to coarse expectations about system-behavior on different layers of abstraction.
This dissertation explores how such abstract knowledge can be exploited to formulate structured Bayesian models that reproduce such expectations but are still able to learn from data.
We will investigate the properties of such models in concrete applications and discuss how a requirements such as trustworthiness can be formalized by taking downstream tasks into account.
In this chapter, we will introduce the notion of structured models and discuss their relation to other modeling approaches and the main contributions of the work presented in this thesis.


\section{Structured Machine Learning Models}
\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{intro_model_nature}
        \caption{
            \label{fig:introduction:models:nature}
            Nature
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{intro_model_black_box}
        \caption{
            \label{fig:introduction:models:black_box}
            Black-Box
        }
    \end{subfigure}
    \caption{
        \label{fig:introduction:models:one}
        Nature and black box models.
    }
\end{figure}
\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{intro_model_white_box}
        \caption{
            \label{fig:introduction:models:white_box}
            White-Box
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{intro_model_structured}
        \caption{
            \label{fig:introduction:models:structured}
            Structured
        }
    \end{subfigure}
    \caption{
        \label{fig:introduction:models:two}
        Nature and black box models.
    }
\end{figure}
There are two cultures.
We explore a third.
This fall.

\begin{itemize}
    \item Introduce regression as a functional dependency in nature we want to model.
    \item Following \textcite{breiman_statistical_2001,shmueli_explain_2010}, we can tackle the problem using algorithmic (black-box) and data (white-box) models.
    \item Traditional statistics favor white-box models and care about describing and understanding observational data~\parencite{giulio_d._agostini_bayesian_2003,andrew_gelman_bayesian_2013,casella_statistical_2002,cox_planning_1958}.
          In this view of the world, we need to come up with a model of nature that completely explains all aspects of the data and learning comes down to fitting a limited number of highly interpretable parameters.
          The smaller the number of parameters the better, as we want to develop a simple and specific theory that explains the data well (Occam's razor).
          We do not necessarily care about predictions.
    \item The other end of the spectrum are black-box or algorithmic models which are more closely associated with machine learning~\parencite{mitchell_machine_1997,goodfellow_deep_2016}.
          Here, we accept that we cannot model nature and instead recognize that there is some functional dependency in the data a general function approximator can, given enough data, approximate arbitrarily well.
          Since a successfully powerful approximator can explain any observation set, we need to measure performance in terms of predictions and that is actually what we care about.
          We never claim that we are able to uncover the correct model or the true generative process.
    \item In this thesis, we want to study what we call structured models.
          Structured models live between these extremes because we design models based on our understanding of nature (the underlying problem).
          At the same time, we accept that there are things we do not (or cannot) understand and insert general function approximators~\parencite{friedman_data_1998}.
          Our goal is to be more data efficient compared to black-box models because there is less we need to infer from data thanks to our prior knowledge.
          At the same time, we want to gain insights from the data thanks to the general function approximators, so our model is not purely descriptive either.
    \item Many successes in machine learning are based on highly structured models which are often motivated differently.
          Examples include convolutional networks for vision, LSTM for language processing or Monte-Carl tree search for two-player perfect information games.
          Since these models are generally understood as being black-box, they do not care about statistical measures such as uncertainty quantification or actual insights about the generating process.
          They are being used as engineering-tools instead.
          In this thesis we care about well-founded probabilistic models.
\end{itemize}


\section{Thesis outline}
\todoi{Is it finished yet?}
We will explore two things in the thesis.
\begin{itemize}
    \item We show two approaches to adding structure to a learning problem.
          First, we use what might be called formal structure motivated by the mathematical formulation of an underlying problem, the data association problem.
          This is closer to black-box models.
          And second, we use structure motivated by expert understanding of the underlying physical process that generated the data, which is closer to white-box models.
    \item As it turns out, since the models live in-between the two extremes, their respective tools for judging whether a model is good are not sufficient.
          Only looking at data-fits (both in training and test) is not enough since things like symmetries or ambiguities in the general function approximators can lead to multiple plausible models that show similar performance.
          Marignal likelihoods do not capture the inner shape of a model and we there is no straightforward way to formalize success.
          Because of this, we discuss measuring success based on a downstream task which requires a model to actually represent the true underlying generative process.
\end{itemize}


\section{Publications}
\subsubsection{Papers}
\begin{itemize}
    \item \fullfullcite{kaiser_bayesian_2018}

          Here, I did great things.
    \item \fullfullcite{kaiser_data_2019}
    \item \fullfullcite{kaiser_interpretable_2019}
    \item \fullfullcite{kaiser_bayesian_2020}
    \item \fullfullcite{bodin_modulating_2020}
    \item \fullfullcite{ustyuzhaninov_compositional_2020}
\end{itemize}

\subsubsection{Patents}
\begin{itemize}
    \item \fullfullcite{egedal_verfahren_2019}
    \item \fullfullcite{kaiser_verfahren_2019}
    \item \fullfullcite{geipel_transferlernen_2020}
    \item \fullfullcite{depeweg_computer_2020}
\end{itemize}
