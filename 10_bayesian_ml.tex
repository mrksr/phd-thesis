\chapter{Bayesian Machine Learning}
\label{toc:bayesian_ml}

\begin{itemize}
    \item Machine Learning is an extension of algorithmics (and numerics) with different goals
    \item In algorithmics the goal is usually clear and well-defined
    \item Use polynomial interpolation as an example.
          We can use that for curve-fitting, but we generally do not associate it with machine learning because the goal is not to minimize some loss function but rather to find a uniquely defined solution.
          The "ML-variant" of polynomial interpolation would be generalized linear models with polynomial basis functions and there, the correct answer is much less clear. (PCA, "least squares" quadratic fit, etc)
    \item In ML, an essential element is a lack of information and our freedom to handle that lack of information. A subjective \emph{goal} is integral to machine learning. ML is about making the best of the situation and is inherently subjective. And inherently uncertain.
    \item The lack of information is generally introduced via finite information for an infinite object
    \item We can formulate this situation nicely in terms of probability distributions
          \begin{itemize}
              \item A ML problem is an inference problem
              \item We always care about some kind of joint distribution
              \item Depending on which structure we assume, different branches of ML arise
              \item The two main strategies to formulate finiteness (parameters and data) give rise to the (non-)parametric split
          \end{itemize}
    \item Things like learning theory try for objectivity but that does not scale
    \item In practice, ML is about the step from risk minimization to empirical risk minimization and ways to handle the annoying results
    \item We get back to global arguments via encoding knowledge in
          \begin{itemize}
              \item information operator
              \item hypothesis space
              \item loss function
              \item optimization algorithm
          \end{itemize}
    \item Turns out the problem statement and hypothesis space (same thing?) are generally preferred
    \item Bayesian ML then offers a nice language to formulate such hypotheses
    \item Maybe insert the core probabilistic numerics argument about algorithms here?
    \item Most importantly, Bayes' rule offers a consistent and simple way to combine local and global assumptions that does not break probability theory.
          And has a nice semantic interpretation.
          It is not a unique solution though.
    \item With Bayes' rule, a structurally simple \emph{learning algorithm} can be formulated where the complexity is in the hypothesis space (prior) and loss function (likelihood). No optimization needed at first glance.
    \item Turns out this is a nice fit for problems in the physical word which has a few notable properties:
          \begin{itemize}
              \item Inevitability of uncertainty
              \item Availability of knowledge
              \item Need for interpretability
          \end{itemize}
    \item Discuss the differences between Web ML and Industrial ML
    \item Industrial ML motivates our thesis, where we want to formulate models that are inherently interpretable and trustworthy
    \item It will turn out that in this space, judging whether a model is good is actually very tricky.
\end{itemize}

\todo[inline]{Industrial Applications of ML as a start?}

\section{Machine learning problems}

One of the roots of machine learning (ML) lies in the study of algorithms in theoretical computer science.
An algorithm is a well-defined sequence of computational steps transforming a set of inputs to a set of outputs.
It is a tool for solving a computational problem, which is defined by an abstract problem of admissable inputs and expected outputs.
An algorithm solves such a problem if for every possible input, the algorithm provably yields the correct output.

Consider the computational problem of sorting a list of numbers in ascending order.
One possible problem~\parencite{cormen_introduction_2009} of the sorting problem is:
\begin{problem}[Sorting]
\label{prob:bayesian_ml:sorting}
\begin{labeling}{Output:}
    \item[Input:] A sequence of $N$ integers $\mat{I} = (i_1, \dots, i_N)$
    \item[Output:] A reordering of $\mat{I}$ called $\mat{O} = (o_1, \dots, o_N)$ such that $o_1 \leq \dots \leq o_N$.
\end{labeling}
\end{problem}
For example, for the input $\mat{\hat{I}}=(12, 8, 23, 4)$ the correct output is $\mat{\hat{O}}=(4, 8, 12, 23)$.
A concrete input $\mat{\hat{I}}$ is called an instance of a problem.
Importantly, such an instance contains all the required information to compute the unique output $\mat{\hat{O}}$.
The correctness of the output can be checked via the formal problem problem.
The various available (correct) sorting algorithms only differ in which and how many computational steps they take to arrive at the output, not in the output itself.

Machine learning can be seen as an extension of algorithmics towards problems where a formal description of a uniquely defined solution does not exist.
Instead, problems in ML are characterized by the observation of finitely many examples or data together with the objective to derive knowledge about how these examples were generated.
This knowledge can then be used to describe common patterns in the data or generate new examples that conform to previous observations.

\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_data}
        \caption{
            Data
            \label{fig:bayesian_ml:polynoms:data}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_lagrange}
        \caption{
            \enquote{Algorithms}
            \label{fig:bayesian_ml:polynoms:lagrange}
        }
    \end{subfigure}
    \caption[Algorithmic Interpolation]{
        ???
        \label{fig:bayesian_ml:polynoms:ml}
    }
\end{figure}

As an example, consider the data shown in \cref{fig:bayesian_ml:polynoms:data}, a set of $N$ pairs of real numbers for which we assume that all $x_i$ are pairwise different.
The knowledge about this data to be uncovered is the functional dependency between the two data dimensions that was used to generate the data.
This is called a regression problem and is strongly under-specified:
There exist uncountably many functions on the real numbers that explain any finite set of observed points.
The problem
\begin{problem}[Regression]
\label{prob:bayesian_ml:regression}
\begin{labeling}{Output:}
    \item[Input:] A set of $N$ pairs of real numbers $\Dc = \Set*{\left(x_n, y_n\right)}_{n=1}^N \subseteq \Xc \times \Yc$
    \item[Output:] The functional dependency $f : \Xc \to \Yc$ used to generate the data.
\end{labeling}
\end{problem}
is therefore not a computational problem and asking for a solution or algorithm for this problem is not a well-posed question.

One can however derive computational problems from the regression problem by making assumptions about nature of the function $f$ that characterize a unique solution.
For example, one could ask for the simplest polynomial that explains the data.
\begin{problem}[Lagrange Polynomial]
\label{prob:bayesian_ml:lagrange}
\begin{labeling}{Output:}
    \item[Input:] A set of $N$ pairs of real numbers $\Dc = \Set*{\left(x_n, y_n\right)}_{n=1}^N \subseteq \Xc \times \Yc$
    \item[Output:] The polynomial of smallest degree for which $f(x_n) = y_n$ holds for all $(x_n, y_n) \in \Dc$.
\end{labeling}
\end{problem}
It can be shown that this is indeed a well-posed computational problem whose unique solution is the Lagrange polynomial $L$~\parencite{waring_vii._1779} given by the explicit form
\begin{align}
    \begin{split}
        L(x) &= \sum_{i=0}^N y_i \ell_i(x)\text{, with} \\
        \ell_i &= \prod_{\substack{j = 0\\j \neq i}}^N \frac{x - x_j}{x_i - x_j}.
    \end{split}
    \label{eq:bayesian_ml:lagrange}
\end{align}
\Cref{fig:bayesian_ml:polynoms:lagrange} shows the Lagrange polynomial interpolating the example dataset.
The figure also shows another derived computational problem of linear interpolation.
Here, the function is defined as being piecewise linear between the different data points, thus also reproducing the data.

Both of these problems and derived algorithms would typically not be identified as ML approaches as it can be argued that they do not derive new insights from data.
Because of the explicit nature of their algorithms as seen in \cref{eq:bayesian_ml:lagrange}, they much more closely resemble classical algorithms such as sorting algorithms.

\begin{figure}[t]
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_linear}
        \caption{
            Linear ML
            \label{fig:bayesian_ml:polynoms:linear}
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\halffigurewidth}
        \centering
        \includestandalonewithpath{figures/polynoms_polynomial}
        \caption{
            Polynomial ML
            \label{fig:bayesian_ml:polynoms:polynomial}
        }
    \end{subfigure}
    \caption[ML Interpolation]{
        ???
        \label{fig:bayesian_ml:polynoms:ml}
    }
\end{figure}
A common approach to formulating a problem that lets the data speak for itself is to formulate more implicit requirements.
Instead of describing exactly one possible function (such as the Lagrange Polynomial), one can choose a broader set of candidate functions or hypotheses $\Hc$ and then select one of the candidates $f \in \Hc$ that is optimal with respect to some measure of performance.
For example, a common assumption about the process used to generate the data is that it separates into two additive components
\begin{align}
    y_n = f(x_n) + \epsilon(x_n).
\end{align}
The first summand $f$ captures the truly informative functional dependency between $x$ and $y$ that applies for all observations while the second term $\epsilon$ captures local error or noise that can be ignored.

A direct consequence of this assumption is that output of the regression problem $f$ need no longer interpolate the data perfectly.
However, this introduces the need for additional assumptions about how far from the data $f$ is allowed to be or, equivalently, about the shape of $\epsilon$.
One can choose $f$ to be a linear function, giving rise to the linear regression problem.
\begin{problem}[Linear regression]
\begin{labeling}{Output:}
    \item[Input:] A set of $N$ pairs of real numbers $\Dc = \Set*{\left(x_n, y_n\right)}_{n=1}^N \subseteq \Xc \times \Yc$ and an error function $e : \Hc \times \Xc \times \Yc \to \Rb$
    \item[Output:] A function $f$ such that
    \begin{align}
        f \in \argmin_{f \in \Hc} e(f, \Dc)
    \end{align}
    with $\Hc$ being the set of linear functions.
\end{labeling}
\end{problem}
\Cref{fig:bayesian_ml:polynoms:linear} shows the different results of three algorithms with different choices of error functions $e$.
With $f(x) = \mat{W}x + b$ those are
\begin{labeling}{Absolute error:}
    \item[Squared error:] $e(f, \Dc) = \sum_{n=1}^N (y_n - f(x_n))^2$
    \item[Absolute error:] $e(f, \Dc) = \sum_{n=1}^N \abs{y_n - f(x_n)}$
    \item[Ridge:] $e(f, \Dc) = \sum_{n=1}^N (y_n - f(x_n))^2 - \norm{\mat{W}}_2 - b^2$.
\end{labeling}
Altering the set of hypotheses $\Hc$ between linear, quadratic and cubic polynomials together with the squared error function results in the functions shown in \cref{fig:bayesian_ml:polynoms:polynomial}.

It is important to note that none of these proposed algorithms and plotted functions is \emph{ojectively} the correct solution to the regression problem.
None of them is equal to the function that was used to generate the data and even if it was, there would be no way to tell.
A core property of machine learning problems is that what characterizes the correct solution is an inherently \emph{subjective} question.
This subjectiveness is represented in multiple choices:
\begin{enumerate}
    \item The assumed structure underlying the data.
    \item The space of hypotheses for valid solutions.
    \item The algorithm used to select from these hypotheses.
\end{enumerate}
In the following, statistical learning theory and Bayesian machine learning are introduced as tools to formalize these choices and establish a mathematical framework.

\section{Statistical learning}

\section{Bayesian machine learning}

\section{Thesis outline}
